[["Map",1,2,9,10,477,478,740,741,886,887,936,937,1223,1224],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.4","content-config-digest","26c8cbf0039c4c4e","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://semio.community/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":true,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"prefetch\":true,\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[\"webmention.io\"],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[[null,{\"rel\":[\"nofollow\",\"noreferrer\"],\"target\":\"_blank\"}],[null,{\"theme\":{\"light\":\"rose-pine-dawn\",\"dark\":\"rose-pine\"},\"transformers\":[{\"name\":\"@shikijs/transformers:notation-diff\"},{\"name\":\"@shikijs/transformers:meta-highlight\"}]}],null],\"remarkRehype\":{\"footnoteLabelProperties\":{\"className\":[\"\"]},\"footnoteBackContent\":\"â¤´\"},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{\"WEBMENTION_API_KEY\":{\"context\":\"server\",\"access\":\"secret\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_URL\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_PINGBACK\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"}},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","people",["Map",11,12,47,48,73,74,102,103,118,119,131,132,153,154,176,177,199,200,226,227,247,248,274,275,299,300,315,316,331,332,352,353,378,379,394,395,407,408,427,428,447,448],"andy-schoen",{"id":11,"data":13,"body":17,"filePath":42,"assetImports":43,"digest":46,"deferredRender":31},{"id":11,"name":14,"displayName":15,"title":16,"bio":17,"expertise":18,"affiliations":27,"email":32,"links":33,"images":36,"visibility":39,"featured":40,"lastUpdated":41},"Andy Schoen","Dr. Andy Schoen","Interaction Designer and Frontend Developer","Andy Schoen is an interaction designer and frontend developer working at Semio AI. He completed his Ph.D. in Computer Science at the University of Wisconsin - Madison, working with Dr. Bilge Mutlu on human-robot interaction. His research focused on developing systems for specifying and designing robot behaviors. Andy is passionate about creating intuitive tools that empower users to effectively design and interact with robotic systems.",[19,20,21,22,23,24,25,26],"Software Engineering","System Architecture","Robotics Software","Full-Stack Development","Authoring Tools","User Experience Design","Open Source Development","Human-Robot Interaction",[28],{"partnerId":29,"role":30,"current":31},"semio-ai","Designer & Developer",true,"andy@semio.ai",{"website":34,"github":35},"https://andrewjschoen.github.io","andrewjschoen",{"avatar":37,"hero":38},"__ASTRO_IMAGE_@/assets/images/people/andy-schoen.jpg","__ASTRO_IMAGE_@/assets/images/partners/semio-hero.png","public",false,["Date","2024-01-15T00:00:00.000Z"],"src/content/people/andy-schoen.mdx",[44,45],"@/assets/images/people/andy-schoen.jpg","@/assets/images/partners/semio-hero.png","9cface6f1531ec89","brian-scassellati",{"id":47,"data":49,"body":70,"filePath":71,"digest":72,"deferredRender":31},{"id":47,"name":50,"displayName":51,"title":52,"bio":53,"expertise":54,"affiliations":63,"email":68,"visibility":39,"featured":31,"lastUpdated":69},"Brian Scassellati","Prof. Brian Scassellati","A. Bartlett Giamatti Professor of Computer Science","Brian Scassellati is the A. Bartlett Giamatti Professor of Computer Science at Yale University and Director of the Yale Social Robotics Lab. A pioneer in social robotics, his research focuses on building embodied computational models of human social behavior, especially for assistive robotics applications in autism therapy, education, and rehabilitation.",[55,26,56,57,58,59,60,61,62],"Social Robotics","Autism Therapy Robotics","Developmental Robotics","Cognitive Science","Machine Learning","Computer Vision","Assistive Technology","Educational Robotics",[64],{"partnerId":65,"role":66,"department":67,"current":31},"yale-university","Professor","Computer Science","brian.scassellati@yale.edu",["Date","2024-01-15T00:00:00.000Z"],"Brian Scassellati is the A. Bartlett Giamatti Professor of Computer Science at Yale University and Director of the Yale Social Robotics Lab. A pioneer in social robotics, his research focuses on building embodied computational models of human social behavior, especially for assistive robotics applications in autism therapy, education, and rehabilitation.\n\nWith over two decades of experience in the field, Brian has made fundamental contributions to our understanding of how robots can perceive, understand, and respond to human social cues. His work on robots for autism therapy has been particularly influential, developing systems that can engage children with autism spectrum disorders in therapeutic interactions that improve social skills and communication.\n\nAs the director of the Yale Social Robotics Lab, he leads a multidisciplinary team exploring questions at the intersection of robotics, artificial intelligence, and human cognition. His research has resulted in numerous breakthroughs in areas such as joint attention, theory of mind for robots, and the development of socially assistive robots that can adapt to individual users' needs.\n\nBrian is a Fellow of the American Association for the Advancement of Science (AAAS) and has received numerous awards for his contributions to robotics and human-robot interaction. He is deeply committed to translating research into real-world applications that can improve quality of life for individuals with developmental and cognitive differences.","src/content/people/brian-scassellati.mdx","4017af65df6c92f5","chris-birmingham",{"id":73,"data":75,"body":79,"filePath":98,"assetImports":99,"digest":101,"deferredRender":31},{"id":73,"name":76,"displayName":77,"title":78,"bio":79,"expertise":80,"affiliations":89,"email":92,"links":93,"images":95,"visibility":39,"featured":40,"lastUpdated":97},"Chris Birmingham","Dr. Chris Birmingham","Human-Robot Interaction Researcher and Software Engineer","Chris Birmingham is a human-robot interaction researcher and software engineer at Semio AI. He earned a Ph.D. in Computer Science from the University of Southern California in March 2023 with a concentration in Socially Assistive Robotics. His dissertation, Multiparty Human-Robot Interaction: Methods For Facilitating Social Support, developed computational models of group dynamics (e.g., turn-taking and trust) and a facilitation framework grounded in empathy and disclosure. Chris builds open-source tools for reproducible HRI, including HCI-FACE (a web-based facial animation and conversation engine) and HARMONI (a ROS-based modular interaction framework). His work spans multimodal perception, time-series modeling for affect recognition, and end-to-end deployment across cloud and robot platforms.",[26,81,82,83,84,85,86,60,87,88,22,25],"Socially Assistive Robotics","Multimodal Perception","Affective Computing","Conversational Interfaces","Time Series Modeling","Natural Language Processing","Speech Processing","ROS & Robotics Software",[90],{"partnerId":29,"role":91,"current":31},"Researcher & Engineer","chris@semio.ai",{"github":94},"chrismbirmingham",{"avatar":96,"hero":38},"__ASTRO_IMAGE_@/assets/images/people/chris-birmingham.webp",["Date","2025-10-09T00:00:00.000Z"],"src/content/people/chris-birmingham.mdx",[100,45],"@/assets/images/people/chris-birmingham.webp","8d3a8829b3f701f2","kayla-matheus",{"id":102,"data":104,"body":115,"filePath":116,"digest":117,"deferredRender":31},{"id":102,"name":105,"displayName":105,"title":106,"bio":107,"expertise":108,"affiliations":111,"email":113,"visibility":39,"featured":40,"lastUpdated":114},"Kayla Matheus","Ph.D. Candidate","Kayla Matheus is a Ph.D. Candidate at Yale University, working in the Social Robotics Lab. Her research focuses on human-robot interaction, social robotics, and developing technologies that can effectively support human social and cognitive development.",[26,55,109,62,110,61],"Child-Robot Interaction","Cognitive Development",[112],{"partnerId":65,"role":106,"department":67,"current":31},"kayla.matheus@yale.edu",["Date","2024-01-15T00:00:00.000Z"],"Kayla Matheus is a Ph.D. Candidate at Yale University, working in the Social Robotics Lab under the direction of Brian Scassellati. Her research focuses on human-robot interaction, social robotics, and developing technologies that can effectively support human social and cognitive development.\n\nHer work involves designing and evaluating robotic systems for educational and therapeutic applications, with particular emphasis on supporting children with developmental differences and creating inclusive technologies that can adapt to diverse user needs.","src/content/people/kayla-matheus.mdx","a3fe0479082a346f","andrea-alcorn",{"id":118,"data":120,"body":128,"filePath":129,"digest":130,"deferredRender":31},{"id":118,"name":121,"affiliations":122,"email":126,"visibility":39,"featured":40,"draft":31,"lastUpdated":127},"Andrea Alcorn",[123],{"partnerId":124,"role":125,"current":31},"ologic-inc","Team Member","andrea@ologicinc.com",["Date","2024-01-15T00:00:00.000Z"],"Andrea Alcorn is a key team member at OLogic, Inc., contributing to the development and support of robotics hardware and software solutions for the Semio Community.","src/content/people/andrea-alcorn.mdx","ce240eff4cdcaa2f","maja-mataric",{"id":131,"data":133,"body":150,"filePath":151,"digest":152,"deferredRender":31},{"id":131,"name":134,"displayName":135,"title":136,"bio":137,"expertise":138,"affiliations":144,"email":148,"visibility":39,"featured":31,"lastUpdated":149},"Maja MatariÄ","Prof. Maja J MatariÄ","Chan Soon-Shiong Distinguished Professor of Computer Science","Maja MatariÄ is the Chan Soon-Shiong Distinguished Professor of Computer Science at the University of Southern California, where she leads pioneering research in socially assistive robotics, human-robot interaction, and multi-robot systems.",[81,26,139,140,141,142,143],"Multi-Robot Systems","Rehabilitation Robotics","Robotics for Healthcare","Machine Learning for HRI","Embodied AI",[145],{"partnerId":146,"role":147,"department":67,"current":31},"university-of-southern-california","Chan Soon-Shiong Distinguished Professor","mataric@usc.edu",["Date","2024-01-15T00:00:00.000Z"],"Maja MatariÄ is the Chan Soon-Shiong Distinguished Professor of Computer Science at the University of Southern California, where she leads pioneering research in socially assistive robotics, human-robot interaction, and multi-robot systems.\n\nAs the founding director of the USC Robotics and Autonomous Systems Center (RASC) and co-director of the USC Robotics Research Lab, she has made fundamental contributions to the field of socially assistive robotics, developing robot-assisted therapies for autism spectrum disorders, stroke rehabilitation, Alzheimer's disease, and healthy aging.\n\nHer work bridges computer science, neuroscience, and rehabilitation, creating innovative robotic systems that provide personalized assistance and motivation to users through social interaction rather than physical contact. She is a Fellow of multiple prestigious organizations including the AAAS, IEEE, AAAI, and ACM.","src/content/people/maja-mataric.mdx","3a3b04e5dc02f01d","jeremy-marvel",{"id":153,"data":155,"body":173,"filePath":174,"digest":175,"deferredRender":31},{"id":153,"name":156,"displayName":157,"title":158,"bio":159,"expertise":160,"affiliations":167,"email":171,"visibility":39,"featured":40,"draft":31,"lastUpdated":172},"Jeremy Marvel","Dr. Jeremy Marvel","Project Leader","Jeremy Marvel is a Project Leader at the National Institute of Standards and Technology (NIST), where he leads research initiatives in human-robot collaboration, robotics safety standards, and performance metrics for robotic systems.",[161,162,163,164,165,166],"Human-Robot Collaboration","Robotics Safety Standards","Performance Metrics","Industrial Robotics","Standards Development","Test Methods",[168],{"partnerId":169,"role":158,"department":170,"current":31},"nist","Intelligent Systems Division","jeremy.marvel@nist.gov",["Date","2024-01-15T00:00:00.000Z"],"Jeremy Marvel is a Project Leader at the National Institute of Standards and Technology (NIST), where he leads research initiatives in human-robot collaboration, robotics safety standards, and performance metrics for robotic systems.\n\nHis work focuses on developing test methods and metrics for evaluating the performance and safety of collaborative robot systems, contributing to international standards development, and advancing the state of human-robot interaction in manufacturing and industrial settings.","src/content/people/jeremy-marvel.mdx","f4bc0b8c499ba9ee","bill-smart",{"id":176,"data":178,"body":196,"filePath":197,"digest":198,"deferredRender":31},{"id":176,"name":179,"displayName":180,"title":66,"bio":181,"expertise":182,"affiliations":190,"email":194,"visibility":39,"featured":40,"lastUpdated":195},"Bill Smart","Prof. Bill Smart","Bill Smart is a Professor at Oregon State University, specializing in robotics, machine learning, and human-robot interaction. With decades of experience in the field, he has made significant contributions to mobile robotics, robot learning, and the development of practical robotic systems for real-world applications.",[183,184,26,185,186,187,188,189],"Mobile Robotics","Machine Learning for Robotics","Robot Navigation","Reinforcement Learning","Field Robotics","Robot Software Architecture","Assistive Robotics",[191],{"partnerId":192,"role":66,"department":193,"current":31},"oregon-state-university","School of Mechanical, Industrial, and Manufacturing Engineering","bill.smart@oregonstate.edu",["Date","2024-01-15T00:00:00.000Z"],"Bill Smart is a Professor at Oregon State University, specializing in robotics, machine learning, and human-robot interaction. With decades of experience in the field, he has made significant contributions to mobile robotics, robot learning, and the development of practical robotic systems for real-world applications.\n\nHis research spans a wide range of topics including autonomous navigation, reinforcement learning for robotics, and the development of robust software architectures for complex robotic systems. Bill has been instrumental in advancing the state of mobile robotics and has worked extensively on making robots more capable of operating in unstructured, real-world environments.\n\nAs an educator and mentor, Bill is committed to training the next generation of roboticists and has supervised numerous graduate students who have gone on to make their own significant contributions to the field. His practical approach to robotics research emphasizes building systems that work reliably outside the laboratory, making robotics technology more accessible and useful for society.","src/content/people/bill-smart.mdx","2337647bb2e6a666","mark-yim",{"id":199,"data":201,"body":223,"filePath":224,"digest":225,"deferredRender":31},{"id":199,"name":202,"displayName":203,"title":204,"bio":205,"expertise":206,"affiliations":216,"email":221,"visibility":39,"featured":40,"draft":31,"lastUpdated":222},"Mark Yim","Prof. Mark Yim","Asa Whitney Professor of Mechanical Engineering","Mark Yim is the Asa Whitney Professor of Mechanical Engineering at the University of Pennsylvania and Director of the GRASP Lab. A pioneer in modular robotics, his research focuses on the design and control of modular self-reconfigurable robots, biologically inspired mechanisms, and novel robotic systems.",[207,208,209,210,211,212,213,214,215],"Modular Robotics","Self-Reconfigurable Robots","Bio-inspired Robotics","Mechanical Design","Robot Kinematics","Distributed Systems","Soft Robotics","Medical Robotics","Flying Robots",[217],{"partnerId":218,"role":219,"department":220,"current":31},"university-of-pennsylvania","Asa Whitney Professor","Mechanical Engineering and Applied Mechanics","yim@seas.upenn.edu",["Date","2024-01-15T00:00:00.000Z"],"Mark Yim is the Asa Whitney Professor of Mechanical Engineering at the University of Pennsylvania and Director of the GRASP Lab. A pioneer in modular robotics, his research focuses on the design and control of modular self-reconfigurable robots, biologically inspired mechanisms, and novel robotic systems.\n\nHis groundbreaking work in modular robotics has established fundamental principles for how robots can be built from standardized, interchangeable modules that can self-assemble and reconfigure to accomplish different tasks. This work has profound implications for creating adaptable, resilient robotic systems that can morph their shape and functionality to meet changing requirements.\n\nBeyond modular robotics, Mark's research spans a diverse range of innovative robotic systems, from snake-like robots that can navigate through rubble for search and rescue, to flying robots with novel propulsion mechanisms, to soft robots for medical applications. His approach combines rigorous mechanical design principles with creative, bio-inspired solutions to create robots that push the boundaries of what's possible.\n\nAs Director of the GRASP Lab, one of the world's leading robotics research centers, Mark fosters an environment of interdisciplinary collaboration and innovation. His mentorship has produced numerous leaders in the robotics field, and his work continues to influence the development of next-generation robotic systems across academia and industry.","src/content/people/mark-yim.mdx","ebbc56c6ffd5c3a4","megan-zimmerman",{"id":226,"data":228,"body":244,"filePath":245,"digest":246,"deferredRender":31},{"id":226,"name":229,"displayName":230,"title":231,"bio":232,"expertise":233,"affiliations":240,"email":242,"visibility":39,"featured":40,"draft":31,"lastUpdated":243},"Megan Zimmerman","Dr. Megan Zimmerman","Research Engineer","Megan Zimmerman is a Research Engineer at the National Institute of Standards and Technology (NIST), where she contributes to the development of measurement science, standards, and test methods for robotics and automation systems.",[234,235,163,161,236,237,238,239],"Robotics Standards","Test Methods Development","Manufacturing Robotics","Measurement Science","Automation Systems","Safety Standards",[241],{"partnerId":169,"role":231,"department":170,"current":31},"megan.zimmerman@nist.gov",["Date","2024-01-15T00:00:00.000Z"],"Megan Zimmerman is a Research Engineer at the National Institute of Standards and Technology (NIST), where she contributes to the development of measurement science, standards, and test methods for robotics and automation systems.\n\nHer work focuses on establishing rigorous, reproducible methods for evaluating robotic systems, particularly in manufacturing and collaborative environments. She plays a key role in developing standards that ensure the safety, reliability, and performance of robots working alongside humans.\n\nThrough her research at NIST, Megan helps bridge the gap between academic research and industrial applications, creating practical standards and metrics that enable the safe and effective deployment of advanced robotic systems in real-world settings. Her contributions support the broader goal of accelerating the adoption of robotics technologies while maintaining the highest standards of safety and performance.","src/content/people/megan-zimmerman.mdx","72f28f05373e00c7","ross-mead",{"id":247,"data":249,"body":269,"filePath":270,"assetImports":271,"digest":273,"deferredRender":31},{"id":247,"name":250,"displayName":251,"title":252,"bio":253,"expertise":254,"affiliations":261,"email":263,"links":264,"images":266,"visibility":39,"featured":31,"lastUpdated":268},"Ross Mead","Dr. Ross Mead","CEO & Founder","Ross Mead is the CEO and Founder of Semio AI, and a driving force behind the Semio Community initiative. With a Ph.D. in Computer Science from USC, he has dedicated his career to advancing human-robot interaction and making social robotics accessible to researchers, educators, and developers worldwide.",[26,55,255,256,257,258,259,260],"Proxemics in HRI","Multi-modal Interaction","Robot Behavior Design","Open Source Robotics","Reproducible Research","Community Building",[262],{"partnerId":29,"role":252,"current":31},"ross@semio.ai",{"linkedin":265},"rossmead",{"avatar":267,"hero":38},"__ASTRO_IMAGE_@/assets/images/people/ross-mead.jpg",["Date","2024-01-15T00:00:00.000Z"],"Ross Mead is the CEO and Founder of Semio AI, and a driving force behind the Semio Community initiative. With a Ph.D. in Computer Science from the University of Southern California, he has dedicated his career to advancing human-robot interaction and making social robotics accessible to researchers, educators, and developers worldwide.\n\nHis research focuses on understanding and modeling the social dynamics of human-robot interaction, particularly in the areas of proxemics (spatial relationships) and multi-modal communication. Through Semio, he works to democratize access to social robotics technologies and foster reproducible, replicable research in the HRI community.\n\nAs a leader in the field, Ross is committed to building bridges between academia and industry, promoting open science practices, and creating sustainable infrastructure for the next generation of robotics researchers and practitioners. His vision for the Semio Community encompasses not just technology development, but the cultivation of an inclusive, collaborative ecosystem that advances the entire field of human-centered robotics.","src/content/people/ross-mead.mdx",[272,45],"@/assets/images/people/ross-mead.jpg","43e67fa1460f1a7f","saad-elbeleidy",{"id":274,"data":276,"body":279,"filePath":294,"assetImports":295,"digest":298,"deferredRender":31},{"id":274,"name":277,"title":278,"bio":279,"expertise":280,"affiliations":283,"email":286,"links":287,"images":290,"visibility":39,"featured":31,"lastUpdated":293},"Saad Elbeleidy","Executive Director","Saad is the Executive Director of Peerbots, a U.S. based nonprofit organization providing a research-informed social robot platform for roboticists and non-roboticists. He received his Ph.D. from the Colorado School of Mines where he studied how therapists and educators use social robots in the wild. Now, at Peerbots, his research has expanded to broadly focus on the design and deployment of Social Robot End-User tools.",[26,55,281,282],"End-User Programming","Robotics for Therapy",[284],{"partnerId":285,"role":278,"current":31},"peerbots","saad@peerbots.org",{"website":288,"scheduling":289},"https://saad.phd","https://calendar.google.com/calendar/u/0/appointments/AcZssZ15Qjxw5v1lQu8FUPKjIiZW8K7obt8RxeqlAio=",{"avatar":291,"hero":292},"__ASTRO_IMAGE_@/assets/images/people/saad-elbeleidy.jpg","__ASTRO_IMAGE_@/assets/images/partners/peerbots-hero.png",["Date","2024-01-15T00:00:00.000Z"],"src/content/people/saad-elbeleidy.mdx",[296,297],"@/assets/images/people/saad-elbeleidy.jpg","@/assets/images/partners/peerbots-hero.png","fb1ac531eb2b3d9c","naomi-fitter",{"id":299,"data":301,"body":305,"filePath":313,"digest":314,"deferredRender":31},{"id":299,"name":302,"displayName":303,"title":304,"bio":305,"expertise":306,"affiliations":309,"email":311,"visibility":39,"featured":40,"draft":31,"lastUpdated":312},"Naomi Fitter","Prof. Naomi Fitter","Assistant Professor","Naomi Fitter is a faculty member at Oregon State University, specializing in human-robot interaction research. Her work focuses on developing socially assistive robots and studying how robots can effectively interact with and support humans in various contexts.",[26,81,307,308],"Physical Human-Robot Interaction","Robotics for Health and Wellness",[310],{"partnerId":192,"role":304,"department":193,"current":31},"naomi.fitter@oregonstate.edu",["Date","2024-01-15T00:00:00.000Z"],"src/content/people/naomi-fitter.mdx","9bfff40207db30cb","nicholas-houser",{"id":315,"data":317,"body":320,"filePath":329,"digest":330,"deferredRender":31},{"id":315,"name":318,"title":319,"bio":320,"expertise":321,"affiliations":324,"email":327,"visibility":39,"featured":40,"draft":31,"lastUpdated":328},"Nicholas Houser","Design Engineer","Nicholas Houser is a member of the IK Studio team, working on innovative robotics design and development projects that bridge the gap between artistic expression and technological functionality in human-robot interaction.",[322,26,210,323],"Robotics Design","Interactive Systems",[325],{"partnerId":326,"role":319,"current":31},"ik-studio","nick.houser@i-k-studio.com",["Date","2024-01-15T00:00:00.000Z"],"src/content/people/nicholas-houser.mdx","2c3e368b340ebae1","reuth-mirsky",{"id":331,"data":333,"body":349,"filePath":350,"digest":351,"deferredRender":31},{"id":331,"name":334,"displayName":335,"title":304,"bio":336,"expertise":337,"affiliations":344,"email":347,"visibility":39,"featured":40,"draft":31,"lastUpdated":348},"Reuth Mirsky","Prof. Reuth Mirsky","Reuth Mirsky is an Assistant Professor at Tufts University, specializing in artificial intelligence, multi-agent systems, and human-AI collaboration. Her research focuses on developing AI systems that can effectively understand, predict, and collaborate with humans in complex, dynamic environments.",[338,339,340,341,342,26,59,343],"Artificial Intelligence","Multi-Agent Systems","Human-AI Collaboration","Plan Recognition","Theory of Mind","Explainable AI",[345],{"partnerId":346,"role":304,"department":67,"current":31},"tufts-university","Reuth.Mirsky@tufts.edu",["Date","2024-01-15T00:00:00.000Z"],"Reuth Mirsky is an Assistant Professor at Tufts University, specializing in artificial intelligence, multi-agent systems, and human-AI collaboration. Her research focuses on developing AI systems that can effectively understand, predict, and collaborate with humans in complex, dynamic environments.\n\nHer work bridges the gap between theoretical AI and practical applications in human-robot interaction, with particular emphasis on plan recognition, theory of mind modeling, and creating AI agents that can adapt to and work alongside human partners. She is passionate about making AI systems more transparent, trustworthy, and effective in real-world collaborative scenarios.\n\nPrior to joining Tufts, she completed her Ph.D. at Ben-Gurion University and held postdoctoral positions at UT Austin and Harvard University, building a strong interdisciplinary foundation that informs her approach to human-centered AI and robotics research.","src/content/people/reuth-mirsky.mdx","ca6b06b896322685","simon-kim",{"id":352,"data":354,"body":375,"filePath":376,"digest":377,"deferredRender":31},{"id":352,"name":355,"displayName":356,"title":357,"bio":358,"expertise":359,"affiliations":365,"email":371,"links":372,"visibility":39,"featured":40,"lastUpdated":374},"Simon Kim","Prof. Simon Kim","Professor of Practice in Architecture / Principal","Simon Kim is a multidisciplinary researcher and designer with dual affiliations at the University of Pennsylvania's Weitzman School of Design and IK Studio. His work explores the intersection of architecture, robotics, and human-robot interaction, focusing on how robotic systems can be integrated into built environments and everyday spaces.",[360,361,362,26,363,364],"Architectural Robotics","Interactive Design","Spatial Computing","Digital Fabrication","Responsive Environments",[366,369],{"partnerId":218,"role":367,"department":368,"current":31},"Professor of Practice in Architecture","Weitzman School of Design",{"partnerId":326,"role":370,"current":31},"Principal","simonkim@design.upenn.edu",{"website":373},"https://www.i-k-studio.com",["Date","2024-01-15T00:00:00.000Z"],"Simon Kim is a multidisciplinary researcher and designer with dual affiliations at the University of Pennsylvania's Weitzman School of Design and IK Studio. His work explores the intersection of architecture, robotics, and human-robot interaction, focusing on how robotic systems can be integrated into built environments and everyday spaces.\n\nAt UPenn, he conducts research on spatial robotics and interactive design, while at IK Studio, he leads innovative projects that combine artistic vision with technological advancement in social robotics.","src/content/people/simon-kim.mdx","d46c868587c6057a","steve-goodgame",{"id":378,"data":380,"body":382,"filePath":392,"digest":393,"deferredRender":31},{"id":378,"name":381,"title":278,"bio":382,"expertise":383,"affiliations":387,"email":390,"visibility":39,"featured":40,"draft":31,"lastUpdated":391},"Steve Goodgame","Steve Goodgame is a leader at the KISS Institute for Practical Robotics (KIPR), where he works to advance educational robotics and make robotics technology accessible to students and educators worldwide.",[62,384,385,386],"STEM Education","Robotics Competitions","Curriculum Development",[388],{"partnerId":389,"role":278,"current":31},"kipr","sgoodgame@kipr.org",["Date","2024-01-15T00:00:00.000Z"],"src/content/people/steve-goodgame.mdx","5e991d83589a4d09","shelly-bagchi",{"id":394,"data":396,"body":404,"filePath":405,"digest":406,"deferredRender":31},{"id":394,"name":397,"displayName":398,"affiliations":399,"email":402,"visibility":39,"featured":40,"draft":31,"lastUpdated":403},"Shelly Bagchi","Dr. Shelly Bagchi",[400],{"partnerId":169,"role":401,"current":31},"Researcher","shelly.bagchi@nist.gov",["Date","2024-01-15T00:00:00.000Z"],"Shelly Bagchi is a researcher at the National Institute of Standards and Technology (NIST), contributing to standards development and research initiatives in human-robot interaction and robotics technologies.","src/content/people/shelly-bagchi.mdx","9a51cea1f8fbd8f8","ted-larson",{"id":407,"data":409,"body":424,"filePath":425,"digest":426,"deferredRender":31},{"id":407,"name":410,"displayName":410,"title":411,"bio":412,"expertise":413,"affiliations":420,"email":422,"visibility":39,"featured":40,"draft":31,"lastUpdated":423},"Ted Larson","CEO","Ted Larson is the CEO of OLogic, Inc., where he oversees the development and manufacturing of robotics hardware platforms. With extensive experience in embedded systems and robotics engineering, Ted plays a key role in bringing research robots like Quori from concept to production.",[414,415,416,417,418,419],"Robotics Engineering","Embedded Systems","Manufacturing","Hardware Development","Product Development","Systems Integration",[421],{"partnerId":124,"role":411,"current":31},"ted@ologicinc.com",["Date","2024-01-15T00:00:00.000Z"],"Ted Larson is the CEO of OLogic, Inc., where he oversees the development and manufacturing of robotics hardware platforms. With extensive experience in embedded systems and robotics engineering, Ted plays a key role in bringing research robots like Quori from concept to production, ensuring they meet the needs of the research community while maintaining manufacturability and reliability.\n\nOLogic has been instrumental in the development and production of several key robotics platforms for the Semio Community, including the Quori social robot platform.","src/content/people/ted-larson.mdx","c11e6b0ec8e595cc","xuesu-xiao",{"id":427,"data":429,"body":444,"filePath":445,"digest":446,"deferredRender":31},{"id":427,"name":430,"displayName":431,"title":304,"bio":432,"expertise":433,"affiliations":439,"email":442,"visibility":39,"featured":40,"draft":31,"lastUpdated":443},"Xuesu Xiao","Prof. Xuesu Xiao","Xuesu Xiao is an Assistant Professor at George Mason University, specializing in mobile robotics, autonomous navigation, and learning-based approaches for robot decision-making. His research focuses on enabling robots to navigate complex, unstructured environments through innovative machine learning and planning techniques.",[183,434,184,435,436,437,187,186,438],"Autonomous Navigation","Motion Planning","Terrain Traversability","Robot Learning","Computer Vision for Robotics",[440],{"partnerId":441,"role":304,"department":67,"current":31},"george-mason-university","xiao@gmu.edu",["Date","2024-01-15T00:00:00.000Z"],"Xuesu Xiao is an Assistant Professor at George Mason University, specializing in mobile robotics, autonomous navigation, and learning-based approaches for robot decision-making. His research focuses on enabling robots to navigate complex, unstructured environments through innovative machine learning and planning techniques.\n\nHis work addresses fundamental challenges in mobile robotics, including terrain traversability analysis, adaptive navigation in challenging environments, and learning-based approaches that allow robots to improve their performance through experience. By combining classical robotics techniques with modern machine learning methods, his research aims to create more capable and adaptable autonomous systems.\n\nAt George Mason University, Xuesu leads research initiatives that bridge the gap between theoretical advances in robot learning and practical applications in real-world environments. His contributions to the field include novel approaches for terrain-aware navigation, learning-based motion planning, and systems that can adapt to diverse and dynamic environments. He is actively involved in the robotics community and collaborates with researchers across academia and industry to advance the state of autonomous mobile robotics.","src/content/people/xuesu-xiao.mdx","517127b668c6e9a2","dylan-thomas-doyle",{"id":447,"data":449,"body":472,"filePath":473,"assetImports":474,"digest":476,"deferredRender":31},{"id":447,"name":450,"displayName":451,"title":452,"bio":453,"expertise":454,"affiliations":462,"images":469,"visibility":39,"featured":40,"lastUpdated":471},"Dylan Thomas Doyle","Dr. Dylan Thomas Doyle","Post-Doctoral Researcher","Dylan Thomas Doyle is a post-doctoral researcher at the University of Colorado Boulder and the 2025 Peerbots Research Fellow. As an HRI researcher, Dylan's work focuses on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity.",[26,455,456,457,458,459,460,461],"Values-Based Design","Robot Identity Perception","Socio-Technical Systems","Qualitative Research","Speculative Design","Expressive Robotics","Robot Facial Expression",[463,465],{"partnerId":464,"role":452,"current":31},"university-of-colorado-boulder",{"partnerId":285,"role":466,"department":467,"startDate":468,"current":31},"Research Fellow","Research Fellowship Program",["Date","2025-01-01T00:00:00.000Z"],{"avatar":470},"__ASTRO_IMAGE_@/assets/images/people/dylan-thomas-doyle.png",["Date","2024-12-01T00:00:00.000Z"],"Dylan Thomas Doyle is a post-doctoral researcher at the University of Colorado Boulder and the 2025 Peerbots Research Fellow. As an HRI researcher, Dylan's work focuses on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity.\n\n## Research Focus\n\nDylan's fellowship research conducts a qualitative study examining the adoption of expressive faces for humanoid robots in industry and academia, with particular attention to systems like the Peerbots face. The study employs speculative design methods to understand the considerations and needs that decision-makers take into account when determining the facial expression capabilities of the robots they are designing.\n\n## Background\n\nOutside of research, Dylan serves as the Director of the AI for All Tomorrows media collective and podcast, bringing together perspectives on technology and its societal impacts.\n\nDylan received his PhD from the University of Colorado Boulder, Masters of Divinity from Columbia University, and BA from Sarah Lawrence College. This unique interdisciplinary background combines technical expertise with deep humanistic training. Prior to a career in technology research, Dylan served as a Unitarian Universalist minister and hospital chaplain, bringing a distinctive perspective on human values and care to robotics research.\n\n## Peerbots Research Fellowship\n\nAs the 2025 Peerbots Research Fellow, Dylan is conducting research that aligns with Peerbots' mission to raise the floor of Human-Robot Interaction research and center people in how robots are researched, developed, and used. The fellowship supports completion of at least one study and submission of a publication to a leading academic venue.","src/content/people/dylan-thomas-doyle.mdx",[475],"@/assets/images/people/dylan-thomas-doyle.png","929a5f1bde4b99e4","hardware",["Map",479,480,564,565,627,628,687,688],"quoriv1",{"id":479,"data":481,"body":559,"filePath":560,"assetImports":561,"digest":563,"deferredRender":31},{"name":482,"description":483,"shortDescription":484,"category":485,"status":486,"specifications":487,"features":503,"applications":514,"researchAreas":522,"links":525,"images":528,"contributors":530,"leadOrganization":218,"supportingOrganizations":547,"tags":548,"featured":40,"publishDate":558},"Quori v1","A modular, affordable socially interactive robot platform developed for enabling human-robot interaction research. Features an expressive projected face, gesturing arms with shoulder-like articulation, flexible spine, and omnidirectional mobility.","Modular social robot platform for HRI research with projected face and expressive gestures","social","deprecated",{"height":488,"weight":489,"battery":490,"sensors":491,"actuators":497,"computePlatform":502},"1.35 meters (resting position)","Approx. 45-50 kg","Onboard rechargeable battery system",[492,493,494,495,496],"Depth camera","RGB camera","Microphone array","Touch sensors","Proximity sensors",[498,499,500,501],"2 DOF shoulder joints per arm","Omnidirectional base motors","Spine articulation motors","Turret rotation motor","Intel NUC with ROS integration",[504,505,506,507,508,509,510,511,512,513],"Rear-projected animated face for flexible expression design","Two gesturing arms with shoulder-like ball joint articulation (2 DOF each)","Bowing spine mechanism for body language expression","Omnidirectional mobile base (0.8 m/s linear, 180Â°/s rotational)","Modular panelized design with magnetic attachment system","ROS-based control interfaces at multiple abstraction levels","Browser-based content creation and animation tools","Built-in text-to-speech capabilities","ADA-compliant base dimensions","Low noise operation (quiet at 1 meter distance)",[515,516,517,518,519,520,521],"Human-robot interaction studies","Social robotics research","Non-contact interaction experiments","In-lab behavioral studies","Field deployment research","Multi-modal communication research","Gesture and expression studies",[26,55,523,338,524,323,143],"Non-verbal Communication","Behavioral Computing",{"documentation":526,"website":527},"https://quori-robot.github.io/quori_v1_documentation/","https://quori.org",{"hero":529},"__ASTRO_IMAGE_@/assets/images/hardware/quori.v1-hero.jpg",[531,534,537,539,541,543,545],{"type":532,"id":218,"role":533,"current":40},"organization","Lead Development",{"type":535,"id":199,"role":536,"current":40},"person","Principal Investigator",{"type":532,"id":146,"role":538,"current":40},"Co-Development Partner",{"type":535,"id":131,"role":540,"current":40},"Co-Principal Investigator",{"type":535,"id":352,"role":542,"current":40},"Design Lead",{"type":532,"id":29,"role":544,"current":31},"Platform Support",{"type":535,"id":247,"role":546,"current":31},"Software Architecture",[146,29],[549,550,551,552,553,554,555,556,557],"social-robotics","hri","research-platform","modular","open-hardware","ros","projected-face","gesture","mobile",["Date","2024-01-15T00:00:00.000Z"],"## Overview\n\nQuori is an affordable, modular social robot platform developed through a National Science Foundation initiative to democratize human-robot interaction research. Created through extensive community consultation, the robot features a distinctive rear-projected animated face for flexible expression design, two articulated arms with shoulder-like movement, a bowing spine mechanism, and an omnidirectional mobile base capable of smooth navigation in both laboratory and real-world settings. The platform's modular architecture allows researchers to customize and extend capabilities while maintaining standardization across the ten units distributed to U.S. research institutions.\n\nThe robot bridges technical accessibility gaps by providing control interfaces at multiple levels - from low-level ROS commands for direct hardware control to browser-based tools for creating conversational content and animations without extensive programming expertise. This tiered approach, combined with open-source hardware documentation and swappable components, enables diverse research applications ranging from non-verbal communication studies to field deployments in public spaces, all while maintaining ADA compliance and quiet operation suitable for human interaction contexts.\n\n## Research Publications\n\n- Specian, A., Eckenstein, N., Mead, R., McDorman, B., Kim, S., Mataric, M., & Yim, M. (2018). **Preliminary system and hardware design for Quori, a low-cost, modular, socially interactive robot.** *2018 HRI Workshop Social Robots in the Wild*, 1-6.","src/content/hardware/quori.v1.mdx",[562],"@/assets/images/hardware/quori.v1-hero.jpg","130d933549914287","quoriv2",{"id":564,"data":566,"body":622,"filePath":623,"assetImports":624,"digest":626,"deferredRender":31},{"name":567,"description":568,"shortDescription":569,"category":485,"status":570,"specifications":571,"features":583,"applications":595,"researchAreas":596,"links":597,"images":598,"contributors":600,"leadOrganization":192,"supportingOrganizations":616,"tags":617,"featured":31,"publishDate":621},"Quori v2","An advanced modular socially interactive robot platform with enhanced sensing and interaction capabilities. Features a touchscreen display, dual speakers, 14-DOF articulation including 4-DOF arms, multiple IMUs and laser rangefinders, programmable light arrays, and 3-DOF holonomic mobility.","Advanced modular social robot with touchscreen, enhanced sensors, and 14-DOF articulation","in-progress",{"height":488,"weight":489,"battery":490,"sensors":572,"actuators":576,"computePlatform":582},[573,494,574,575],"RGB+D camera","Laser rangefinders (x2)","IMUs (x4)",[577,578,579,580,581],"14 DOFs total","2-DOF head","1-DOF neck","4-DOF arms (x2)","3-DOF holonomic base","Onboard computer",[584,585,586,587,588,589,590,591,592,593,594,509,510],"Touchscreen display for interactive communication","Dual speakers for enhanced audio output","Two articulated arms with 4 DOF each for complex gestures","2-DOF head with 1-DOF neck for expressive movements","3-DOF holonomic base for smooth omnidirectional mobility","Dual laser rangefinders for precise navigation","Four IMUs for enhanced motion sensing and stability","Four programmable light arrays for visual feedback","Onboard storage bin for carrying items","Customizable badge for personalization","Enhanced modular design and functionality",[515,516,517,518,519,520,521],[26,55,523,338,524,323,143],{"documentation":526,"website":527},{"hero":599},"__ASTRO_IMAGE_@/assets/images/hardware/quori.v2-hero.png",[601,603,605,607,609,611,612,614,615],{"type":535,"id":199,"role":602,"current":40},"Principle Investigator",{"type":532,"id":192,"role":604,"current":31},"Community Development",{"type":532,"id":124,"role":606,"current":31},"Design For Manufacturing",{"type":532,"id":326,"role":608,"current":31},"Industrial Design",{"type":535,"id":176,"role":610,"current":31},"Co-Investigator",{"type":535,"id":299,"role":610,"current":31},{"type":532,"id":29,"role":613,"current":31},"Software Development",{"type":535,"id":247,"role":610,"current":31},{"type":535,"id":352,"role":610,"current":40},[29],[549,550,551,552,553,554,618,557,619,620],"touchscreen","multi-sensor","interactive-display",["Date","2024-01-15T00:00:00.000Z"],"## Overview\n\nQuori v2.0 represents a significant advancement in socially interactive robot platforms, building upon the successful foundation established by the National Science Foundation initiative. This enhanced version features a comprehensive sensor suite including an RGB+D camera, dual laser rangefinders, and four IMUs for superior environmental perception and stability. The robot's interaction capabilities have been dramatically expanded with a touchscreen display for direct user engagement, dual speakers for immersive audio experiences, and four programmable light arrays that enable rich visual communication patterns.\n\nThe mechanical design has evolved to offer 14 degrees of freedom, including sophisticated 4-DOF arms (8 DOF total) for complex gestural expression, a 2-DOF head with 1-DOF neck for nuanced non-verbal communication, and a 3-DOF holonomic base for smooth omnidirectional navigation. Additional features like the onboard storage bin enable practical applications in service scenarios, while the customizable badge system allows for personalization in multi-robot deployments. This modular architecture maintains backward compatibility while offering researchers unprecedented flexibility in customizing and extending the platform's capabilities for diverse human-robot interaction studies.\n\n## Research Publications\n\n- Specian, A., Eckenstein, N., Mead, R., McDorman, B., Kim, S., Mataric, M., & Yim, M. (2018). **Preliminary system and hardware design for Quori, a low-cost, modular, socially interactive robot.** *2018 HRI Workshop Social Robots in the Wild*, 1-6.","src/content/hardware/quori.v2.mdx",[625],"@/assets/images/hardware/quori.v2-hero.png","02d961c08448a850","ommie",{"id":627,"data":629,"body":682,"filePath":683,"assetImports":684,"digest":686,"deferredRender":31},{"name":630,"description":631,"shortDescription":632,"category":633,"status":570,"specifications":634,"features":647,"applications":654,"researchAreas":659,"links":663,"images":666,"contributors":668,"leadOrganization":65,"tags":673,"featured":31,"publishDate":681},"Ommie","A novel socially assistive robot designed to support deep breathing practices for anxiety reduction. Ommie uses haptic interaction and non-verbal cues to guide users through calming breathing exercises.","Social robot for anxiety reduction through guided deep breathing","assistive",{"height":635,"weight":636,"battery":637,"sensors":638,"actuators":643,"computePlatform":646},"Approx. 0.3 meters (tabletop size)","Approx. 2-3 kg","Powered via wall adapter",[639,640,641,642],"Capacitive touch","IMU","Motor encoders","Optional camera",[644,645],"Dynamixel MX-64AT (breathing motion)","Dynamixel AX-12A (head nodding)","Raspberry Pi",[648,649,650,651,652,653],"Haptic breathing guidance through physical expansion/contraction","Non-verbal interaction through eye animations and audio chimes","Soft sweater covering for comfortable touch","Body (breathing) and head motions (nodding)","Capacitive touch inputs","Multiple breathing patterns supported (customizable)",[655,656,657,658],"Anxiety reduction","Deep breathing","Stress management","Mental health support",[81,26,660,661,662],"Mental Health Technology","Haptic Interaction","Therapeutic Robotics",{"documentation":664,"website":665},"https://dl.acm.org/doi/10.1145/3706122","https://scazlab.yale.edu/ommie-robot",{"hero":667},"__ASTRO_IMAGE_@/assets/images/hardware/ommie-hero.png",[669,670,671],{"type":532,"id":65,"role":533,"current":31},{"type":535,"id":47,"role":536,"current":31},{"type":535,"id":102,"role":672,"current":31},"Research Lead",[674,675,676,677,678,679,680],"mental-health","therapeutic","haptic","social-robot","anxiety","breathing","wellness",["Date","2025-02-19T00:00:00.000Z"],"## Overview\n\nOmmie is a socially assistive robot developed at Yale University's Social Robotics Lab to help individuals manage anxiety through guided deep breathing exercises. The robot's core functionality centers on its unique haptic interaction design: it physically expands and contracts in a rhythmic breathing pattern while users place their hands on its soft, sweater-covered body. This mechanical breathing motion, combined with synchronized audio chimes and expressive eye animations, helps users naturally synchronize their own breathing to therapeutic deep breathing patterns that have been scientifically shown to calm the autonomic nervous system.\n\nInitial research with 43 participants at a university wellness center demonstrated Ommie's effectiveness, with users experiencing statistically significant reductions in anxiety state measures after interacting with the robot. Beyond the quantitative improvements, participants consistently rated the robot highly for its calming, approachable, and engaging qualities. Users particularly highlighted the focusing effect of the haptic interaction, reporting that the physical sensation helped them achieve a calmer state more quickly than traditional breathing exercises. Many participants also described experiencing a sense of companionship during the interaction, comparing it to the motivational benefits of group meditation. The robot has shown promise beyond adult anxiety management, with ongoing applications in pediatric healthcare settings where it has been successfully deployed to help children manage stress during oral allergy challenges. Recent work has also explored the application of machine learning algorithms to detect deep breathing phase for monitoring and adjusting deep breathing practices. Future work involves expanding use of the robot to new populations benefitting from deep breathing, as well as more longitudinal, in-the-wild experimentation.\n\n## Research Publications\n\n- **Beilenson, J., Sahin, R., Zhong, Y., Brewer, R., Tang, F., Nguyen, L., Juca, M., Kofinas, D., Nelson, J., Scassellati, B.** (2024). A Socially Assistive Robot to Help Children Cope with Medical Procedures: Exploring the Effect of the Robot's Embodiment. In Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI '24).\n\n- **Nelson, J., Sahin, R., & Scassellati, B.** (2023). Ommie: A Socially Assistive Robot for Deep Breathing. In Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction (HRI '23).","src/content/hardware/ommie.mdx",[685],"@/assets/images/hardware/ommie-hero.png","755745331b9f2fb6","jibo",{"id":687,"data":689,"body":735,"filePath":736,"assetImports":737,"digest":739,"deferredRender":31},{"name":690,"description":691,"shortDescription":692,"category":485,"status":570,"specifications":693,"features":703,"applications":713,"researchAreas":718,"links":721,"images":724,"contributors":726,"leadOrganization":727,"supportingOrganizations":728,"tags":729,"featured":40,"draft":31,"publishDate":734},"Jibo","A social robot platform originally developed as the world's first commercial social robot for the home. Features an expressive animated face on a touchscreen display, three-axis motor system for fluid movement, and interactive capabilities for family engagement.","Social robot with animated touchscreen face and expressive movement",{"height":694,"weight":695,"battery":696,"sensors":697,"actuators":700,"computePlatform":702},"11 inches (28 cm)","6 pounds (2.7 kg)","AC power adapter",[698,699,495],"2 color stereo cameras","360-degree microphone array",[701],"3 full-revolute axes motor system","Embedded Linux system",[704,705,706,707,708,709,710,711,712],"HD LCD touchscreen display for animated face","Three-axis motor system enabling fluid turns and movements","360-degree sound localization for voice interaction","Face recognition and tracking capabilities","Full-body touch sensing","WiFi and Bluetooth connectivity","Premium stereo speakers","Full spectrum ambient LED lighting","Jibo Alive SDK for developer applications",[714,516,515,715,716,717],"Family companion robot","Interactive storytelling","Video calling and telepresence","Home assistance",[55,719,338,720,83,59],"Human-Computer Interaction","Behavioral Science",{"documentation":722,"website":723},"https://hri2024.jibo.media.mit.edu","https://www.media.mit.edu/projects/jibo-research-platform/overview/",{"hero":725},"__ASTRO_IMAGE_@/assets/images/hardware/jibo-hero.jpg",[],"",[],[549,550,730,731,732,618,733],"family-robot","companion","mit","expressive",["Date","2024-03-01T00:00:00.000Z"],"## Overview\n\nJibo emerged from MIT Media Lab's Personal Robots Group under the direction of Cynthia Breazeal, pioneering the concept of a social robot designed specifically for home environments. The robot's distinctive design features a stationary base with a three-axis motor system that enables fluid, lifelike movements including turns and expressive gestures. Its animated face displayed on an HD touchscreen creates engaging social interactions through dynamic expressions and eye contact tracking, while 360-degree sound localization allows natural voice interactions from anywhere in the room.\n\nThe platform has evolved into the Jibo Research Platform, providing researchers with a deployable infrastructure for social robotics experimentation and data collection. This research-oriented version extends the original hardware and software architecture with enhanced capabilities for academic study, including improved data security measures and developer tools. The system's Linux-based architecture and accompanying SDK enable researchers and developers to create custom applications and behaviors, facilitating diverse studies in human-robot interaction and social robotics applications.\n\n## Research Publications\n\n- (2024). **Jibo Community Social Robot Research Platform @Scale.** *HRI 2024 Workshop/Tutorial*, Boulder, Colorado. MIT Media Lab.\n\n- Breazeal, C. et al. **Jibo: The World's First Social Robot for the Home.** *MIT Media Lab Personal Robots Group*.","src/content/hardware/jibo.mdx",[738],"@/assets/images/hardware/jibo-hero.jpg","20da193d78c3b98d","software",["Map",742,743,814,815],"arora",{"id":742,"data":744,"body":809,"filePath":810,"assetImports":811,"digest":813,"deferredRender":31},{"name":745,"description":746,"shortDescription":747,"category":748,"status":570,"license":749,"language":750,"platform":754,"requirements":758,"features":767,"useCases":776,"links":783,"images":787,"contributors":789,"leadOrganization":29,"tags":798,"featured":31,"lastUpdate":807,"publishDate":808},"Arora","A software-as-a-service platform for creating, deploying, and executing multimodal natural language-based applications for interactive personal robots and digital/virtual characters. Provides tools for voice scripting, animation, and custom functionality development.","SaaS platform for natural language robot applications","framework","MIT",[751,752,753],"Python","TypeScript","JavaScript",[755,756,757],"Linux","Docker","Cloud",{"runtime":759,"hardware":762,"dependencies":764},[760,761],"Node.js 18+","Python 3.8+",[763],"Compatible robot platform",[765,766],"ROS/ROS2 (optional)","WebRTC",[768,769,770,771,772,773,774,775],"Voice script creation for conversational HRI","Animation tools for expressive movements","JavaScript SDK for custom functionality","Real-time simulation and testing","Multi-robot fleet orchestration","Cloud-based deployment pipeline","Usage analytics and monitoring","Natural language interaction engine",[777,778,779,780,781,782],"Conversational robot applications","Multi-robot research studies","Interactive character development","Human-robot interaction experiments","Fleet coordination and management","Behavior animation and scripting",{"documentation":784,"github":785,"website":786},"https://arora.readthedocs.io","https://github.com/semio-community/arora","https://semio.ai",{"hero":788},"__ASTRO_IMAGE_@/assets/images/software/arora-hero.png",[790,792,794,796],{"type":532,"id":29,"role":791,"current":31},"Lead Developer",{"type":535,"id":247,"role":793,"current":31},"Product Visionary",{"type":535,"id":73,"role":795,"current":31},"Engineer",{"type":535,"id":11,"role":797,"current":31},"User Experience",[799,800,801,802,803,804,805,806],"orchestration","fleet-management","multi-robot","natural-language","saas","animation","voice","conversational-ai",["Date","2024-11-20T00:00:00.000Z"],["Date","2023-06-15T00:00:00.000Z"],"## Overview\n\nArora is Semio's comprehensive software-as-a-service platform designed to streamline the development and deployment of natural language-based applications for robots and virtual characters. Built on the principle that natural language should be the primary interface for human-robot interaction, Arora provides an integrated suite of tools that significantly reduces development time - aiming to cut content creation time by 60%, software integration by 75%, and product launch timelines by 90%.\n\nThe platform enables developers and researchers to rapidly prototype and deploy sophisticated robot behaviors through its three core modules: voice scripting for conversational interactions, animation tools for expressive movements, and a JavaScript SDK for custom functionality. This unified approach allows teams to create, test in simulation, deploy to hardware, and analyze usage data all within a single ecosystem, making it ideal for both research institutions conducting HRI studies and companies developing commercial robot applications.","src/content/software/arora.mdx",[812],"@/assets/images/software/arora-hero.png","43842dfaca0cc140","vizij",{"id":814,"data":816,"body":881,"filePath":882,"assetImports":883,"digest":885,"deferredRender":31},{"name":817,"description":818,"shortDescription":819,"category":748,"status":570,"license":749,"language":820,"platform":821,"requirements":825,"features":834,"useCases":846,"links":854,"images":856,"contributors":858,"leadOrganization":285,"supportingOrganizations":869,"tags":870,"featured":31,"lastUpdate":879,"publishDate":880},"Vizij","An open-source ecosystem of tools that provide a pipeline for building, animating, sharing, and deploying rendered robot faces. Defines a standardized-yet-modular, user-informed rigging and controller system including eye gaze, visemes, and emotional expression capabilities.","Open-source ecosystem for rendered robot faces",[751,753,752],[755,822,823,824],"Windows","macOS","Web",{"runtime":826,"hardware":828,"dependencies":830},[761,827],"Node.js 16+",[829],"Display device for robot face rendering",[831,832,833],"WebGL","Three.js","Blender (optional)",[835,836,837,838,839,840,841,842,843,844,845],"Standardized rigging system for robot faces","Face-agnostic abstraction layers","Built-in support for eye gaze control","Viseme generation for lip synchronization","Emotional expression mapping","Multi-screen face rendering support","Import/export for GLTF/GLB formats","Face designer and rig designer interfaces","Animation recording and playback","Programmatic control APIs","Community-standard rig templates",[847,848,849,850,851,852,853],"Human-robot interaction research","Expressive robot face development","Educational robotics with speech visualization","Commercial robot character design","Multi-robot face synchronization","Emotion expression studies","Gaze behavior research",{"github":855},"https://github.com/vizij-ai",{"logo":857},"__ASTRO_IMAGE_@/assets/images/software/vizij-icon.png",[859,861,862,864,866,868],{"type":535,"id":274,"role":860,"current":31},"Co-Lead Developer",{"type":535,"id":11,"role":860,"current":31},{"type":535,"id":73,"role":863,"current":31},"Core Developer",{"type":535,"id":247,"role":865,"current":31},"Project Advisor",{"type":532,"id":285,"role":867,"current":31},"Development Partner",{"type":532,"id":29,"role":867,"current":31},[29],[871,872,804,873,874,875,876,877,550,878],"robot-faces","rendering","rigging","gaze","visemes","emotion","expression","open-source",["Date","2024-12-01T00:00:00.000Z"],["Date","2024-09-01T00:00:00.000Z"],"## Overview\n\nVizij is an open-source ecosystem designed to address the fragmentation in robot face development by providing a unified platform for designing, animating, and deploying expressive rendered robot faces. Unlike existing solutions that require researchers to create custom, non-standard tools for each project, Vizij offers a comprehensive pipeline that enables code sharing, collaboration between diverse stakeholders, and the development of reusable components that can advance the entire field of human-robot interaction.\n\nAt its core, Vizij implements a data-flow architecture with multiple abstraction layers, allowing different rolesâfrom 3D designers to HRI researchersâto contribute their expertise without needing to understand the entire system. The platform provides standardized interfaces for three critical aspects of robot facial expression: emotion mapping, viseme generation for speech synchronization, and gaze control for joint attention and turn-taking behaviors.\n\n## Key Components\n\n### Face Properties & Rendering\nThe base layer handles the rendering of primitive shapes and elements that compose the robot face, supporting both single and multi-screen configurations. Faces can be imported from standard 3D formats (GLTF/GLB) created in tools like Blender, or composed using Vizij's built-in component system.\n\n### Rigging System\nVizij introduces a dual-layer rigging approach:\n- **Face-Specific Rigs**: Map high-level properties to low-level rendering values for individual face designs\n- **Abstract Rigs**: Provide face-agnostic control interfaces that work across different robot platforms\n\n### Standard Control Interfaces\nThe platform defines community standards for:\n- **Emotion Expression**: Supporting multiple emotion models including FACS, PAD, and layered approaches\n- **Visemes**: Enabling lip synchronization for educational and communication applications\n- **Gaze Systems**: Implementing joint attention, turn-taking, and intimacy regulation behaviors\n\n## Supported Roles\n\nVizij is designed to accommodate various stakeholder roles in the robot face development pipeline:\n\n- **Designers**: Create and modify face elements using existing 3D modeling tools\n- **Face Riggers**: Connect rig systems to specific face elements\n- **Abstraction Riggers**: Define face-agnostic control abstractions\n- **Animators**: Create behaviors and animations for face elements\n- **Interaction Designers**: Design the overall user experience and interaction flows\n- **Developers**: Programmatically control faces through well-defined APIs\n\n## Research Support\n\nDeveloped with support from NSF grants (IIS-2235042/2235043), Vizij enables replicable scientific studies by allowing researchers to share face assets, rigs, and animations. The standardized interfaces mean that code developed for one robot face can be easily adapted to work with others, significantly improving research reproducibility and accelerating development cycles.","src/content/software/vizij.mdx",[884],"@/assets/images/software/vizij-icon.png","d623033a4adba406","events",["Map",888,889],"hri-2025",{"id":888,"data":890,"body":933,"filePath":934,"digest":935,"deferredRender":31},{"name":891,"displayName":892,"description":893,"type":894,"format":895,"startDate":896,"endDate":897,"registrationDeadline":898,"location":899,"organizers":906,"speakers":907,"sponsors":908,"tracks":911,"topics":918,"links":926,"featured":31,"tags":930},"HRI 2025: ACM/IEEE International Conference on Human-Robot Interaction","HRI 2025","The 20th Annual ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for presenting and discussing cutting-edge research in human-robot interaction. HRI 2025 brings together researchers, practitioners, and industry leaders to share the latest advances in HRI theory, methods, technologies, and applications.","conference","hybrid",["Date","2025-03-04T00:00:00.000Z"],["Date","2025-03-06T00:00:00.000Z"],["Date","2025-02-15T00:00:00.000Z"],{"venue":900,"city":901,"country":902,"online":31,"coordinates":903},"Melbourne Convention and Exhibition Centre","Melbourne","Australia",{"lat":904,"lng":905},-37.8258,144.9559,[],[],[909],{"partnerId":29,"level":910},"supporter",[912,913,914,915,916,917],"Technical Sessions","Late Breaking Reports","Demonstrations","Student Design Competition","Workshops and Tutorials","Video Presentations",[919,55,920,921,922,923,62,924,256,925],"Human-Robot Interaction Theory","Robot Design and Aesthetics","Ethics and Trust in HRI","Collaborative Robotics","Healthcare and Assistive Robotics","Field Studies and Applications","Robot Learning from Humans",{"website":927,"registration":928,"program":929},"https://humanrobotinteraction.org/2025/","https://humanrobotinteraction.org/2025/registration","https://humanrobotinteraction.org/2025/program",[550,894,549,931,932],"human-robot-interaction","research","## About HRI 2025\n\nThe ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for showcasing the very best interdisciplinary and multidisciplinary research in human-robot interaction. Researchers from diverse backgrounds including robotics, computer science, engineering, design, behavioral and social sciences come together to define and advance the state-of-the-art in HRI.\n\n## Conference Theme: \"Robots in the Wild\"\n\nHRI 2025's theme focuses on deploying robots in real-world, uncontrolled environments. As robots move from laboratories into homes, workplaces, and public spaces, understanding how they interact with diverse populations in complex, dynamic settings becomes crucial.","src/content/events/hri-2025.mdx","7d1c9a76a50a8106","partners",["Map",441,938,124,962,285,994,389,1021,192,1050,29,1069,169,1102,346,1132,326,1149,218,1168,65,1188,146,1207],{"id":441,"data":939,"body":959,"filePath":960,"digest":961,"deferredRender":31},{"id":441,"name":940,"shortName":941,"description":942,"type":943,"category":932,"keyContacts":944,"collaboration":947,"website":954,"location":955,"featured":40,"order":958},"George Mason University","GMU","George Mason University is a leading research institution partnering with Semio Community on robotics and human-robot interaction initiatives, including the development of innovative hardware platforms and advancing research in autonomous systems.","academic",[945],{"personId":427,"role":946},"Primary Contact",{"areas":948,"projects":951,"startDate":953,"active":31},[949,417,950,434,183],"Human-Robot Interaction Research","Educational Programs",[952],"MuSoHu (GMU Helmet)",["Date","2023-01-01T00:00:00.000Z"],"https://www.gmu.edu",{"city":956,"country":957},"Fairfax","United States",999,"## Partnership Overview\n\nGeorge Mason University is an academic partner of the Semio Community, contributing to the development of cutting-edge robotics platforms and advancing research in human-robot interaction and autonomous systems.\n\n## Key Contributions\n\n### MuSoHu (GMU Helmet)\n\nGMU is the lead institution developing the MuSoHu (Multi-modal Social Human) helmet system, an innovative wearable device for HRI research that enables researchers to study human social signals and behaviors in naturalistic settings.\n\n### Research Excellence\n\nThe university's Computer Science department, led by faculty including Dr. Xuesu Xiao, conducts pioneering research in:\n- Mobile robotics and autonomous navigation\n- Machine learning for robotics\n- Terrain traversability and adaptive navigation\n- Human-robot interaction\n\n## Collaborative Impact\n\nThrough this partnership, GMU contributes both theoretical advances and practical implementations that benefit the entire Semio Community. The university's commitment to open science and reproducible research aligns perfectly with Semio's mission to democratize access to robotics technologies.\n\n## Contact\n\nFor partnership inquiries related to George Mason University's involvement with the Semio Community, please reach out to our primary contact, Dr. Xuesu Xiao, or contact us through the main Semio Community channels.","src/content/partners/george-mason-university.mdx","1382420f74775968",{"id":124,"data":963,"body":989,"filePath":990,"assetImports":991,"digest":993,"deferredRender":31},{"id":124,"name":964,"shortName":965,"description":966,"type":967,"category":968,"keyContacts":969,"collaboration":973,"website":983,"images":984,"location":986,"featured":31,"order":988},"OLogic, Inc.","OLogic","OLogic, Inc. is a leading robotics engineering and manufacturing company partnering with Semio Community to develop, produce, and support advanced robotics hardware platforms, bringing research robots from concept to production with a focus on quality, reliability, and manufacturability.","industry","development",[970,971],{"personId":407,"role":411},{"personId":118,"role":972},"Operations Lead",{"areas":974,"projects":979,"startDate":982,"active":31},[417,416,415,975,419,976,977,978],"Product Engineering","Supply Chain Management","Technical Support","Quality Assurance",[980,981],"Quori Robot Platform","Custom Robotics Solutions",["Date","2023-01-01T00:00:00.000Z"],"https://www.ologicinc.com",{"logo":985},"__ASTRO_IMAGE_@/assets/images/partners/ologic-logo.png",{"city":987,"country":957},"Sunnyvale",4,"## Partnership Overview\n\nOLogic, Inc. is a crucial industry partner of the Semio Community, providing engineering expertise and manufacturing capabilities that transform research concepts into reliable, production-ready robotic systems. Led by CEO Ted Larson and supported by a talented team including Andrea Alcorn, OLogic has been instrumental in bringing advanced social robotics platforms like Quori to the research community.\n\n## Core Capabilities\n\n### Engineering Excellence\n\nOLogic brings comprehensive engineering expertise:\n- **Mechanical Design**: Robust, manufacturable robot hardware\n- **Electrical Engineering**: Circuit design and power systems\n- **Embedded Systems**: Real-time control and sensor integration\n- **Software Development**: Low-level drivers and control systems\n- **Systems Integration**: Bringing together complex subsystems\n\n### Manufacturing Expertise\n\nFrom prototype to production:\n- **Design for Manufacturing (DFM)**: Optimizing designs for production\n- **Supply Chain Management**: Sourcing components globally\n- **Quality Control**: Rigorous testing and validation\n- **Assembly and Production**: Skilled manufacturing processes\n- **Inventory Management**: Maintaining stock for research community\n\n## Key Contributions\n\n### Quori Robot Platform\n\nOLogic's flagship contribution to Semio Community:\n- **Development Partner**: Engineering and refining the Quori design\n- **Manufacturing**: Producing units for research institutions\n- **Quality Assurance**: Ensuring reliability and consistency\n- **Technical Support**: Ongoing maintenance and troubleshooting\n- **Continuous Improvement**: Iterating based on user feedback\n\n### Technical Innovation\n\nAdvancing robotics through:\n- Custom actuator development\n- Sensor integration solutions\n- Power management systems\n- Modular design approaches\n- Cost optimization strategies\n\n### Community Support\n\n- Technical documentation and guides\n- Repair and maintenance services\n- Spare parts availability\n- Training for researchers\n- Remote troubleshooting assistance\n\n## Engineering Process\n\n### From Concept to Reality\n\nOLogic's proven methodology:\n1. **Requirements Analysis**: Understanding research needs\n2. **Design Optimization**: Balancing performance and cost\n3. **Prototyping**: Rapid iteration and testing\n4. **Validation**: Extensive testing and refinement\n5. **Production**: Scalable manufacturing processes\n6. **Support**: Ongoing maintenance and updates\n\n### Quality Standards\n\nCommitment to excellence:\n- ISO-compliant processes\n- Comprehensive testing protocols\n- Traceability and documentation\n- Continuous improvement practices\n- Customer feedback integration\n\n## Collaborative Impact with Semio\n\n### Hardware Development\n\nOLogic and Semio Community collaborate on:\n- Next-generation robot platforms\n- Modular hardware systems\n- Cost-effective solutions for researchers\n- Standardized components and interfaces\n- Open hardware initiatives\n\n### Knowledge Sharing\n\n- Manufacturing best practices\n- Design guidelines for researchers\n- Cost optimization strategies\n- Supply chain insights\n- Technical troubleshooting guides\n\n### Community Engagement\n\n- Participation in robotics conferences\n- Workshops on hardware development\n- Support for student projects\n- Mentorship for hardware startups\n- Open documentation initiatives\n\n## Industry Expertise\n\n### Robotics Specialization\n\nDeep experience in:\n- **Social Robots**: Platforms for HRI research\n- **Mobile Robots**: Autonomous navigation systems\n- **Manipulators**: Arms and grippers for interaction\n- **Sensing Systems**: Integration of diverse sensors\n- **Control Systems**: Real-time robot control\n\n### Cross-Domain Applications\n\nBringing expertise from:\n- Consumer electronics\n- Medical devices\n- Industrial automation\n- Aerospace systems\n- IoT and connected devices\n\n## Future Directions\n\n### Innovation Pipeline\n\nOLogic and Semio are developing:\n- More affordable research platforms\n- Modular and reconfigurable systems\n- Cloud-connected robotics\n- Advanced sensing capabilities\n- Sustainable manufacturing practices\n\n### Scaling Impact\n\nInitiatives to expand access:\n- Volume production for cost reduction\n- Rental and leasing programs\n- Refurbishment services\n- Educational discounts\n- International distribution\n\n### Technology Advancement\n\n- Integration of AI accelerators\n- Advanced materials and composites\n- Energy-efficient designs\n- Miniaturization of components\n- Improved durability and reliability\n\n## Partnership Benefits\n\n### For Researchers\n\n- Access to professional-grade hardware\n- Reliable, tested platforms\n- Technical support and expertise\n- Customization options\n- Competitive pricing through volume\n\n### For OLogic\n\n- Direct connection to research community\n- Feedback for product improvement\n- Opportunities for innovation\n- Collaborative development projects\n- Market expansion in research sector\n\n## Success Stories\n\n### Impact Metrics\n\n- Dozens of Quori robots deployed globally\n- Hundreds of researchers supported\n- Thousands of hours of reliable operation\n- Continuous platform improvements\n- Growing community of users\n\n### Research Enablement\n\nOLogic's hardware has enabled:\n- Breakthrough HRI research\n- Multi-site collaborative studies\n- Long-term interaction experiments\n- Educational programs\n- Public demonstrations\n\n## Commitment to Open Science\n\n### Supporting Reproducible Research\n\n- Standardized hardware platforms\n- Detailed technical documentation\n- Open communication protocols\n- Modular, serviceable designs\n- Transparent pricing models\n\n### Community First\n\n- Responsive to researcher needs\n- Flexible customization options\n- Commitment to long-term support\n- Knowledge sharing and education\n- Sustainable business practices\n\n## Contact\n\nFor partnership inquiries, hardware development projects, or technical support, please contact Ted Larson (CEO) or Andrea Alcorn, or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [OLogic Website](https://www.ologicinc.com)\n- [Quori Robot Platform](https://www.quori.com)\n- [Technical Documentation](https://docs.ologicinc.com)","src/content/partners/ologic-inc.mdx",[992],"@/assets/images/partners/ologic-logo.png","d48c618b56f00154",{"id":285,"data":995,"body":1015,"filePath":1016,"assetImports":1017,"digest":1020,"deferredRender":31},{"id":285,"name":996,"shortName":996,"description":997,"type":998,"category":968,"keyContacts":999,"collaboration":1001,"website":1008,"images":1009,"location":1012,"featured":31,"order":1014},"Peerbots","Peerbots is a nonprofit organization partnering with Semio Community to advance accessible robotics technologies and foster community-driven development in human-robot interaction, with a focus on democratizing access to social robotics.","nonprofit",[1000],{"personId":274,"role":278},{"areas":1002,"startDate":1007,"active":31},[1003,25,260,55,26,1004,1005,1006],"Accessible Robotics","Democratization of Technology","Inclusive Design","Educational Outreach",["Date","2023-01-01T00:00:00.000Z"],"https://peerbots.org",{"logo":1010,"hero":1011},"__ASTRO_IMAGE_@/assets/images/partners/peerbots-icon.png","__ASTRO_IMAGE_@/assets/images/partners/peerbots-halloween-hero.png",{"city":1013,"country":957},"Washington, D.C.",1,"## Partnership Overview\n\nPeerbots is an innovative nonprofit partner of the Semio Community, dedicated to making robotics technology accessible to everyone. Founded by Saad Elbeleidy, Peerbots focuses on creating open, affordable, and approachable robotic systems that enable more people to participate in the development and deployment of human-robot interaction technologies.\n\n## Mission and Vision\n\n### Core Values\n\nPeerbots operates on fundamental principles:\n- **Accessibility First**: Removing barriers to robotics technology\n- **Community-Driven**: Development guided by user needs\n- **Open Source**: Commitment to transparent, shareable technology\n- **Inclusive Design**: Creating robots for diverse populations\n- **Collaborative Innovation**: Building together, not in isolation\n\n### Impact Goals\n\n- Democratize access to social robotics platforms\n- Foster an inclusive community of developers and users\n- Bridge the gap between research and practical applications\n- Support underrepresented groups in robotics\n- Accelerate innovation through collaboration\n\n## Key Initiatives\n\n### Open Robotics Platforms\n\nPeerbots develops accessible robotic systems:\n- **Affordable Hardware**: Cost-effective robot designs\n- **Modular Architecture**: Customizable and extensible platforms\n- **User-Friendly Interfaces**: Lowering technical barriers\n- **Documentation**: Comprehensive guides for all skill levels\n- **Community Support**: Active forums and help resources\n\n### Community Development\n\nBuilding a vibrant robotics community:\n- **Developer Network**: Connecting robotics enthusiasts globally\n- **Hackathons**: Regular events for collaborative development\n- **Online Resources**: Tutorials, code samples, and best practices\n- **Mentorship Programs**: Pairing experienced developers with newcomers\n- **Project Showcases**: Highlighting community innovations\n\n### Educational Programs\n\nMaking robotics education accessible:\n- **Workshops**: Hands-on learning experiences\n- **Online Courses**: Self-paced robotics education\n- **School Partnerships**: Bringing robotics to classrooms\n- **Summer Programs**: Intensive robotics experiences for students\n- **Teacher Training**: Empowering educators with robotics skills\n\n## Collaborative Projects with Semio\n\n### Joint Development\n\nPeerbots and Semio Community collaborate on:\n- Open-source software tools for HRI\n- Affordable hardware designs for research\n- Community-driven feature development\n- Standardization of robotics interfaces\n- Accessibility improvements for existing platforms\n\n### Knowledge Sharing\n\n- Joint workshops and tutorials\n- Shared documentation and resources\n- Cross-platform compatibility efforts\n- Best practices for inclusive design\n- Community feedback integration\n\n### Outreach Initiatives\n\n- Programs for underserved communities\n- Diversity and inclusion efforts\n- Public demonstrations and exhibitions\n- Media engagement to raise awareness\n- Policy advocacy for accessible technology\n\n## Technology Focus\n\n### Social Robotics\n\nAdvancing human-centered robotics:\n- **Interactive Systems**: Robots that engage naturally with people\n- **Emotional Expression**: Developing expressive robot behaviors\n- **Personalization**: Adaptive systems for individual users\n- **Social Context**: Understanding and responding to social situations\n- **Ethical Design**: Responsible development practices\n\n### Accessibility Features\n\nMaking robots usable by everyone:\n- Multi-modal interaction interfaces\n- Support for assistive technologies\n- Simplified programming environments\n- Culturally inclusive design\n- Low-cost alternatives to expensive components\n\n### Open Source Contributions\n\n- Robot operating system packages\n- Behavior libraries for social interaction\n- Hardware designs and schematics\n- Educational curricula and materials\n- Development tools and utilities\n\n## Community Impact\n\n### Success Stories\n\n- Students building their first robots\n- Researchers accessing affordable platforms\n- Community-driven feature improvements\n- Cross-cultural robotics collaborations\n- Innovations from unexpected contributors\n\n### Metrics and Outcomes\n\n- Growing global community of contributors\n- Increased diversity in robotics development\n- Reduced costs for robotics research\n- Accelerated innovation through collaboration\n- Broader public engagement with robotics\n\n## Future Directions\n\n### Expanding Access\n\nPeerbots and Semio are working to:\n- Further reduce hardware costs\n- Develop cloud-based robotics tools\n- Create mobile and web interfaces\n- Support remote robot operation\n- Enable distributed collaboration\n\n### Technology Advancement\n\n- Integration of AI and machine learning\n- Advanced perception capabilities\n- Natural language interaction\n- Emotional intelligence in robots\n- Adaptive behavior generation\n\n### Community Growth\n\n- Regional chapters and meetups\n- International partnerships\n- Industry collaborations\n- Research institution connections\n- Public-private partnerships\n\n## Partnership Benefits\n\n### For the Robotics Ecosystem\n\n- Expanded developer community\n- Diverse perspectives and innovations\n- Rapid prototyping and iteration\n- Real-world testing and feedback\n- Sustainable development model\n\n### For Society\n\n- Increased access to robotics technology\n- Educational opportunities for all\n- Solutions for social challenges\n- Economic opportunities in robotics\n- Advancement of human-robot collaboration\n\n## Get Involved\n\n### How to Participate\n\n- Join the Peerbots community\n- Contribute to open-source projects\n- Attend workshops and events\n- Share your robotics projects\n- Mentor newcomers to robotics\n\n### Resources\n\n- Developer documentation\n- Hardware build guides\n- Software repositories\n- Community forums\n- Educational materials\n\n## Contact\n\nFor partnership inquiries, community involvement, or more information about Peerbots' initiatives, please contact Saad Elbeleidy, Founder and Lead, or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [Peerbots Website](https://peerbots.org)\n- [GitHub Organization](https://github.com/peerbots)\n- [Community Forum](https://community.peerbots.org)\n- [Documentation](https://docs.peerbots.org)","src/content/partners/peerbots.mdx",[1018,1019],"@/assets/images/partners/peerbots-icon.png","@/assets/images/partners/peerbots-halloween-hero.png","f504cded90b07f3b",{"id":389,"data":1022,"body":1045,"filePath":1046,"assetImports":1047,"digest":1049,"deferredRender":31},{"id":389,"name":1023,"shortName":1024,"description":1025,"type":998,"category":1026,"keyContacts":1027,"collaboration":1029,"website":1040,"images":1041,"location":1043,"featured":40,"order":958},"KISS Institute for Practical Robotics","KIPR","The KISS Institute for Practical Robotics is a nonprofit organization partnering with Semio Community to advance educational robotics, making robotics technology accessible to students and educators worldwide through innovative programs, competitions, and curriculum development.","outreach",[1028],{"personId":378,"role":278},{"areas":1030,"projects":1035,"startDate":1039,"active":31},[62,384,385,386,1031,1032,1033,1034],"Teacher Training","Student Outreach","Open Hardware Development","Accessible Robotics Platforms",[1036,1037,1038],"Botball Educational Robotics Program","Junior Botball Challenge","Open Source Controller Development",["Date","2023-01-01T00:00:00.000Z"],"https://www.kipr.org",{"logo":1042},"__ASTRO_IMAGE_@/assets/images/partners/kipr-logo.png",{"city":1044,"country":957},"Norman","## Partnership Overview\n\nThe KISS Institute for Practical Robotics (KIPR) is a vital nonprofit partner of the Semio Community, dedicated to making robotics education accessible and engaging for students of all backgrounds. With decades of experience in educational robotics, KIPR brings expertise in curriculum development, competition organization, and the creation of affordable, educational robotics platforms.\n\n## Mission and Impact\n\n### Educational Philosophy\n\nKIPR's approach to robotics education emphasizes:\n- **Hands-on Learning**: Students build and program real robots\n- **Problem-Based Curriculum**: Challenging, real-world scenarios\n- **Inclusive Access**: Programs designed for diverse student populations\n- **Teacher Empowerment**: Comprehensive training and support for educators\n- **Open Source Values**: Commitment to accessible, modifiable technologies\n\n## Key Programs\n\n### Botball Educational Robotics Program\n\nKIPR's flagship program engaging middle and high school students:\n- **Global Reach**: Hundreds of schools participating worldwide\n- **Autonomous Robotics**: Students program fully autonomous robots\n- **Team-Based Learning**: Collaborative problem-solving and project management\n- **Annual Tournament**: Regional and international competitions\n- **STEM Integration**: Comprehensive coverage of science, technology, engineering, and mathematics\n\n### Junior Botball Challenge (JBC)\n\nElementary and middle school introduction to robotics:\n- Age-appropriate robotics challenges\n- Focus on fundamental programming concepts\n- Creative problem-solving activities\n- Introduction to engineering design process\n- Pathway to advanced robotics programs\n\n### Professional Development\n\nSupporting educators in robotics education:\n- **Teacher Workshops**: Hands-on training in robotics and programming\n- **Curriculum Resources**: Standards-aligned lesson plans and activities\n- **Online Training**: Accessible professional development opportunities\n- **Mentorship Programs**: Connecting experienced and new robotics educators\n- **Technical Support**: Ongoing assistance for classroom implementation\n\n## Technology Development\n\n### Open Hardware Initiatives\n\nKIPR develops and maintains educational robotics platforms:\n- **Controllers**: User-friendly, powerful robotics controllers\n- **Sensors and Actuators**: Educational-grade components\n- **Software Tools**: Programming environments for students\n- **Documentation**: Comprehensive guides and tutorials\n- **Community Support**: Forums and resources for users\n\n### Software Contributions\n\n- Educational programming environments\n- Simulation tools for robotics learning\n- Curriculum management systems\n- Competition scoring and management platforms\n- Open-source libraries and frameworks\n\n## Collaborative Impact with Semio\n\n### Shared Goals\n\nThe KIPR-Semio partnership advances:\n- Democratization of robotics technology\n- Development of open educational resources\n- Creation of affordable robotics platforms\n- Support for underserved communities\n- Advancement of STEM education\n\n### Joint Initiatives\n\n- Development of educational versions of research robots\n- Creation of curriculum bridging education and research\n- Support for student transitions to advanced robotics\n- Shared infrastructure for competitions and events\n- Knowledge transfer between educational and research communities\n\n## Community Building\n\n### Student Impact\n\nKIPR's programs have:\n- Reached thousands of students globally\n- Inspired careers in STEM fields\n- Developed critical thinking and problem-solving skills\n- Fostered teamwork and leadership abilities\n- Created pathways to higher education in robotics\n\n### Educator Network\n\nBuilding a community of robotics educators:\n- Annual educator conferences\n- Regional teacher meetups\n- Online communities and forums\n- Resource sharing platforms\n- Peer mentorship programs\n\n### Industry Connections\n\nBridging education and industry:\n- Partnerships with technology companies\n- Internship opportunities for students\n- Career pathway development\n- Industry mentorship programs\n- Real-world problem challenges\n\n## Future Directions\n\n### Innovation in Education\n\nKIPR and Semio are working together on:\n- Next-generation educational robotics platforms\n- AI and machine learning in K-12 education\n- Virtual and augmented reality for robotics learning\n- Remote and hybrid learning solutions\n- Inclusive design for diverse learners\n\n### Expanding Access\n\nInitiatives to reach more students:\n- Programs for underrepresented communities\n- Low-cost robotics solutions\n- Online and remote participation options\n- Multi-language resources\n- Partnerships with schools in developing regions\n\n### Research Integration\n\nConnecting educational and research robotics:\n- Student research opportunities\n- Educational applications of research platforms\n- Teacher-researcher collaborations\n- Student participation in research projects\n- Pipeline from education to research careers\n\n## Measurable Outcomes\n\n### Program Success\n\n- Thousands of students engaged annually\n- Hundreds of schools participating globally\n- Documented improvements in STEM skills\n- Increased interest in robotics careers\n- High student retention rates\n\n### Long-term Impact\n\n- Alumni pursuing STEM degrees\n- Former participants entering robotics fields\n- Teachers reporting improved student engagement\n- Schools expanding STEM programs\n- Community-driven innovation\n\n## Partnership Benefits\n\n### For the Robotics Community\n\n- Pipeline of future robotics researchers and engineers\n- Testing ground for educational technologies\n- Feedback from diverse user populations\n- Community engagement and outreach\n- Broader impact for research innovations\n\n### For Education\n\n- Access to cutting-edge robotics technology\n- Connection to research community\n- Professional development opportunities\n- Resources for program development\n- Support for sustainability\n\n## Contact\n\nFor partnership inquiries, educational program information, or collaboration opportunities with KIPR, please contact Steve Goodgame, Executive Director, or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [KIPR Website](https://www.kipr.org)\n- [Botball Program](https://www.botball.org)\n- [Educational Resources](https://www.kipr.org/resources)","src/content/partners/kipr.mdx",[1048],"@/assets/images/partners/kipr-logo.png","a94092525d0e54fa",{"id":192,"data":1051,"body":1066,"filePath":1067,"digest":1068,"deferredRender":31},{"id":192,"name":1052,"shortName":1053,"description":1054,"type":943,"category":932,"keyContacts":1055,"collaboration":1059,"website":1062,"location":1063,"featured":31,"order":1065},"Oregon State University","OSU","Oregon State University is a premier research institution partnering with Semio Community to advance human-robot interaction, socially assistive robotics, and mobile robotics research through innovative projects and educational initiatives.",[1056,1057],{"personId":176,"role":946},{"personId":299,"role":1058},"HRI Research Lead",{"areas":1060,"startDate":1061,"active":31},[26,81,183,184,307,62],["Date","2023-01-01T00:00:00.000Z"],"https://oregonstate.edu",{"city":1064,"country":957},"Corvallis",6,"## Partnership Overview\n\nOregon State University is a key academic partner of the Semio Community, bringing together expertise in robotics, human-robot interaction, and engineering to advance the field of social robotics and create accessible, impactful robotic technologies.\n\n## Research Excellence\n\nOSU's School of Mechanical, Industrial, and Manufacturing Engineering hosts world-class robotics research led by distinguished faculty:\n\n### Dr. Bill Smart\n- Pioneering work in mobile robotics and autonomous navigation\n- Machine learning approaches for robust robot behavior\n- Development of practical robotic systems for real-world deployment\n- Extensive experience in robot software architecture and field robotics\n\n### Dr. Naomi Fitter\n- Leading research in socially assistive robotics\n- Physical human-robot interaction studies\n- Development of robots for health and wellness applications\n- Innovative approaches to robot-mediated social engagement\n\n## Key Contributions\n\n### Research Areas\n- **Mobile Robotics**: Advancing autonomous navigation in complex environments\n- **Human-Robot Interaction**: Developing intuitive and effective interaction paradigms\n- **Assistive Robotics**: Creating robots that support human health and wellbeing\n- **Robot Learning**: Implementing machine learning techniques for adaptive robot behavior\n\n### Educational Impact\nOSU contributes to the Semio Community through:\n- Training the next generation of roboticists\n- Developing open educational resources\n- Hosting workshops and research experiences\n- Fostering interdisciplinary collaboration\n\n## Collaborative Projects\n\nThe partnership between OSU and Semio Community enables:\n- Shared research infrastructure and resources\n- Joint development of robotics platforms and software\n- Collaborative studies in human-robot interaction\n- Knowledge transfer between academia and industry\n\n## Future Directions\n\nOregon State University and Semio Community are working together to:\n- Expand access to social robotics research tools\n- Develop new standards for HRI research reproducibility\n- Create inclusive robotics technologies\n- Bridge the gap between research and real-world applications\n\n## Contact\n\nFor partnership inquiries or research collaborations with Oregon State University, please contact Dr. Bill Smart or Dr. Naomi Fitter, or reach out through the main Semio Community channels.","src/content/partners/oregon-state-university.mdx","f9bfdc78a9472504",{"id":29,"data":1070,"body":1097,"filePath":1098,"assetImports":1099,"digest":1101,"deferredRender":31},{"id":29,"name":1071,"description":1072,"type":967,"category":1073,"keyContacts":1074,"collaboration":1078,"website":1091,"images":1092,"location":1094,"featured":31,"order":1096},"Semio AI","Semio AI is the founding industry partner of the Semio Community, providing leadership, technical infrastructure, and innovative platforms that democratize access to social robotics and advance human-robot interaction through open science and reproducible research.","infrastructure",[1075,1076,1077],{"personId":247,"role":252},{"personId":11,"role":16},{"personId":73,"role":125},{"areas":1079,"projects":1085,"startDate":1090,"active":31},[1080,1081,1082,1083,260,259,26,1084,165],"Platform Development","Open Source Infrastructure","Cloud Robotics","Behavior Design Tools","Multi-modal Communication",[1086,1087,1088,1089],"Semio Platform","Community Infrastructure","Open Source Tools","Research Collaboration",["Date","2023-01-01T00:00:00.000Z"],"https://www.semio.ai",{"logo":1093,"hero":38},"__ASTRO_IMAGE_@/assets/images/logo.png",{"city":1095,"country":957},"Los Angeles",0,"## Partnership Overview\n\nSemio AI, Inc. is the founding industry partner and driving force behind the Semio Community initiative. Led by CEO Ross Mead, CTO Andy Schoen, and a dedicated team, Semio provides the technical infrastructure, platforms, and vision that enable researchers, educators, and developers worldwide to advance the field of human-robot interaction through open, reproducible science.\n\n## Mission and Vision\n\n### Core Purpose\n\nSemio AI is committed to:\n- **Democratizing Robotics**: Making social robotics accessible to all\n- **Enabling Research**: Providing tools and platforms for HRI studies\n- **Building Community**: Fostering collaboration across disciplines\n- **Advancing Science**: Promoting reproducible and replicable research\n- **Creating Impact**: Translating research into real-world applications\n\n### Strategic Goals\n\n- Develop open platforms for social robotics\n- Create sustainable infrastructure for the community\n- Bridge academia and industry\n- Accelerate innovation through collaboration\n- Establish standards for HRI research\n\n## Technology Leadership\n\n### Semio Platform\n\nComprehensive cloud-based robotics platform:\n- **Behavior Design**: Visual tools for creating robot behaviors\n- **Multi-modal Interaction**: Speech, gesture, and expression systems\n- **Cloud Infrastructure**: Scalable deployment and management\n- **Analytics**: Data collection and analysis for research\n- **Integration**: Support for multiple robot platforms\n\n### Open Source Contributions\n\nSemio's commitment to open science:\n- Robot behavior libraries\n- Communication protocols\n- Development tools and SDKs\n- Research datasets\n- Documentation and tutorials\n\n### Technical Innovation\n\nAdvancing the state of the art:\n- Natural language processing for robots\n- Computer vision for social perception\n- Adaptive behavior generation\n- Multi-robot coordination\n- Real-time interaction systems\n\n## Community Leadership\n\n### Building the Ecosystem\n\nSemio AI drives community growth through:\n- **Infrastructure**: Providing technical foundation for collaboration\n- **Events**: Organizing workshops, hackathons, and conferences\n- **Resources**: Creating educational materials and documentation\n- **Support**: Offering technical assistance and mentorship\n- **Partnerships**: Connecting researchers, educators, and industry\n\n### Enabling Collaboration\n\n- Shared research platforms\n- Collaborative development tools\n- Community forums and discussions\n- Knowledge sharing initiatives\n- Cross-institutional projects\n\n## Research Contributions\n\n### Areas of Expertise\n\nDr. Ross Mead and team bring expertise in:\n- **Proxemics**: Spatial dynamics in HRI\n- **Multi-modal Communication**: Integrated interaction channels\n- **Social Robotics**: Robots that understand social context\n- **Behavior Design**: Creating natural robot behaviors\n- **System Architecture**: Scalable robotics platforms\n\n### Academic Collaboration\n\nStrong ties with research institutions:\n- Joint research projects\n- Student internships and mentorships\n- Conference participation\n- Publication collaborations\n- Technology transfer\n\n## Platform Services\n\n### For Researchers\n\nTools and services for HRI research:\n- Robot control and programming\n- Experiment design and execution\n- Data collection and analysis\n- Remote robot access\n- Reproducibility frameworks\n\n### For Educators\n\nEducational technology solutions:\n- Classroom-friendly interfaces\n- Curriculum integration tools\n- Student project support\n- Learning analytics\n- Teacher resources\n\n### For Developers\n\nDevelopment ecosystem:\n- APIs and SDKs\n- Testing and simulation tools\n- Deployment infrastructure\n- Version control integration\n- Community libraries\n\n## Impact and Achievements\n\n### Community Growth\n\n- Hundreds of researchers using Semio platforms\n- Dozens of institutions participating\n- Thousands of experiments conducted\n- Growing open-source contributions\n- Expanding global reach\n\n### Research Enablement\n\nSemio's platforms have enabled:\n- Breakthrough HRI studies\n- Multi-site collaborative research\n- Reproducible experiment protocols\n- Faster research iteration\n- Broader participation in robotics\n\n### Industry Advancement\n\n- Standards development for HRI\n- Best practices documentation\n- Technology democratization\n- Knowledge transfer\n- Innovation acceleration\n\n## Future Roadmap\n\n### Technology Development\n\nSemio AI is developing:\n- Next-generation behavior design tools\n- Advanced AI integration\n- Expanded robot platform support\n- Enhanced collaboration features\n- Improved accessibility\n\n### Community Expansion\n\nGrowth initiatives:\n- Regional community chapters\n- International partnerships\n- Educational program expansion\n- Industry engagement\n- Public outreach\n\n### Research Advancement\n\nSupporting future research:\n- Large-scale studies infrastructure\n- Standardized evaluation metrics\n- Shared datasets and benchmarks\n- Reproducibility tools\n- Open research protocols\n\n## Partnership Philosophy\n\n### Collaborative Approach\n\nSemio AI believes in:\n- Open communication and transparency\n- Shared ownership of community resources\n- Inclusive decision-making\n- Mutual benefit and support\n- Long-term sustainability\n\n### Value Creation\n\nCreating value for all stakeholders:\n- Researchers: Better tools and platforms\n- Educators: Accessible teaching resources\n- Students: Learning opportunities\n- Industry: Innovation pipeline\n- Society: Beneficial technologies\n\n## Leadership Team\n\n### Dr. Ross Mead - CEO & Founder\n\n- Ph.D. in Computer Science from USC\n- Pioneer in proxemics and HRI\n- Committed to democratizing robotics\n- Vision for open, reproducible science\n\n### Andy Schoen - CTO & Co-Founder\n\n- Expert in software engineering and systems\n- Architect of Semio's technical infrastructure\n- Passionate about accessible technology\n- Leader in open-source development\n\n### Chris Birmingham - Team Member\n\n- Contributor to platform development\n- Support for community initiatives\n- Technical expertise in robotics\n- Commitment to user success\n\n## Get Involved\n\n### Partnership Opportunities\n\n- Research collaborations\n- Technology development\n- Educational programs\n- Community initiatives\n- Industry partnerships\n\n### Using Semio Platform\n\n- Sign up for free researcher accounts\n- Access documentation and tutorials\n- Join community discussions\n- Contribute to open-source projects\n- Participate in events\n\n## Contact\n\nFor partnership inquiries, platform access, or more information about Semio AI's role in the Semio Community, please contact Dr. Ross Mead (CEO), Andy Schoen (CTO), or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [Semio AI Website](https://www.semio.ai)\n- [Semio Platform](https://platform.semio.ai)\n- [Documentation](https://docs.semio.ai)\n- [GitHub](https://github.com/semio)\n- [Community Forum](https://community.semio.ai)","src/content/partners/semio-ai.mdx",[1100,45],"@/assets/images/logo.png","e120b1d05b9a9ed6",{"id":169,"data":1103,"body":1127,"filePath":1128,"assetImports":1129,"digest":1131,"deferredRender":31},{"id":169,"name":1104,"shortName":1105,"description":1106,"type":1107,"category":1073,"keyContacts":1108,"collaboration":1113,"website":1122,"images":1123,"location":1125,"featured":40,"order":958},"National Institute of Standards and Technology","NIST","The National Institute of Standards and Technology is a federal agency partnering with Semio Community to develop measurement science, standards, and test methods for robotics and automation, ensuring the safety, reliability, and performance of human-robot interaction systems.","government",[1109,1110,1112],{"personId":153,"role":158},{"personId":394,"role":1111},"Research Scientist",{"personId":226,"role":231},{"areas":1114,"projects":1117,"startDate":1121,"active":31},[165,237,166,163,239,161,236,1115,1116],"Reproducibility Frameworks","Benchmarking",[1118,1119,1120],"HRI Performance Metrics","Collaborative Robot Standards","Test Method Development",["Date","2023-01-01T00:00:00.000Z"],"https://www.nist.gov",{"logo":1124},"__ASTRO_IMAGE_@/assets/images/partners/nist-logo.png",{"city":1126,"country":957},"Gaithersburg","## Partnership Overview\n\nThe National Institute of Standards and Technology (NIST) is a critical government partner of the Semio Community, providing the scientific foundation for measurement, standards, and testing that enables safe, reliable, and effective human-robot interaction. Through its Intelligent Systems Division, NIST brings rigorous scientific methodology and standards expertise that helps ensure robotics technologies can be safely and effectively deployed in real-world applications.\n\n## Mission and Role\n\n### Federal Leadership\n\nNIST serves as the nation's measurement and standards laboratory:\n- **Measurement Science**: Developing fundamental measurement capabilities\n- **Standards Development**: Creating consensus standards for industry\n- **Technology Transfer**: Moving innovations from lab to market\n- **Economic Security**: Enhancing U.S. innovation and competitiveness\n- **Quality Infrastructure**: Ensuring reliability and interoperability\n\n### Robotics Focus\n\nNIST's robotics programs address:\n- Performance measurement for robotic systems\n- Safety standards for human-robot collaboration\n- Test methods for autonomous systems\n- Benchmarking for AI and robotics\n- Reproducibility in robotics research\n\n## Research Excellence\n\n### Intelligent Systems Division\n\nLed by researchers including Dr. Jeremy Marvel, Shelly Bagchi, and Dr. Megan Zimmerman:\n- **Human-Robot Collaboration**: Standards for safe and effective teamwork\n- **Performance Metrics**: Quantitative measures of robot capabilities\n- **Test Methods**: Reproducible evaluation procedures\n- **Manufacturing Robotics**: Standards for industrial applications\n- **Safety Protocols**: Guidelines for human-robot interaction\n\n### Scientific Rigor\n\nNIST brings unique capabilities:\n- Metrology expertise and precision measurement\n- Standardized test environments\n- Statistical analysis and validation\n- International standards participation\n- Industry consensus building\n\n## Key Contributions\n\n### Standards Development\n\nNIST leads critical standardization efforts:\n- **ISO Standards**: Contributing to international robotics standards\n- **ASTM Standards**: Developing test methods for robotics\n- **IEEE Standards**: Participating in robotics and automation standards\n- **Safety Standards**: Ensuring safe human-robot interaction\n- **Performance Standards**: Defining metrics for robot capabilities\n\n### Test Methods and Metrics\n\nCreating reproducible evaluation frameworks:\n- Standardized test scenarios\n- Performance benchmarks\n- Measurement protocols\n- Data collection standards\n- Analysis methodologies\n\n### Industry Support\n\nEnabling technology adoption:\n- Guidance for manufacturers\n- Certification frameworks\n- Best practices documentation\n- Technology readiness assessments\n- Risk evaluation methods\n\n## Collaborative Impact with Semio\n\n### Reproducible Research\n\nNIST and Semio Community advance:\n- Standardized experimental protocols\n- Reproducible test procedures\n- Common evaluation metrics\n- Shared benchmarking tools\n- Data quality standards\n\n### Community Standards\n\nJoint efforts in:\n- HRI evaluation frameworks\n- Safety guidelines for social robots\n- Performance metrics for assistive systems\n- Ethical considerations in testing\n- Accessibility standards\n\n### Knowledge Dissemination\n\n- Technical publications and reports\n- Workshop organization\n- Training programs\n- Online resources\n- Community guidelines\n\n## Research Programs\n\n### Human-Robot Collaboration\n\nAdvancing safe and effective collaboration:\n- **Safety Standards**: Ensuring human safety in shared workspaces\n- **Performance Metrics**: Measuring collaboration effectiveness\n- **Communication Protocols**: Standards for human-robot communication\n- **Trust and Transparency**: Guidelines for trustworthy systems\n- **Ergonomics**: Human factors in robot design\n\n### Autonomous Systems\n\nStandards for autonomous robots:\n- Navigation and mobility metrics\n- Perception system evaluation\n- Decision-making transparency\n- Reliability and robustness testing\n- Failure mode analysis\n\n### Manufacturing and Industrial Robotics\n\nSupporting industry adoption:\n- Production system standards\n- Quality assurance methods\n- Integration guidelines\n- Performance benchmarks\n- Return on investment metrics\n\n## Testing Infrastructure\n\n### Facilities and Resources\n\nNIST provides world-class testing capabilities:\n- Robotics test facilities\n- Motion capture systems\n- Force and torque measurement\n- Environmental testing chambers\n- Calibration services\n\n### Artifact Development\n\nCreating standardized test artifacts:\n- Reference test pieces\n- Calibration objects\n- Standard test scenarios\n- Benchmark datasets\n- Evaluation software\n\n## Future Directions\n\n### Emerging Standards\n\nNIST is developing standards for:\n- AI-enabled robotics systems\n- Social robot evaluation\n- Ethical AI in robotics\n- Explainable robot behaviors\n- Long-term autonomy\n\n### Research Priorities\n\nFocus areas include:\n- Human-aware robot planning\n- Collaborative manipulation\n- Multi-robot coordination\n- Resilient autonomous systems\n- Trustworthy AI for robotics\n\n### Community Engagement\n\nExpanding participation through:\n- Public-private partnerships\n- Academic collaborations\n- International cooperation\n- Small business support\n- Public comment processes\n\n## Impact on the Field\n\n### Enabling Innovation\n\nNIST's work enables:\n- Faster technology adoption\n- Reduced development costs\n- Improved system reliability\n- Enhanced safety\n- Market confidence\n\n### Economic Benefits\n\nContributing to:\n- U.S. competitiveness in robotics\n- Job creation in advanced manufacturing\n- Productivity improvements\n- Quality enhancements\n- Export opportunities\n\n### Societal Impact\n\nEnsuring robotics benefits society:\n- Safe deployment of robots\n- Accessible technologies\n- Ethical development practices\n- Public trust in robotics\n- Inclusive innovation\n\n## Partnership Benefits\n\n### For Semio Community\n\n- Access to measurement expertise\n- Participation in standards development\n- Testing and validation resources\n- Scientific rigor and credibility\n- Federal research partnerships\n\n### For NIST\n\n- Connection to research community\n- Real-world use cases\n- Technology feedback\n- Collaborative development\n- Broader impact for standards\n\n## Get Involved\n\n### Participation Opportunities\n\n- Standards development committees\n- Public comment on draft standards\n- Collaborative research projects\n- Testing and validation programs\n- Workshops and conferences\n\n### Resources Available\n\n- Technical reports and publications\n- Standard reference materials\n- Testing protocols\n- Measurement tools\n- Training materials\n\n## Contact\n\nFor partnership inquiries, standards information, or collaboration opportunities with NIST, please contact Dr. Jeremy Marvel, Shelly Bagchi, or Dr. Megan Zimmerman, or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [NIST Robotics](https://www.nist.gov/el/intelligent-systems-division)\n- [NIST Standards](https://www.nist.gov/standards)\n- [Publications](https://www.nist.gov/publications)\n- [Manufacturing USA](https://www.manufacturingusa.com)","src/content/partners/nist.mdx",[1130],"@/assets/images/partners/nist-logo.png","7b4f14d7450efe5d",{"id":346,"data":1133,"body":1146,"filePath":1147,"digest":1148,"deferredRender":31},{"id":346,"name":1134,"shortName":1135,"description":1136,"type":943,"category":932,"keyContacts":1137,"collaboration":1139,"website":1142,"location":1143,"featured":40,"order":1145},"Tufts University","Tufts","Tufts University is a premier research institution partnering with Semio Community to advance artificial intelligence, multi-agent systems, and human-AI collaboration, with particular emphasis on creating intelligent systems that can effectively understand and work alongside humans.",[1138],{"personId":331,"role":946},{"areas":1140,"startDate":1141,"active":31},[338,340,339,26,341,342,343,59],["Date","2023-01-01T00:00:00.000Z"],"https://www.tufts.edu",{"city":1144,"country":957},"Medford",5,"## Partnership Overview\n\nTufts University is a distinguished academic partner of the Semio Community, bringing cutting-edge expertise in artificial intelligence, human-AI collaboration, and multi-agent systems. Through its Computer Science department, Tufts contributes fundamental research that enhances how robots and AI systems understand, predict, and collaborate with humans.\n\n## Research Excellence\n\n### Dr. Reuth Mirsky - Assistant Professor\n\nDr. Reuth Mirsky leads innovative research at the intersection of AI and human collaboration:\n\n- **Expertise**: Multi-agent systems and human-AI collaboration\n- **Focus**: Creating AI systems for complex, dynamic environments\n- **Approach**: Bridging theoretical AI with practical HRI applications\n- **Background**: Ph.D. from Ben-Gurion University, postdocs at UT Austin and Harvard\n\n## Key Research Areas\n\n### Human-AI Collaboration\n\nTufts advances the science of human-AI teamwork:\n- **Plan Recognition**: Enabling AI to understand human intentions and goals\n- **Theory of Mind Modeling**: Creating agents that can reason about human mental states\n- **Adaptive Systems**: Developing AI that adjusts to human partners\n- **Trust and Transparency**: Building explainable and trustworthy AI systems\n\n### Multi-Agent Systems\n\nResearch on coordinated intelligent systems:\n- Agent coordination and communication\n- Distributed decision-making\n- Team formation and task allocation\n- Emergent behaviors in agent societies\n\n### Applied AI for Robotics\n\nPractical applications in human-robot interaction:\n- Social signal processing and interpretation\n- Collaborative task planning and execution\n- Learning from human feedback\n- Real-world deployment of intelligent systems\n\n## Collaborative Contributions\n\n### Research Synergies\n\nThe Tufts-Semio partnership enables:\n- Integration of advanced AI into social robotics platforms\n- Development of more intelligent and adaptive robot behaviors\n- Creation of standardized benchmarks for HRI evaluation\n- Advancement of human-centered AI principles\n\n### Interdisciplinary Approach\n\nTufts brings together expertise from:\n- Computer Science and AI\n- Cognitive Science\n- Psychology and Human Factors\n- Engineering\n- Ethics and Philosophy\n\n## Current Initiatives\n\n### Active Projects\n- Developing theory of mind models for social robots\n- Creating explainable AI systems for HRI\n- Building adaptive agents for collaborative tasks\n- Researching trust dynamics in human-robot teams\n\n### Community Contributions\n- Open-source AI algorithms for robotics\n- Datasets for plan recognition and intent prediction\n- Evaluation frameworks for human-AI collaboration\n- Best practices for transparent AI systems\n\n## Educational Impact\n\n### Training Future Leaders\n\nTufts contributes to workforce development through:\n- Graduate programs in AI and robotics\n- Undergraduate research opportunities\n- Interdisciplinary coursework\n- Industry partnerships and internships\n\n### Knowledge Dissemination\n\n- Publications in top AI and robotics venues\n- Workshops and tutorials at major conferences\n- Public lectures on AI and society\n- Community outreach programs\n\n## Research Infrastructure\n\n### Facilities and Resources\n- AI and robotics research laboratories\n- High-performance computing clusters\n- Human subject research facilities\n- Collaborative workspaces for interdisciplinary teams\n\n### Software and Tools\n- Custom AI frameworks and libraries\n- Simulation environments for multi-agent systems\n- Data collection and analysis platforms\n- Visualization tools for explainable AI\n\n## Future Directions\n\n### Emerging Research Areas\n\nTufts and Semio Community are exploring:\n- Large language models for human-robot interaction\n- Embodied AI for social robotics\n- Ethical AI for assistive technologies\n- Scalable solutions for real-world deployment\n\n### Long-term Vision\n\n- Creating AI systems that truly understand human needs\n- Developing robots that can seamlessly integrate into human teams\n- Establishing standards for ethical AI in robotics\n- Building inclusive technologies that benefit all users\n\n## Partnership Benefits\n\n### For the Research Community\n- Access to cutting-edge AI research\n- Collaborative opportunities with leading researchers\n- Shared resources and infrastructure\n- Accelerated innovation through partnership\n\n### For Society\n- More capable and helpful robotic assistants\n- Improved human-AI collaboration in various domains\n- Ethical and responsible AI development\n- Technologies that enhance quality of life\n\n## Contact\n\nFor partnership inquiries or research collaborations with Tufts University, please contact Dr. Reuth Mirsky or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [Tufts Computer Science](https://engineering.tufts.edu/cs)\n- [Tufts School of Engineering](https://engineering.tufts.edu)","src/content/partners/tufts-university.mdx","2f2ae615b5e0a2c7",{"id":326,"data":1150,"body":1165,"filePath":1166,"digest":1167,"deferredRender":31},{"id":326,"name":1151,"shortName":1151,"description":1152,"type":967,"category":968,"keyContacts":1153,"collaboration":1156,"website":373,"location":1162,"featured":31,"order":1164},"IK Studio","IK Studio is a design and innovation firm partnering with Semio Community to advance the integration of robotics into architectural and interactive environments, bridging the gap between artistic expression and technological functionality in human-robot interaction.",[1154,1155],{"personId":352,"role":370},{"personId":315,"role":319},{"areas":1157,"startDate":1161,"active":31},[360,361,26,362,363,364,1158,1159,1160],"Creative Technology","Experience Design","Robotic Art Installations",["Date","2023-01-01T00:00:00.000Z"],{"city":1163,"country":957},"Philadelphia",2,"## Partnership Overview\n\nIK Studio is a pioneering design and innovation partner of the Semio Community, bringing unique expertise at the intersection of architecture, design, and robotics. Led by Simon Kim and Nicholas Houser, IK Studio creates transformative experiences that blend physical and digital realms, making robots not just functional tools but integral parts of our built environment and cultural experiences.\n\n## Design Philosophy\n\n### Bridging Art and Technology\n\nIK Studio's approach combines:\n- **Architectural Vision**: Integrating robotics into spatial design\n- **Artistic Expression**: Creating emotionally resonant robotic experiences\n- **Technical Innovation**: Pushing boundaries of what's possible\n- **Human-Centered Design**: Prioritizing user experience and interaction\n- **Cultural Relevance**: Creating meaningful connections between technology and society\n\n### Innovation Through Design\n\n- Experimental prototyping and iteration\n- Cross-disciplinary collaboration\n- User research and testing\n- Speculative design for future scenarios\n- Evidence-based design decisions\n\n## Key Expertise\n\n### Architectural Robotics\n\nIK Studio pioneers the integration of robots in built environments:\n- **Spatial Robotics**: Robots as architectural elements\n- **Dynamic Spaces**: Environments that adapt and respond\n- **Interactive Installations**: Public engagement through robotic art\n- **Building Systems Integration**: Seamless incorporation of robotic technologies\n- **Future Living Concepts**: Envisioning human-robot cohabitation\n\n### Interactive Design\n\nCreating compelling human-robot experiences:\n- Multi-sensory interaction design\n- Gesture and movement-based interfaces\n- Emotional design for robots\n- Narrative experiences with robotic actors\n- Social dynamics in robotic spaces\n\n### Technical Capabilities\n\n- Custom robot hardware development\n- Software for interactive behaviors\n- Sensor integration and data processing\n- Real-time responsive systems\n- Rapid prototyping and fabrication\n\n## Collaborative Projects with Semio\n\n### Design Innovation\n\nIK Studio contributes to Semio Community through:\n- Design research for social robots\n- User experience studies\n- Prototype development and testing\n- Creative applications of existing platforms\n- Design guidelines and best practices\n\n### Cross-Pollination\n\nBringing together diverse fields:\n- Architecture and robotics\n- Art and engineering\n- Design and computer science\n- Performance and technology\n- Culture and innovation\n\n### Public Engagement\n\n- Interactive exhibitions and installations\n- Design workshops and charrettes\n- Public demonstrations\n- Educational outreach\n- Media and communications\n\n## Portfolio Highlights\n\n### Robotic Installations\n\nIK Studio has created numerous innovative projects:\n- **Public Art**: Large-scale robotic sculptures and performances\n- **Museum Exhibitions**: Interactive educational experiences\n- **Commercial Spaces**: Retail and hospitality robot integration\n- **Research Prototypes**: Experimental platforms for HRI studies\n- **Cultural Events**: Robotic performances and experiences\n\n### Design Research\n\nAdvancing the field through:\n- Studies on human perception of robotic motion\n- Exploration of robot personality through design\n- Investigation of spatial relationships in HRI\n- Development of new interaction paradigms\n- Creation of design patterns for social robotics\n\n## Industry Impact\n\n### Setting Standards\n\nIK Studio influences the field by:\n- Establishing design principles for robotic spaces\n- Creating benchmarks for interactive experiences\n- Developing evaluation methods for robotic design\n- Sharing knowledge through publications\n- Mentoring emerging designers in robotics\n\n### Commercial Applications\n\nTranslating research into practice:\n- Retail robot experiences\n- Hospitality and service robots\n- Entertainment and media applications\n- Healthcare environment design\n- Educational technology integration\n\n## Future Directions\n\n### Emerging Areas\n\nIK Studio and Semio Community are exploring:\n- Augmented reality integration with robotics\n- Biomorphic and organic robot designs\n- Sustainable and eco-friendly robotic systems\n- Cultural preservation through robotic performances\n- Adaptive environments for aging populations\n\n### Vision for Tomorrow\n\n- Seamless integration of robots in daily life\n- Culturally sensitive robotic designs\n- Democratized access to design tools\n- Community-driven innovation\n- Ethical and responsible design practices\n\n## Partnership Benefits\n\n### For Semio Community\n\n- Design expertise and creative vision\n- User experience research and insights\n- Prototype development capabilities\n- Public engagement and outreach\n- Bridge between research and application\n\n### For IK Studio\n\n- Access to cutting-edge research\n- Collaboration with technical experts\n- Testing ground for innovative concepts\n- Community feedback and iteration\n- Broader impact for design work\n\n## Collaborative Approach\n\n### Working Methods\n\n- Design thinking workshops\n- Rapid prototyping sprints\n- User testing and feedback sessions\n- Interdisciplinary team collaboration\n- Open documentation and sharing\n\n### Knowledge Transfer\n\n- Design documentation and guidelines\n- Case studies and best practices\n- Workshop facilitation\n- Mentorship programs\n- Conference presentations\n\n## Get Involved\n\n### Collaboration Opportunities\n\n- Design challenges and competitions\n- Residency programs\n- Joint research projects\n- Exhibition partnerships\n- Educational initiatives\n\n## Contact\n\nFor partnership inquiries, design collaborations, or more information about IK Studio's work with the Semio Community, please contact Simon Kim (Principal) or Nicholas Houser (Design Engineer), or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [IK Studio Website](https://www.i-k-studio.com)\n- [Portfolio](https://www.i-k-studio.com/projects)\n- [Research](https://www.i-k-studio.com/research)","src/content/partners/ik-studio.mdx","1c925cb2c9116b4f",{"id":218,"data":1169,"body":1185,"filePath":1186,"digest":1187,"deferredRender":31},{"id":218,"name":1170,"shortName":1171,"description":1172,"type":943,"category":932,"keyContacts":1173,"collaboration":1178,"website":1182,"location":1183,"featured":40,"order":1184},"University of Pennsylvania","UPenn","The University of Pennsylvania is a world-renowned Ivy League institution partnering with Semio Community to advance robotics research through innovative mechanical design, modular robotics, and architectural robotics initiatives.",[1174,1176],{"personId":199,"role":1175},"GRASP Lab Director",{"personId":352,"role":1177},"Architecture & Robotics Lead",{"areas":1179,"startDate":1181,"active":31},[207,209,360,26,210,213,1180],"Interactive Environments",["Date","2023-01-01T00:00:00.000Z"],"https://www.upenn.edu",{"city":1163,"country":957},3,"## Partnership Overview\n\nThe University of Pennsylvania is a premier academic partner of the Semio Community, leveraging its world-class GRASP Laboratory and Weitzman School of Design to push the boundaries of robotics research and create innovative solutions at the intersection of robotics, architecture, and human interaction.\n\n## Research Excellence\n\n### GRASP Laboratory\n\nThe General Robotics, Automation, Sensing and Perception (GRASP) Lab at Penn is one of the world's leading robotics research centers, directed by Dr. Mark Yim. The lab brings together researchers from multiple disciplines to tackle fundamental challenges in robotics.\n\n#### Dr. Mark Yim - Mechanical Engineering\n- Pioneer in modular self-reconfigurable robotics\n- Development of bio-inspired robotic mechanisms\n- Innovation in soft robotics and medical applications\n- Creation of novel flying and crawling robot designs\n- Leadership in distributed robotic systems\n\n### Weitzman School of Design\n\nThe Weitzman School of Design, through faculty like Simon Kim, explores the integration of robotics into architectural and design contexts.\n\n#### Simon Kim - Architecture & Design\n- Architectural robotics and responsive environments\n- Integration of robotic systems in built spaces\n- Interactive design and spatial computing\n- Human-robot interaction in architectural contexts\n- Cross-disciplinary innovation bridging design and technology\n\n## Key Contributions\n\n### Modular Robotics Research\nPenn's groundbreaking work in modular robotics has established fundamental principles for:\n- Self-assembling and reconfigurable robot systems\n- Standardized robotic modules with interchangeable functionality\n- Distributed control algorithms for modular systems\n- Applications in search and rescue, exploration, and adaptive manufacturing\n\n### Bio-Inspired Design\nResearch initiatives that draw inspiration from nature:\n- Snake robots for navigation through complex terrains\n- Flying robots with novel propulsion mechanisms\n- Soft robots for medical and assistive applications\n- Adaptive mechanisms based on biological principles\n\n### Architectural Robotics\nInnovative work at the intersection of architecture and robotics:\n- Robots integrated into building systems\n- Interactive and responsive architectural elements\n- Spatial robotics for dynamic environments\n- Human-centered design for robotic spaces\n\n## Collaborative Impact\n\n### Research Synergies\nThe partnership between UPenn and Semio Community creates unique opportunities:\n- Access to GRASP Lab's extensive research infrastructure\n- Cross-pollination between robotics and design disciplines\n- Development of next-generation social robot platforms\n- Advancement of human-centered robotics technologies\n\n### Educational Excellence\nPenn contributes to workforce development through:\n- Graduate and undergraduate robotics programs\n- Interdisciplinary research opportunities\n- Public outreach and STEM education initiatives\n- Training programs for industry professionals\n\n## Innovation Pipeline\n\n### Current Projects\n- Development of modular components for social robots\n- Research on adaptive robotic behaviors\n- Studies in human-robot collaboration\n- Creation of robots for healthcare applications\n\n### Future Directions\n- Expansion of modular robotics platforms for HRI research\n- Integration of AI and machine learning in modular systems\n- Development of standards for reconfigurable robots\n- Exploration of robots in urban environments\n\n## Facilities and Resources\n\nThe University of Pennsylvania offers state-of-the-art facilities:\n- GRASP Laboratory with specialized robotics equipment\n- Fabrication facilities for rapid prototyping\n- Motion capture systems for interaction studies\n- Computational resources for simulation and AI\n- Testing environments for field robotics\n\n## Contact\n\nFor partnership inquiries or research collaborations with the University of Pennsylvania, please contact Dr. Mark Yim (GRASP Lab) or Simon Kim (Weitzman School of Design), or reach out through the main Semio Community channels.","src/content/partners/university-of-pennsylvania.mdx","d8d80588c4c5b0ac",{"id":65,"data":1189,"body":1204,"filePath":1205,"digest":1206,"deferredRender":31},{"id":65,"name":1190,"shortName":1191,"description":1192,"type":943,"category":932,"keyContacts":1193,"collaboration":1197,"website":1201,"location":1202,"featured":40,"order":1065},"Yale University","Yale","Yale University is a world-renowned Ivy League institution partnering with Semio Community to advance social robotics, human-robot interaction, and the development of assistive technologies for education, therapy, and human development.",[1194,1196],{"personId":47,"role":1195},"Social Robotics Lab Director",{"personId":102,"role":1111},{"areas":1198,"startDate":1200,"active":31},[55,26,56,62,57,1199,59,61,109],"Computer Vision for HRI",["Date","2023-01-01T00:00:00.000Z"],"https://www.yale.edu",{"city":1203,"country":957},"New Haven","## Partnership Overview\n\nYale University is a premier academic partner of the Semio Community, home to the internationally recognized Yale Social Robotics Lab. Through pioneering research in social robotics and human-robot interaction, Yale contributes fundamental advances in how robots can understand, interact with, and assist humans, particularly in therapeutic and educational contexts.\n\n## Research Excellence\n\n### Yale Social Robotics Lab\n\nThe Yale Social Robotics Lab, directed by Dr. Brian Scassellati, is one of the world's leading centers for social robotics research:\n\n- **Founded**: One of the earliest dedicated social robotics laboratories\n- **Mission**: Building embodied computational models of human social behavior\n- **Impact**: Transformative work in autism therapy, education, and assistive robotics\n- **Approach**: Interdisciplinary research combining robotics, AI, and cognitive science\n\n### Research Leadership\n\n#### Dr. Brian Scassellati - A. Bartlett Giamatti Professor\n- Pioneer in social robotics and human-robot interaction\n- Fellow of the American Association for the Advancement of Science (AAAS)\n- Leading expert in robots for autism therapy\n- Over two decades of groundbreaking research in HRI\n\n#### Kayla Matheus - Research Scientist\n- Specialist in child-robot interaction\n- Focus on educational and therapeutic applications\n- Development of adaptive robotic systems\n- Inclusive technology design for diverse populations\n\n## Key Research Areas\n\n### Autism Therapy Robotics\n\nYale's groundbreaking work in autism intervention:\n- **Social Skills Development**: Robots that teach and practice social interactions\n- **Joint Attention**: Systems that help children develop shared attention skills\n- **Communication Support**: Assistive robots for language development\n- **Personalized Therapy**: Adaptive systems tailored to individual needs\n- **Long-term Studies**: Extensive research on therapeutic outcomes\n\n### Educational Robotics\n\nInnovative applications in learning:\n- Tutoring systems for personalized education\n- Social robots as learning companions\n- STEM education through robotics\n- Special education support systems\n- Assessment of learning outcomes\n\n### Fundamental HRI Research\n\nCore contributions to the field:\n- **Theory of Mind**: Robots that understand human mental states\n- **Social Signal Processing**: Interpreting human social cues\n- **Nonverbal Communication**: Gesture and gaze understanding\n- **Emotion Recognition**: Systems that perceive and respond to emotions\n- **Behavioral Modeling**: Computational models of human behavior\n\n## Collaborative Impact\n\n### Research Innovations\n\nThe Yale-Semio partnership produces:\n- Novel therapeutic robotic platforms\n- Advanced social perception algorithms\n- Standardized evaluation methods for HRI\n- Open-source tools for the research community\n\n### Clinical Translation\n\nMoving research into practice:\n- Partnerships with healthcare providers\n- Clinical trials of robotic interventions\n- Development of deployment guidelines\n- Training for therapists and educators\n\n### Interdisciplinary Collaboration\n\nYale brings together expertise from:\n- Computer Science and Engineering\n- Psychology and Cognitive Science\n- Child Study Center\n- School of Medicine\n- School of Education\n\n## Current Projects\n\n### Active Research Initiatives\n- Next-generation robots for autism therapy\n- Adaptive learning systems for education\n- Social robots for mental health support\n- Long-term human-robot relationships\n- Ethical frameworks for assistive robotics\n\n### Community Contributions\n- Open-source robot control software\n- Datasets for social signal recognition\n- Clinical study protocols and findings\n- Best practices for therapeutic robotics\n\n## Educational Excellence\n\n### Training Programs\n- Graduate research in social robotics\n- Undergraduate research opportunities\n- Summer research programs\n- Postdoctoral fellowships\n\n### Knowledge Transfer\n- Publications in top-tier venues\n- Conference workshops and tutorials\n- Public engagement activities\n- Media outreach on robotics and autism\n\n## Research Infrastructure\n\n### State-of-the-Art Facilities\n- Yale Social Robotics Lab\n- Child-friendly interaction spaces\n- Motion capture systems\n- Eye-tracking equipment\n- Computational resources for AI/ML\n\n### Robot Platforms\n- Custom social robots for research\n- Commercial platforms for comparison studies\n- Mobile manipulation systems\n- Wearable sensing technologies\n\n## Future Directions\n\n### Emerging Areas\n\nYale and Semio Community are exploring:\n- Robots for neurodiversity support\n- AI-powered personalization in therapy\n- Multi-modal interaction systems\n- Home deployment of assistive robots\n- Longitudinal impact studies\n\n### Vision for Impact\n\n- Democratizing access to therapeutic robotics\n- Creating evidence-based intervention protocols\n- Establishing ethical guidelines for vulnerable populations\n- Building sustainable models for technology deployment\n- Training the next generation of researchers\n\n## Clinical and Societal Impact\n\n### Research Translation\n- Partnerships with autism centers\n- Collaboration with schools and educators\n- Integration with healthcare systems\n- Community-based deployments\n\n### Measurable Outcomes\n- Improved social skills in children with autism\n- Enhanced educational outcomes\n- Increased accessibility to therapy\n- Support for families and caregivers\n- Evidence-based best practices\n\n## Partnership Benefits\n\n### For Semio Community\n- Access to world-class research expertise\n- Clinical validation of technologies\n- Evidence-based design principles\n- Ethical frameworks for deployment\n\n### For Yale\n- Broader impact for research innovations\n- Access to community platforms and tools\n- Collaborative opportunities\n- Accelerated technology translation\n\n## Contact\n\nFor partnership inquiries or research collaborations with Yale University, please contact Dr. Brian Scassellati or Kayla Matheus, or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [Yale Social Robotics Lab](https://scazlab.yale.edu)\n- [Yale Computer Science](https://cpsc.yale.edu)\n- [Yale Child Study Center](https://medicine.yale.edu/childstudy/)","src/content/partners/yale-university.mdx","cf8184ae124bee21",{"id":146,"data":1208,"body":1220,"filePath":1221,"digest":1222,"deferredRender":31},{"id":146,"name":1209,"shortName":1210,"description":1211,"type":943,"category":932,"keyContacts":1212,"collaboration":1214,"website":1218,"location":1219,"featured":40,"order":988},"University of Southern California","USC","The University of Southern California is a leading research university partnering with Semio Community to advance socially assistive robotics, human-robot interaction, and the development of innovative robotic systems for healthcare, education, and social applications.",[1213],{"personId":131,"role":946},{"areas":1215,"startDate":1217,"active":31},[81,26,1216,140,139,142,143,56],"Healthcare Robotics",["Date","2023-01-01T00:00:00.000Z"],"https://www.usc.edu",{"city":1095,"country":957},"## Partnership Overview\n\nThe University of Southern California is a foundational academic partner of the Semio Community, bringing world-class expertise in socially assistive robotics and human-robot interaction. Through the USC Robotics and Autonomous Systems Center (RASC) and the USC Robotics Research Lab, USC contributes groundbreaking research and innovation to advance human-centered robotics technologies.\n\n## Research Excellence\n\n### Dr. Maja J. MatariÄ - Chan Soon-Shiong Distinguished Professor\n\nDr. Maja MatariÄ leads pioneering research that has defined the field of socially assistive robotics:\n\n- **Founding Director**: USC Robotics and Autonomous Systems Center (RASC)\n- **Co-Director**: USC Robotics Research Lab\n- **Fellow**: AAAS, IEEE, AAAI, and ACM\n- **Pioneer**: Socially assistive robotics as a distinct research field\n- **Innovator**: Robot-assisted therapies for diverse populations\n\n## Key Contributions\n\n### Socially Assistive Robotics\n\nUSC has established the foundations of socially assistive robotics (SAR):\n- Robots that assist through social interaction rather than physical contact\n- Personalized assistance and motivation systems\n- Adaptive behaviors based on user needs and preferences\n- Long-term human-robot interaction studies\n\n### Healthcare Applications\n\nGroundbreaking work in therapeutic robotics:\n- **Autism Spectrum Disorders**: Developing robots that help children with ASD improve social skills\n- **Stroke Rehabilitation**: Creating motivating robotic coaches for motor recovery\n- **Alzheimer's Disease**: Designing companions for cognitive engagement\n- **Healthy Aging**: Supporting older adults' physical and cognitive wellness\n\n### Research Areas\n\nUSC's comprehensive research portfolio includes:\n- **Multi-Robot Coordination**: Algorithms for robot teams and swarms\n- **Human-Robot Interaction**: Natural and intuitive interaction paradigms\n- **Machine Learning for HRI**: Adaptive and personalized robot behaviors\n- **Embodied AI**: Intelligence through physical embodiment\n- **Behavioral Modeling**: Understanding and predicting human behavior\n- **Ethics in Robotics**: Responsible development of assistive technologies\n\n## Collaborative Impact\n\n### Innovation Pipeline\n\nThe USC-Semio partnership enables:\n- Translation of research into practical applications\n- Development of open-source tools for the HRI community\n- Standardization of evaluation metrics for social robots\n- Creation of reproducible research protocols\n\n### Educational Excellence\n\nUSC contributes to the community through:\n- Training next-generation robotics researchers\n- Interdisciplinary programs combining CS, engineering, and healthcare\n- Public engagement and STEM outreach\n- Professional development workshops\n\n## Research Infrastructure\n\n### Facilities\n- State-of-the-art robotics laboratories\n- Human subject research facilities\n- Motion capture and sensing systems\n- Computational resources for AI/ML research\n\n### Platforms\n- Custom socially assistive robot platforms\n- Sensor networks for behavior monitoring\n- Software frameworks for HRI studies\n- Data collection and analysis tools\n\n## Current Projects\n\n### Active Research\n- Personalized robot companions for older adults\n- Robots for special needs education\n- Rehabilitation robotics for motor recovery\n- Social robots for mental health support\n\n### Community Initiatives\n- Open-source SAR software development\n- Shared datasets for HRI research\n- Best practices documentation\n- Reproducibility frameworks\n\n## Future Directions\n\nUSC and Semio Community are collaborating on:\n- Scaling socially assistive robotics to real-world deployment\n- Developing ethical guidelines for assistive robots\n- Creating accessible robotics platforms for researchers\n- Establishing standards for HRI evaluation\n- Building inclusive technologies for diverse populations\n\n## Impact and Recognition\n\nUSC's contributions have been recognized through:\n- Numerous prestigious awards and honors\n- High-impact publications in top venues\n- Technology transfer to industry\n- Policy influence on robotics and AI\n- Media coverage raising awareness of SAR\n\n## Contact\n\nFor partnership inquiries or research collaborations with the University of Southern California, please contact Dr. Maja MatariÄ or reach out through the main Semio Community channels.\n\n## Learn More\n\n- [USC Robotics Research Lab](https://robotics.usc.edu)\n- [USC Viterbi School of Engineering](https://viterbischool.usc.edu)\n- [Interaction Lab](https://robotics.usc.edu/interaction/)","src/content/partners/university-of-southern-california.mdx","bfec769a4a304446","studies",["Map",1225,1226],"peerbots-fellowship-1",{"id":1225,"data":1227,"body":1251,"filePath":1252,"digest":1253,"deferredRender":31},{"title":1228,"abstract":1229,"authors":1230,"type":1234,"venue":1235,"year":1236,"keywords":1237,"researchArea":1245,"fundingOrganizations":1247,"links":1248,"citations":1096,"featured":40,"draft":40,"publishDate":1250},"Examining the Adoption of Expressive Faces for Humanoid Robots in Industry and Academia","A qualitative study examining the adoption of expressive faces, like the Peerbots face, for humanoid robots in industry and academia. Using speculative design methods, this research focuses on understanding the needs that decision-makers are taking into consideration about the facial expression of the robots they are designing.",[1231,1233],{"personId":447,"order":1014,"corresponding":40,"equalContribution":40,"affiliationSnapshot":1232},"University of Colorado Boulder",{"personId":274,"order":1164,"corresponding":40,"equalContribution":40,"affiliationSnapshot":996},"report","Peerbots Research Fellowship",2025,[1238,1239,1240,1241,1242,1243,1244],"human-robot interaction","expressive faces","humanoid robots","speculative design","qualitative research","robot design","facial expression",[26,55,1246],"Robot Design",[285,29],{"website":1249},"https://www.peerbots.org/research-fellowship",["Date","2025-01-01T00:00:00.000Z"],"## Fellowship Overview\n\nDylan Thomas Doyle is the 2025 Peerbots Research Fellow. As a post-doctoral researcher at the University of Colorado Boulder, Dylan's work focuses on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity.\n\n## Research Focus\n\nThis fellowship research conducts a qualitative study examining the adoption of expressive faces for humanoid robots in industry and academia, with particular attention to systems like the Peerbots face. The study employs speculative design methods to understand the considerations and needs that decision-makers take into account when determining the facial expression capabilities of the robots they are designing.\n\n## About the Fellow\n\nDylan Thomas Doyle is an HRI researcher whose work centers on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity. Outside of research, Dylan serves as the Director of the AI for All Tomorrows media collective and podcast.\n\nDylan received his PhD from the University of Colorado Boulder, Masters of Divinity from Columbia University, and BA from Sarah Lawrence College. Prior to a career in technology research, Dylan served as a Unitarian Universalist minister and hospital chaplain.\n\n## About the Peerbots Research Fellowship\n\nThe Peerbots Research Fellowship is part of Peerbots' commitment to raising the floor of Human-Robot Interaction (HRI) research and getting more experts involved in HRI research. The program aims to support researchers centering people, and specifically experts, in how robots are researched, developed, and used. The fellowship funds an early career researcher or researcher in training to complete at least one study and submit a publication to a leading academic venue.","src/content/studies/peerbots-fellowship-1.mdx","a2060e6e5100297e"]