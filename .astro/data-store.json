[["Map",1,2,9,10,220,221,655,656,872,873,18,992,1021,1022],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.4","content-config-digest","ba0bf1054c48be48","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://semio.community/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":\"127.0.0.1\",\"port\":9876,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"prefetch\":true,\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[\"webmention.io\"],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[[null,{\"rel\":[\"nofollow\",\"noreferrer\"],\"target\":\"_blank\"}],[null,{\"theme\":{\"light\":\"rose-pine-dawn\",\"dark\":\"rose-pine\"},\"transformers\":[{\"name\":\"@shikijs/transformers:notation-diff\"},{\"name\":\"@shikijs/transformers:meta-highlight\"}]}],null],\"remarkRehype\":{\"footnoteLabelProperties\":{\"className\":[\"\"]},\"footnoteBackContent\":\"⤴\"},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{\"WEBMENTION_API_KEY\":{\"context\":\"server\",\"access\":\"secret\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_URL\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_PINGBACK\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"}},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","organizations",["Map",11,12,30,31,45,46,64,65,78,79,97,98,117,118,135,136,153,154,167,168,180,181,194,195,207,208],"george-mason-university",{"id":11,"data":13,"filePath":28,"digest":29,"deferredRender":19},{"id":11,"name":14,"shortName":15,"description":16,"type":17,"category":18,"isPartner":19,"collaborationSummary":20,"links":21,"location":23,"featured":26,"order":27},"George Mason University","GMU","George Mason University is a leading research institution partnering with Semio Community on robotics and human-robot interaction initiatives, including the development of innovative hardware platforms and advancing research in autonomous systems.","academic","research",true,"Focus areas include human-robot interaction research, hardware development, autonomous navigation, and educational programs, with key efforts like the MuSoHu (GMU Helmet) project.",{"website":22},"https://www.gmu.edu",{"city":24,"country":25},"Fairfax","United States",false,999,"src/content/organizations/george-mason-university.mdx","e4354d0c1d03bf91","ik-studio",{"id":30,"data":32,"filePath":43,"digest":44,"deferredRender":19},{"id":30,"name":33,"shortName":33,"description":34,"type":35,"category":36,"isPartner":19,"collaborationSummary":37,"links":38,"location":40,"featured":19,"order":42},"IK Studio","IK Studio is a design and innovation firm partnering with Semio Community to advance the integration of robotics into architectural and interactive environments, bridging the gap between artistic expression and technological functionality in human-robot interaction.","industry","development","Design-led partner exploring architectural robotics, interactive installations, and responsive environments that fuse artistic expression with human-robot interaction.",{"website":39},"https://www.i-k-studio.com",{"city":41,"country":25},"Philadelphia",2,"src/content/organizations/ik-studio.mdx","5acad6aa4edccf4f","kipr",{"id":45,"data":47,"filePath":60,"assetImports":61,"digest":63,"deferredRender":19},{"id":45,"name":48,"shortName":49,"description":50,"type":51,"category":52,"isPartner":19,"collaborationSummary":53,"images":54,"links":56,"location":58,"featured":26,"order":27},"KISS Institute for Practical Robotics","KIPR","The KISS Institute for Practical Robotics is a nonprofit organization partnering with Semio Community to advance educational robotics, making robotics technology accessible to students and educators worldwide through innovative programs, competitions, and curriculum development.","nonprofit","outreach","Champions educational robotics through programs like Botball and Junior Botball, emphasizing STEM access, curriculum development, and teacher training.",{"logo":55},"__ASTRO_IMAGE_@/assets/images/partners/kipr-logo.png",{"website":57},"https://www.kipr.org",{"city":59,"country":25},"Norman","src/content/organizations/kipr.mdx",[62],"@/assets/images/partners/kipr-logo.png","761d3beca4fa3922","oregon-state-university",{"id":64,"data":66,"filePath":76,"digest":77,"deferredRender":19},{"id":64,"name":67,"shortName":68,"description":69,"type":17,"category":18,"isPartner":19,"collaborationSummary":70,"links":71,"location":73,"featured":19,"order":75},"Oregon State University","OSU","Oregon State University is a premier research institution partnering with Semio Community to advance human-robot interaction, socially assistive robotics, and mobile robotics research through innovative projects and educational initiatives.","Partners on human-robot interaction, socially assistive robotics, and mobile robotics research, spanning machine learning for HRI and educational outreach.",{"website":72},"https://oregonstate.edu",{"city":74,"country":25},"Corvallis",6,"src/content/organizations/oregon-state-university.mdx","d0ae7548ad6edee8","peerbots",{"id":78,"data":80,"filePath":92,"assetImports":93,"digest":96,"deferredRender":19},{"id":78,"name":81,"shortName":81,"description":82,"type":51,"category":36,"isPartner":19,"collaborationSummary":83,"images":84,"links":87,"location":89,"featured":19,"order":91},"Peerbots","Peerbots is a nonprofit organization partnering with Semio Community to advance accessible robotics technologies and foster community-driven development in human-robot interaction, with a focus on democratizing access to social robotics.","Focuses on accessible, open-source social robotics through community-driven development, inclusive design, and educational outreach.",{"logo":85,"hero":86},"__ASTRO_IMAGE_@/assets/images/partners/peerbots-icon.png","__ASTRO_IMAGE_@/assets/images/partners/peerbots-halloween-hero.png",{"website":88},"https://peerbots.org",{"city":90,"country":25},"Boulder",1,"src/content/organizations/peerbots.mdx",[94,95],"@/assets/images/partners/peerbots-icon.png","@/assets/images/partners/peerbots-halloween-hero.png","dfa83db1691c2f57","semio-ai",{"id":97,"data":99,"filePath":112,"assetImports":113,"digest":116,"deferredRender":19},{"id":97,"name":100,"description":101,"type":35,"category":102,"isPartner":19,"collaborationSummary":103,"images":104,"links":107,"location":109,"featured":19,"order":111},"Semio AI","Semio AI is the founding industry partner of the Semio Community, providing leadership, technical infrastructure, and innovative platforms that democratize access to social robotics and advance human-robot interaction through open science and reproducible research.","infrastructure","Leads platform development, open-source infrastructure, and community programs including the Semio Platform, Community Infrastructure, and research collaboration tooling.",{"logo":105,"hero":106},"__ASTRO_IMAGE_@/assets/images/logo.png","__ASTRO_IMAGE_@/assets/images/partners/semio-hero.png",{"website":108},"https://www.semio.ai",{"city":110,"country":25},"Los Angeles",0,"src/content/organizations/semio-ai.mdx",[114,115],"@/assets/images/logo.png","@/assets/images/partners/semio-hero.png","dc0f8666dea3a742","nist",{"id":117,"data":119,"filePath":131,"assetImports":132,"digest":134,"deferredRender":19},{"id":117,"name":120,"shortName":121,"description":122,"type":123,"category":102,"isPartner":19,"collaborationSummary":124,"images":125,"links":127,"location":129,"featured":26,"order":27},"National Institute of Standards and Technology","NIST","The National Institute of Standards and Technology is a federal agency partnering with Semio Community to develop measurement science, standards, and test methods for robotics and automation, ensuring the safety, reliability, and performance of human-robot interaction systems.","government","Develops robotics measurement science, performance benchmarks, and safety standards through initiatives like HRI metrics, collaborative robot standards, and reproducible test methods.",{"logo":126},"__ASTRO_IMAGE_@/assets/images/partners/nist-logo.png",{"website":128},"https://www.nist.gov",{"city":130,"country":25},"Gaithersburg","src/content/organizations/nist.mdx",[133],"@/assets/images/partners/nist-logo.png","4a046c1db8eb7a03","ologic-inc",{"id":135,"data":137,"filePath":149,"assetImports":150,"digest":152,"deferredRender":19},{"id":135,"name":138,"shortName":139,"description":140,"type":35,"category":36,"isPartner":19,"collaborationSummary":141,"images":142,"links":144,"location":146,"featured":19,"order":148},"OLogic, Inc.","OLogic","OLogic, Inc. is a leading robotics engineering and manufacturing company partnering with Semio Community to develop, produce, and support advanced robotics hardware platforms, bringing research robots from concept to production with a focus on quality, reliability, and manufacturability.","Provides hardware development, manufacturing, and support for platforms like Quori, offering expertise in embedded systems, systems integration, and robotics production.",{"logo":143},"__ASTRO_IMAGE_@/assets/images/partners/ologic-logo.png",{"website":145},"https://www.ologicinc.com",{"city":147,"country":25},"Sunnyvale",4,"src/content/organizations/ologic-inc.mdx",[151],"@/assets/images/partners/ologic-logo.png","6f650f538fc94885","tufts-university",{"id":153,"data":155,"filePath":165,"digest":166,"deferredRender":19},{"id":153,"name":156,"shortName":157,"description":158,"type":17,"category":18,"isPartner":19,"collaborationSummary":159,"links":160,"location":162,"featured":26,"order":164},"Tufts University","Tufts","Tufts University is a premier research institution partnering with Semio Community to advance artificial intelligence, multi-agent systems, and human-AI collaboration, with particular emphasis on creating intelligent systems that can effectively understand and work alongside humans.","Collaborates on AI, human-AI collaboration, multi-agent systems, and explainable HRI research, advancing theory of mind modeling and adaptive agents.",{"website":161},"https://www.tufts.edu",{"city":163,"country":25},"Medford",5,"src/content/organizations/tufts-university.mdx","0a69a0bbf4b171a4","university-of-colorado-boulder",{"id":167,"data":169,"filePath":178,"digest":179,"deferredRender":19},{"id":167,"name":170,"shortName":171,"description":172,"type":17,"category":18,"isPartner":26,"links":173,"location":175,"featured":26,"order":27},"University of Colorado - Boulder","CU Boulder","The University of Colorado Boulder is a public research university in Boulder, Colorado, United States.",{"website":174},"https://www.colorado.edu",{"city":176,"country":177},"Boulder, CO","USA","src/content/organizations/university-of-colorado-boulder.mdx","e377702e431a5230","university-of-pennsylvania",{"id":180,"data":182,"body":191,"filePath":192,"digest":193,"deferredRender":19},{"id":180,"name":183,"shortName":184,"description":185,"type":17,"category":18,"isPartner":19,"collaborationSummary":186,"links":187,"location":189,"featured":26,"order":190},"University of Pennsylvania","UPenn","The University of Pennsylvania is a world-renowned Ivy League institution partnering with Semio Community to advance robotics research through innovative mechanical design, modular robotics, and architectural robotics initiatives.","Joint work spans modular robotics, bio-inspired mechanisms, architectural robotics, and HRI, leveraging the GRASP Lab and Weitzman School of Design.",{"website":188},"https://www.upenn.edu",{"city":41,"country":25},3,"## Partnership Overview\n\nThe University of Pennsylvania is a premier academic partner of the Semio Community, leveraging its world-class GRASP Laboratory and Weitzman School of Design to push the boundaries of robotics research and create innovative solutions at the intersection of robotics, architecture, and human interaction.\n\n## Research Excellence\n\n### GRASP Laboratory\n\nThe General Robotics, Automation, Sensing and Perception (GRASP) Lab at Penn is one of the world's leading robotics research centers, directed by Dr. Mark Yim. The lab brings together researchers from multiple disciplines to tackle fundamental challenges in robotics.\n\n#### Dr. Mark Yim - Mechanical Engineering\n- Pioneer in modular self-reconfigurable robotics\n- Development of bio-inspired robotic mechanisms\n- Innovation in soft robotics and medical applications\n- Creation of novel flying and crawling robot designs\n- Leadership in distributed robotic systems\n\n### Weitzman School of Design\n\nThe Weitzman School of Design, through faculty like Simon Kim, explores the integration of robotics into architectural and design contexts.\n\n#### Simon Kim - Architecture & Design\n- Architectural robotics and responsive environments\n- Integration of robotic systems in built spaces\n- Interactive design and spatial computing\n- Human-robot interaction in architectural contexts\n- Cross-disciplinary innovation bridging design and technology\n\n## Key Contributions\n\n### Modular Robotics Research\nPenn's groundbreaking work in modular robotics has established fundamental principles for:\n- Self-assembling and reconfigurable robot systems\n- Standardized robotic modules with interchangeable functionality\n- Distributed control algorithms for modular systems\n- Applications in search and rescue, exploration, and adaptive manufacturing\n\n### Bio-Inspired Design\nResearch initiatives that draw inspiration from nature:\n- Snake robots for navigation through complex terrains\n- Flying robots with novel propulsion mechanisms\n- Soft robots for medical and assistive applications\n- Adaptive mechanisms based on biological principles\n\n### Architectural Robotics\nInnovative work at the intersection of architecture and robotics:\n- Robots integrated into building systems\n- Interactive and responsive architectural elements\n- Spatial robotics for dynamic environments\n- Human-centered design for robotic spaces\n\n## Collaborative Impact\n\n### Research Synergies\nThe partnership between UPenn and Semio Community creates unique opportunities:\n- Access to GRASP Lab's extensive research infrastructure\n- Cross-pollination between robotics and design disciplines\n- Development of next-generation social robot platforms\n- Advancement of human-centered robotics technologies\n\n### Educational Excellence\nPenn contributes to workforce development through:\n- Graduate and undergraduate robotics programs\n- Interdisciplinary research opportunities\n- Public outreach and STEM education initiatives\n- Training programs for industry professionals\n\n## Innovation Pipeline\n\n### Current Projects\n- Development of modular components for social robots\n- Research on adaptive robotic behaviors\n- Studies in human-robot collaboration\n- Creation of robots for healthcare applications\n\n### Future Directions\n- Expansion of modular robotics platforms for HRI research\n- Integration of AI and machine learning in modular systems\n- Development of standards for reconfigurable robots\n- Exploration of robots in urban environments\n\n## Facilities and Resources\n\nThe University of Pennsylvania offers state-of-the-art facilities:\n- GRASP Laboratory with specialized robotics equipment\n- Fabrication facilities for rapid prototyping\n- Motion capture systems for interaction studies\n- Computational resources for simulation and AI\n- Testing environments for field robotics\n\n## Contact\n\nFor partnership inquiries or research collaborations with the University of Pennsylvania, please contact Dr. Mark Yim (GRASP Lab) or Simon Kim (Weitzman School of Design), or reach out through the main Semio Community channels.","src/content/organizations/university-of-pennsylvania.mdx","38f142e342d296ed","university-of-southern-california",{"id":194,"data":196,"filePath":205,"digest":206,"deferredRender":19},{"id":194,"name":197,"shortName":198,"description":199,"type":17,"category":18,"isPartner":19,"collaborationSummary":200,"links":201,"location":203,"featured":26,"order":148},"University of Southern California","USC","The University of Southern California is a leading research university partnering with Semio Community to advance socially assistive robotics, human-robot interaction, and the development of innovative robotic systems for healthcare, education, and social applications.","Leads socially assistive robotics, healthcare HRI, and embodied AI initiatives across the USC Robotics and Autonomous Systems Center and Interaction Lab.",{"website":202},"https://www.usc.edu",{"city":204,"country":177},"Los Angeles, CA","src/content/organizations/university-of-southern-california.mdx","5df93f31bef968db","yale-university",{"id":207,"data":209,"filePath":218,"digest":219,"deferredRender":19},{"id":207,"name":210,"shortName":211,"description":212,"type":17,"category":18,"isPartner":19,"collaborationSummary":213,"links":214,"location":216,"featured":26,"order":75},"Yale University","Yale","Yale University is a world-renowned Ivy League institution partnering with Semio Community to advance social robotics, human-robot interaction, and the development of assistive technologies for education, therapy, and human development.","Home of the Yale Social Robotics Lab, advancing social robotics for autism therapy, education, developmental robotics, and human-robot interaction research.",{"website":215},"https://www.yale.edu",{"city":217,"country":25},"New Haven","src/content/organizations/yale-university.mdx","94454c4850861aef","people",["Map",222,223,234,235,263,264,286,287,315,316,338,339,359,360,375,376,396,397,423,424,449,450,469,470,484,485,499,500,518,519,530,531,546,547,569,570,591,592,616,617,636,637],"andrea-alcorn",{"id":222,"data":224,"body":231,"filePath":232,"digest":233,"deferredRender":19},{"id":222,"name":225,"affiliations":226,"links":229,"featured":26,"draft":19},"Andrea Alcorn",[227],{"organizationId":135,"role":228,"isPrimary":26},"Team Member",{"email":230},"andrea@ologicinc.com","Andrea Alcorn is a key team member at OLogic, Inc., contributing to the development and support of robotics hardware and software solutions for the Semio Community.","src/content/people/andrea-alcorn.mdx","d919a9a1e34dff0c","andy-schoen",{"id":234,"data":236,"body":240,"filePath":259,"assetImports":260,"digest":262,"deferredRender":19},{"id":234,"name":237,"honorific":238,"title":239,"bio":240,"expertise":241,"affiliations":250,"links":253,"images":257,"featured":26},"Andy Schoen","Dr.","Interaction Designer and Frontend Developer","Andy Schoen is an interaction designer and frontend developer working at Semio AI. He completed his Ph.D. in Computer Science at the University of Wisconsin - Madison, working with Dr. Bilge Mutlu on human-robot interaction. His research focused on developing systems for specifying and designing robot behaviors. Andy is passionate about creating intuitive tools that empower users to effectively design and interact with robotic systems.",[242,243,244,245,246,247,248,249],"Software Engineering","System Architecture","Robotics Software","Full-Stack Development","Authoring Tools","User Experience Design","Open Source Development","Human-Robot Interaction",[251],{"organizationId":97,"role":252,"isPrimary":26},"Designer & Developer",{"email":254,"website":255,"github":256},"andy@semio.ai","https://andrewjschoen.github.io","andrewjschoen",{"avatar":258,"hero":106},"__ASTRO_IMAGE_@/assets/images/people/andy-schoen.jpg","src/content/people/andy-schoen.mdx",[261,115],"@/assets/images/people/andy-schoen.jpg","30e818fa71ae8428","bill-smart",{"id":263,"data":265,"body":283,"filePath":284,"digest":285,"deferredRender":19},{"id":263,"name":266,"honorific":267,"title":268,"bio":269,"expertise":270,"affiliations":278,"links":281,"featured":26},"Bill Smart","Prof.","Professor","Bill Smart is a Professor at Oregon State University, specializing in robotics, machine learning, and human-robot interaction. With decades of experience in the field, he has made significant contributions to mobile robotics, robot learning, and the development of practical robotic systems for real-world applications.",[271,272,249,273,274,275,276,277],"Mobile Robotics","Machine Learning for Robotics","Robot Navigation","Reinforcement Learning","Field Robotics","Robot Software Architecture","Assistive Robotics",[279],{"organizationId":64,"role":268,"department":280,"isPrimary":26},"School of Mechanical, Industrial, and Manufacturing Engineering",{"email":282},"bill.smart@oregonstate.edu","Bill Smart is a Professor at Oregon State University, specializing in robotics, machine learning, and human-robot interaction. With decades of experience in the field, he has made significant contributions to mobile robotics, robot learning, and the development of practical robotic systems for real-world applications.\n\nHis research spans a wide range of topics including autonomous navigation, reinforcement learning for robotics, and the development of robust software architectures for complex robotic systems. Bill has been instrumental in advancing the state of mobile robotics and has worked extensively on making robots more capable of operating in unstructured, real-world environments.\n\nAs an educator and mentor, Bill is committed to training the next generation of roboticists and has supervised numerous graduate students who have gone on to make their own significant contributions to the field. His practical approach to robotics research emphasizes building systems that work reliably outside the laboratory, making robotics technology more accessible and useful for society.","src/content/people/bill-smart.mdx","e66e29a54209c5bb","chris-birmingham",{"id":286,"data":288,"body":291,"filePath":311,"assetImports":312,"digest":314,"deferredRender":19},{"id":286,"name":289,"honorific":238,"title":290,"bio":291,"expertise":292,"affiliations":302,"links":305,"images":309,"featured":26},"Chris Birmingham","Human-Robot Interaction Researcher and Software Engineer","Chris Birmingham is a human-robot interaction researcher and software engineer at Semio AI. He earned a Ph.D. in Computer Science from the University of Southern California in March 2023 with a concentration in Socially Assistive Robotics. His dissertation, Multiparty Human-Robot Interaction: Methods For Facilitating Social Support, developed computational models of group dynamics (e.g., turn-taking and trust) and a facilitation framework grounded in empathy and disclosure. Chris builds open-source tools for reproducible HRI, including HCI-FACE (a web-based facial animation and conversation engine) and HARMONI (a ROS-based modular interaction framework). His work spans multimodal perception, time-series modeling for affect recognition, and end-to-end deployment across cloud and robot platforms.",[249,293,294,295,296,297,298,299,300,301,245,248],"Socially Assistive Robotics","Multimodal Perception","Affective Computing","Conversational Interfaces","Time Series Modeling","Natural Language Processing","Computer Vision","Speech Processing","ROS & Robotics Software",[303],{"organizationId":97,"role":304,"isPrimary":26},"Researcher & Engineer",{"email":306,"website":307,"github":308},"chris@semio.ai","https://chrisbirmingham.me","chrismbirmingham",{"avatar":310,"hero":106},"__ASTRO_IMAGE_@/assets/images/people/chris-birmingham.webp","src/content/people/chris-birmingham.mdx",[313,115],"@/assets/images/people/chris-birmingham.webp","0466372e836cb1b5","brian-scassellati",{"id":315,"data":317,"body":335,"filePath":336,"digest":337,"deferredRender":19},{"id":315,"name":318,"honorific":267,"title":319,"bio":320,"expertise":321,"affiliations":329,"links":332,"featured":26},"Brian Scassellati","A. Bartlett Giamatti Professor of Computer Science","Brian Scassellati is the A. Bartlett Giamatti Professor of Computer Science at Yale University and Director of the Yale Social Robotics Lab. A pioneer in social robotics, his research focuses on building embodied computational models of human social behavior, especially for assistive robotics applications in autism therapy, education, and rehabilitation.",[322,249,323,324,325,326,299,327,328],"Social Robotics","Autism Therapy Robotics","Developmental Robotics","Cognitive Science","Machine Learning","Assistive Technology","Educational Robotics",[330],{"organizationId":207,"role":268,"department":331,"isPrimary":26},"Computer Science",{"email":333,"website":334},"brian.scassellati@yale.edu","https://scazlab.yale.edu","Brian Scassellati is the A. Bartlett Giamatti Professor of Computer Science at Yale University and Director of the Yale Social Robotics Lab. A pioneer in social robotics, his research focuses on building embodied computational models of human social behavior, especially for assistive robotics applications in autism therapy, education, and rehabilitation.\n\nWith over two decades of experience in the field, Brian has made fundamental contributions to our understanding of how robots can perceive, understand, and respond to human social cues. His work on robots for autism therapy has been particularly influential, developing systems that can engage children with autism spectrum disorders in therapeutic interactions that improve social skills and communication.\n\nAs the director of the Yale Social Robotics Lab, he leads a multidisciplinary team exploring questions at the intersection of robotics, artificial intelligence, and human cognition. His research has resulted in numerous breakthroughs in areas such as joint attention, theory of mind for robots, and the development of socially assistive robots that can adapt to individual users' needs.\n\nBrian is a Fellow of the American Association for the Advancement of Science (AAAS) and has received numerous awards for his contributions to robotics and human-robot interaction. He is deeply committed to translating research into real-world applications that can improve quality of life for individuals with developmental and cognitive differences.","src/content/people/brian-scassellati.mdx","4216dffa242eb8e0","jeremy-marvel",{"id":338,"data":340,"body":356,"filePath":357,"digest":358,"deferredRender":19},{"id":338,"name":341,"honorific":238,"title":342,"bio":343,"expertise":344,"affiliations":351,"links":354,"featured":26,"draft":19},"Jeremy Marvel","Project Leader","Jeremy Marvel is a Project Leader at the National Institute of Standards and Technology (NIST), where he leads research initiatives in human-robot collaboration, robotics safety standards, and performance metrics for robotic systems.",[345,346,347,348,349,350],"Human-Robot Collaboration","Robotics Safety Standards","Performance Metrics","Industrial Robotics","Standards Development","Test Methods",[352],{"organizationId":117,"role":342,"department":353,"isPrimary":26},"Intelligent Systems Division",{"email":355},"jeremy.marvel@nist.gov","Jeremy Marvel is a Project Leader at the National Institute of Standards and Technology (NIST), where he leads research initiatives in human-robot collaboration, robotics safety standards, and performance metrics for robotic systems.\n\nHis work focuses on developing test methods and metrics for evaluating the performance and safety of collaborative robot systems, contributing to international standards development, and advancing the state of human-robot interaction in manufacturing and industrial settings.","src/content/people/jeremy-marvel.mdx","37c22ef6b3f99613","kayla-matheus",{"id":359,"data":361,"body":372,"filePath":373,"digest":374,"deferredRender":19},{"id":359,"name":362,"title":363,"bio":364,"expertise":365,"affiliations":368,"links":370,"featured":26},"Kayla Matheus","Ph.D. Candidate","Kayla Matheus is a Ph.D. Candidate at Yale University, working in the Social Robotics Lab. Her research focuses on human-robot interaction, social robotics, and developing technologies that can effectively support human social and cognitive development.",[249,322,366,328,367,327],"Child-Robot Interaction","Cognitive Development",[369],{"organizationId":207,"role":363,"department":331,"isPrimary":26},{"email":371},"kayla.matheus@yale.edu","Kayla Matheus is a Ph.D. Candidate at Yale University, working in the Social Robotics Lab under the direction of Brian Scassellati. Her research focuses on human-robot interaction, social robotics, and developing technologies that can effectively support human social and cognitive development.\n\nHer work involves designing and evaluating robotic systems for educational and therapeutic applications, with particular emphasis on supporting children with developmental differences and creating inclusive technologies that can adapt to diverse user needs.","src/content/people/kayla-matheus.mdx","681df7ec66915953","maja-mataric",{"id":375,"data":377,"body":393,"filePath":394,"digest":395,"deferredRender":19},{"id":375,"name":378,"honorific":267,"title":379,"bio":380,"expertise":381,"affiliations":387,"links":390,"featured":19},"Maja Matarić","Chan Soon-Shiong Distinguished Professor of Computer Science","Maja Matarić is the Chan Soon-Shiong Distinguished Professor of Computer Science at the University of Southern California, where she leads pioneering research in socially assistive robotics, human-robot interaction, and multi-robot systems.",[293,249,382,383,384,385,386],"Multi-Robot Systems","Rehabilitation Robotics","Robotics for Healthcare","Machine Learning for HRI","Embodied AI",[388],{"organizationId":194,"role":389,"department":331,"isPrimary":26},"Chan Soon-Shiong Distinguished Professor",{"email":391,"website":392},"mataric@usc.edu","https://robotics.usc.edu/~maja/","Maja Matarić is the Chan Soon-Shiong Distinguished Professor of Computer Science at the University of Southern California, where she leads pioneering research in socially assistive robotics, human-robot interaction, and multi-robot systems.\n\nAs the founding director of the USC Robotics and Autonomous Systems Center (RASC) and co-director of the USC Robotics Research Lab, she has made fundamental contributions to the field of socially assistive robotics, developing robot-assisted therapies for autism spectrum disorders, stroke rehabilitation, Alzheimer's disease, and healthy aging.\n\nHer work bridges computer science, neuroscience, and rehabilitation, creating innovative robotic systems that provide personalized assistance and motivation to users through social interaction rather than physical contact. She is a Fellow of multiple prestigious organizations including the AAAS, IEEE, AAAI, and ACM.","src/content/people/maja-mataric.mdx","cc3929c0a612a47c","dylan-thomas-doyle",{"id":396,"data":398,"body":418,"filePath":419,"assetImports":420,"digest":422,"deferredRender":19},{"id":396,"name":399,"honorific":238,"title":400,"bio":401,"expertise":402,"affiliations":410,"images":416,"featured":26},"Dylan Thomas Doyle","Post-Doctoral Researcher","Dylan Thomas Doyle is a post-doctoral researcher at the University of Colorado Boulder and the 2025 Peerbots Research Fellow. As an HRI researcher, Dylan's work focuses on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity.",[249,403,404,405,406,407,408,409],"Values-Based Design","Robot Identity Perception","Socio-Technical Systems","Qualitative Research","Speculative Design","Expressive Robotics","Robot Facial Expression",[411,412],{"organizationId":167,"role":400,"isPrimary":26},{"organizationId":78,"role":413,"department":414,"isPrimary":26,"startDate":415},"Research Fellow","Research Fellowship Program",["Date","2025-01-01T00:00:00.000Z"],{"avatar":417},"__ASTRO_IMAGE_@/assets/images/people/dylan-thomas-doyle.png","Dylan Thomas Doyle is a post-doctoral researcher at the University of Colorado Boulder and the 2025 Peerbots Research Fellow. As an HRI researcher, Dylan's work focuses on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity.\n\n## Research Focus\n\nDylan's fellowship research conducts a qualitative study examining the adoption of expressive faces for humanoid robots in industry and academia, with particular attention to systems like the Peerbots face. The study employs speculative design methods to understand the considerations and needs that decision-makers take into account when determining the facial expression capabilities of the robots they are designing.\n\n## Background\n\nOutside of research, Dylan serves as the Director of the AI for All Tomorrows media collective and podcast, bringing together perspectives on technology and its societal impacts.\n\nDylan received his PhD from the University of Colorado Boulder, Masters of Divinity from Columbia University, and BA from Sarah Lawrence College. This unique interdisciplinary background combines technical expertise with deep humanistic training. Prior to a career in technology research, Dylan served as a Unitarian Universalist minister and hospital chaplain, bringing a distinctive perspective on human values and care to robotics research.\n\n## Peerbots Research Fellowship\n\nAs the 2025 Peerbots Research Fellow, Dylan is conducting research that aligns with Peerbots' mission to raise the floor of Human-Robot Interaction research and center people in how robots are researched, developed, and used. The fellowship supports completion of at least one study and submission of a publication to a leading academic venue.","src/content/people/dylan-thomas-doyle.mdx",[421],"@/assets/images/people/dylan-thomas-doyle.png","30ccfa0f32c1477d","mark-yim",{"id":423,"data":425,"body":446,"filePath":447,"digest":448,"deferredRender":19},{"id":423,"name":426,"honorific":267,"title":427,"bio":428,"expertise":429,"affiliations":439,"links":443,"featured":26,"draft":19},"Mark Yim","Asa Whitney Professor of Mechanical Engineering","Mark Yim is the Asa Whitney Professor of Mechanical Engineering at the University of Pennsylvania and Director of the GRASP Lab. A pioneer in modular robotics, his research focuses on the design and control of modular self-reconfigurable robots, biologically inspired mechanisms, and novel robotic systems.",[430,431,432,433,434,435,436,437,438],"Modular Robotics","Self-Reconfigurable Robots","Bio-inspired Robotics","Mechanical Design","Robot Kinematics","Distributed Systems","Soft Robotics","Medical Robotics","Flying Robots",[440],{"organizationId":180,"role":441,"department":442,"isPrimary":26},"Asa Whitney Professor","Mechanical Engineering and Applied Mechanics",{"email":444,"website":445},"yim@seas.upenn.edu","https://www.seas.upenn.edu/~yim/","Mark Yim is the Asa Whitney Professor of Mechanical Engineering at the University of Pennsylvania and Director of the GRASP Lab. A pioneer in modular robotics, his research focuses on the design and control of modular self-reconfigurable robots, biologically inspired mechanisms, and novel robotic systems.\n\nHis groundbreaking work in modular robotics has established fundamental principles for how robots can be built from standardized, interchangeable modules that can self-assemble and reconfigure to accomplish different tasks. This work has profound implications for creating adaptable, resilient robotic systems that can morph their shape and functionality to meet changing requirements.\n\nBeyond modular robotics, Mark's research spans a diverse range of innovative robotic systems, from snake-like robots that can navigate through rubble for search and rescue, to flying robots with novel propulsion mechanisms, to soft robots for medical applications. His approach combines rigorous mechanical design principles with creative, bio-inspired solutions to create robots that push the boundaries of what's possible.\n\nAs Director of the GRASP Lab, one of the world's leading robotics research centers, Mark fosters an environment of interdisciplinary collaboration and innovation. His mentorship has produced numerous leaders in the robotics field, and his work continues to influence the development of next-generation robotic systems across academia and industry.","src/content/people/mark-yim.mdx","7e2f0d6989fb5f6b","megan-zimmerman",{"id":449,"data":451,"body":466,"filePath":467,"digest":468,"deferredRender":19},{"id":449,"name":452,"honorific":238,"title":453,"bio":454,"expertise":455,"affiliations":462,"links":464,"featured":26,"draft":19},"Megan Zimmerman","Research Engineer","Megan Zimmerman is a Research Engineer at the National Institute of Standards and Technology (NIST), where she contributes to the development of measurement science, standards, and test methods for robotics and automation systems.",[456,457,347,345,458,459,460,461],"Robotics Standards","Test Methods Development","Manufacturing Robotics","Measurement Science","Automation Systems","Safety Standards",[463],{"organizationId":117,"role":453,"department":353,"isPrimary":26},{"email":465},"megan.zimmerman@nist.gov","Megan Zimmerman is a Research Engineer at the National Institute of Standards and Technology (NIST), where she contributes to the development of measurement science, standards, and test methods for robotics and automation systems.\n\nHer work focuses on establishing rigorous, reproducible methods for evaluating robotic systems, particularly in manufacturing and collaborative environments. She plays a key role in developing standards that ensure the safety, reliability, and performance of robots working alongside humans.\n\nThrough her research at NIST, Megan helps bridge the gap between academic research and industrial applications, creating practical standards and metrics that enable the safe and effective deployment of advanced robotic systems in real-world settings. Her contributions support the broader goal of accelerating the adoption of robotics technologies while maintaining the highest standards of safety and performance.","src/content/people/megan-zimmerman.mdx","458bb629c08933a5","naomi-fitter",{"id":469,"data":471,"body":474,"filePath":482,"digest":483,"deferredRender":19},{"id":469,"name":472,"honorific":267,"title":473,"bio":474,"expertise":475,"affiliations":478,"links":480,"featured":26,"draft":19},"Naomi Fitter","Assistant Professor","Naomi Fitter is a faculty member at Oregon State University, specializing in human-robot interaction research. Her work focuses on developing socially assistive robots and studying how robots can effectively interact with and support humans in various contexts.",[249,293,476,477],"Physical Human-Robot Interaction","Robotics for Health and Wellness",[479],{"organizationId":64,"role":473,"department":280,"isPrimary":26},{"email":481},"naomi.fitter@oregonstate.edu","src/content/people/naomi-fitter.mdx","1430c77b3037dd71","nicholas-houser",{"id":484,"data":486,"body":489,"filePath":497,"digest":498,"deferredRender":19},{"id":484,"name":487,"title":488,"bio":489,"expertise":490,"affiliations":493,"links":495,"featured":26,"draft":19},"Nicholas Houser","Design Engineer","Nicholas Houser is a member of the IK Studio team, working on innovative robotics design and development projects that bridge the gap between artistic expression and technological functionality in human-robot interaction.",[491,249,433,492],"Robotics Design","Interactive Systems",[494],{"organizationId":30,"role":488,"isPrimary":26},{"email":496},"nick.houser@i-k-studio.com","src/content/people/nicholas-houser.mdx","fc6ca1ef6f42df17","reuth-mirsky",{"id":499,"data":501,"body":515,"filePath":516,"digest":517,"deferredRender":19},{"id":499,"name":502,"honorific":267,"title":473,"bio":503,"expertise":504,"affiliations":511,"links":513,"featured":26,"draft":19},"Reuth Mirsky","Reuth Mirsky is an Assistant Professor at Tufts University, specializing in artificial intelligence, multi-agent systems, and human-AI collaboration. Her research focuses on developing AI systems that can effectively understand, predict, and collaborate with humans in complex, dynamic environments.",[505,506,507,508,509,249,326,510],"Artificial Intelligence","Multi-Agent Systems","Human-AI Collaboration","Plan Recognition","Theory of Mind","Explainable AI",[512],{"organizationId":153,"role":473,"department":331,"isPrimary":26},{"email":514},"Reuth.Mirsky@tufts.edu","Reuth Mirsky is an Assistant Professor at Tufts University, specializing in artificial intelligence, multi-agent systems, and human-AI collaboration. Her research focuses on developing AI systems that can effectively understand, predict, and collaborate with humans in complex, dynamic environments.\n\nHer work bridges the gap between theoretical AI and practical applications in human-robot interaction, with particular emphasis on plan recognition, theory of mind modeling, and creating AI agents that can adapt to and work alongside human partners. She is passionate about making AI systems more transparent, trustworthy, and effective in real-world collaborative scenarios.\n\nPrior to joining Tufts, she completed her Ph.D. at Ben-Gurion University and held postdoctoral positions at UT Austin and Harvard University, building a strong interdisciplinary foundation that informs her approach to human-centered AI and robotics research.","src/content/people/reuth-mirsky.mdx","50ac7542ae553479","shelly-bagchi",{"id":518,"data":520,"body":527,"filePath":528,"digest":529,"deferredRender":19},{"id":518,"name":521,"honorific":238,"affiliations":522,"links":525,"featured":26,"draft":19},"Shelly Bagchi",[523],{"organizationId":117,"role":524,"isPrimary":26},"Researcher",{"email":526},"shelly.bagchi@nist.gov","Shelly Bagchi is a researcher at the National Institute of Standards and Technology (NIST), contributing to standards development and research initiatives in human-robot interaction and robotics technologies.","src/content/people/shelly-bagchi.mdx","3c2d4969ca279ac7","steve-goodgame",{"id":530,"data":532,"body":535,"filePath":544,"digest":545,"deferredRender":19},{"id":530,"name":533,"title":534,"bio":535,"expertise":536,"affiliations":540,"links":542,"featured":26,"draft":19},"Steve Goodgame","Executive Director","Steve Goodgame is a leader at the KISS Institute for Practical Robotics (KIPR), where he works to advance educational robotics and make robotics technology accessible to students and educators worldwide.",[328,537,538,539],"STEM Education","Robotics Competitions","Curriculum Development",[541],{"organizationId":45,"role":534,"isPrimary":26},{"email":543},"sgoodgame@kipr.org","src/content/people/steve-goodgame.mdx","f06a5b219c4e1d98","simon-kim",{"id":546,"data":548,"body":566,"filePath":567,"digest":568,"deferredRender":19},{"id":546,"name":549,"honorific":267,"title":550,"bio":551,"expertise":552,"affiliations":558,"links":564,"featured":26},"Simon Kim","Professor of Practice in Architecture / Principal","Simon Kim is a multidisciplinary researcher and designer with dual affiliations at the University of Pennsylvania's Weitzman School of Design and IK Studio. His work explores the intersection of architecture, robotics, and human-robot interaction, focusing on how robotic systems can be integrated into built environments and everyday spaces.",[553,554,555,249,556,557],"Architectural Robotics","Interactive Design","Spatial Computing","Digital Fabrication","Responsive Environments",[559,562],{"organizationId":180,"role":560,"department":561,"isPrimary":26},"Professor of Practice in Architecture","Weitzman School of Design",{"organizationId":30,"role":563,"isPrimary":26},"Principal",{"email":565,"website":39},"simonkim@design.upenn.edu","Simon Kim is a multidisciplinary researcher and designer with dual affiliations at the University of Pennsylvania's Weitzman School of Design and IK Studio. His work explores the intersection of architecture, robotics, and human-robot interaction, focusing on how robotic systems can be integrated into built environments and everyday spaces.\n\nAt UPenn, he conducts research on spatial robotics and interactive design, while at IK Studio, he leads innovative projects that combine artistic vision with technological advancement in social robotics.","src/content/people/simon-kim.mdx","748fff06fbb694fb","saad-elbeleidy",{"id":569,"data":571,"body":573,"filePath":586,"assetImports":587,"digest":590,"deferredRender":19},{"id":569,"name":572,"title":534,"bio":573,"expertise":574,"affiliations":577,"links":579,"images":583,"featured":19},"Saad Elbeleidy","Saad is the Executive Director of Peerbots, a U.S. based nonprofit organization providing a research-informed social robot platform for roboticists and non-roboticists. He received his Ph.D. from the Colorado School of Mines where he studied how therapists and educators use social robots in the wild. Now, at Peerbots, his research has expanded to broadly focus on the design and deployment of Social Robot End-User tools.",[249,322,575,576],"End-User Programming","Robotics for Therapy",[578],{"organizationId":78,"role":534,"isPrimary":26},{"email":580,"website":581,"scheduling":582},"saad@peerbots.org","https://saad.phd","https://calendar.google.com/calendar/u/0/appointments/AcZssZ15Qjxw5v1lQu8FUPKjIiZW8K7obt8RxeqlAio=",{"avatar":584,"hero":585},"__ASTRO_IMAGE_@/assets/images/people/saad-elbeleidy.jpg","__ASTRO_IMAGE_@/assets/images/partners/peerbots-hero.png","src/content/people/saad-elbeleidy.mdx",[588,589],"@/assets/images/people/saad-elbeleidy.jpg","@/assets/images/partners/peerbots-hero.png","7ddcda376df29daa","ross-mead",{"id":591,"data":593,"body":611,"filePath":612,"assetImports":613,"digest":615,"deferredRender":19},{"id":591,"name":594,"honorific":238,"title":595,"bio":596,"expertise":597,"affiliations":604,"links":606,"images":609,"featured":19},"Ross Mead","Founder and CEO","Ross Mead is the Founder and CEO of Semio AI, and Executive Director of Semio Community. With a Ph.D. in Computer Science from USC, he has dedicated his career to advancing human-robot interaction and making social robotics accessible to researchers, educators, and developers worldwide.",[249,322,598,599,600,601,602,603],"Proxemics in HRI","Multi-modal Interaction","Robot Behavior Design","Open Source Robotics","Reproducible Research","Community Building",[605],{"organizationId":97,"role":595,"isPrimary":26},{"email":607,"linkedin":608},"ross@semio.ai","rossmead",{"avatar":610,"hero":106},"__ASTRO_IMAGE_@/assets/images/people/ross-mead.jpg","Ross Mead is the CEO and Founder of Semio AI, and a driving force behind the Semio Community initiative. With a Ph.D. in Computer Science from the University of Southern California, he has dedicated his career to advancing human-robot interaction and making social robotics accessible to researchers, educators, and developers worldwide.\n\nHis research focuses on understanding and modeling the social dynamics of human-robot interaction, particularly in the areas of proxemics (spatial relationships) and multi-modal communication. Through Semio, he works to democratize access to social robotics technologies and foster reproducible, replicable research in the HRI community.\n\nAs a leader in the field, Ross is committed to building bridges between academia and industry, promoting open science practices, and creating sustainable infrastructure for the next generation of robotics researchers and practitioners. His vision for the Semio Community encompasses not just technology development, but the cultivation of an inclusive, collaborative ecosystem that advances the entire field of human-centered robotics.","src/content/people/ross-mead.mdx",[614,115],"@/assets/images/people/ross-mead.jpg","857760c590e570a9","ted-larson",{"id":616,"data":618,"body":633,"filePath":634,"digest":635,"deferredRender":19},{"id":616,"name":619,"title":620,"bio":621,"expertise":622,"affiliations":629,"links":631,"featured":26,"draft":19},"Ted Larson","CEO","Ted Larson is the CEO of OLogic, Inc., where he oversees the development and manufacturing of robotics hardware platforms. With extensive experience in embedded systems and robotics engineering, Ted plays a key role in bringing research robots like Quori from concept to production.",[623,624,625,626,627,628],"Robotics Engineering","Embedded Systems","Manufacturing","Hardware Development","Product Development","Systems Integration",[630],{"organizationId":135,"role":620,"isPrimary":26},{"email":632},"ted@ologicinc.com","Ted Larson is the CEO of OLogic, Inc., where he oversees the development and manufacturing of robotics hardware platforms. With extensive experience in embedded systems and robotics engineering, Ted plays a key role in bringing research robots like Quori from concept to production, ensuring they meet the needs of the research community while maintaining manufacturability and reliability.\n\nOLogic has been instrumental in the development and production of several key robotics platforms for the Semio Community, including the Quori social robot platform.","src/content/people/ted-larson.mdx","5ffb9d3ee9087402","xuesu-xiao",{"id":636,"data":638,"body":652,"filePath":653,"digest":654,"deferredRender":19},{"id":636,"name":639,"honorific":267,"title":473,"bio":640,"expertise":641,"affiliations":647,"links":649,"featured":26,"draft":19},"Xuesu Xiao","Xuesu Xiao is an Assistant Professor at George Mason University, specializing in mobile robotics, autonomous navigation, and learning-based approaches for robot decision-making. His research focuses on enabling robots to navigate complex, unstructured environments through innovative machine learning and planning techniques.",[271,642,272,643,644,645,275,274,646],"Autonomous Navigation","Motion Planning","Terrain Traversability","Robot Learning","Computer Vision for Robotics",[648],{"organizationId":11,"role":473,"department":331,"isPrimary":26},{"email":650,"website":651},"xiao@gmu.edu","https://cs.gmu.edu/~xiao/","Xuesu Xiao is an Assistant Professor at George Mason University, specializing in mobile robotics, autonomous navigation, and learning-based approaches for robot decision-making. His research focuses on enabling robots to navigate complex, unstructured environments through innovative machine learning and planning techniques.\n\nHis work addresses fundamental challenges in mobile robotics, including terrain traversability analysis, adaptive navigation in challenging environments, and learning-based approaches that allow robots to improve their performance through experience. By combining classical robotics techniques with modern machine learning methods, his research aims to create more capable and adaptable autonomous systems.\n\nAt George Mason University, Xuesu leads research initiatives that bridge the gap between theoretical advances in robot learning and practical applications in real-world environments. His contributions to the field include novel approaches for terrain-aware navigation, learning-based motion planning, and systems that can adapt to diverse and dynamic environments. He is actively involved in the robotics community and collaborates with researchers across academia and industry to advance the state of autonomous mobile robotics.","src/content/people/xuesu-xiao.mdx","fbbd48547bdc027b","hardware",["Map",657,658,701,702,769,770,816,817],"jibo",{"id":657,"data":659,"body":696,"filePath":697,"assetImports":698,"digest":700,"deferredRender":19},{"id":657,"name":660,"description":661,"shortDescription":662,"category":663,"status":664,"specifications":665,"features":676,"topics":686,"links":690,"images":693,"contributors":695,"featured":26,"draft":19},"Jibo","A social robot platform originally developed as the world's first commercial social robot for the home. Features an expressive animated face on a touchscreen display, three-axis motor system for fluid movement, and interactive capabilities for family engagement.","Social robot with animated touchscreen face and expressive movement","social","in-progress",{"height":666,"weight":667,"battery":668,"sensors":669,"actuators":673,"computePlatform":675},"11 inches (28 cm)","6 pounds (2.7 kg)","AC power adapter",[670,671,672],"2 color stereo cameras","360-degree microphone array","Touch sensors",[674],"3 full-revolute axes motor system","Embedded Linux system",[677,678,679,680,681,682,683,684,685],"HD LCD touchscreen display for animated face","Three-axis motor system enabling fluid turns and movements","360-degree sound localization for voice interaction","Face recognition and tracking capabilities","Full-body touch sensing","WiFi and Bluetooth connectivity","Premium stereo speakers","Full spectrum ambient LED lighting","Jibo Alive SDK for developer applications",[322,249,687,688,689],"Companion Robots","Expressive Movement","Touchscreen Interaction",{"website":691,"documentation":692},"https://www.media.mit.edu/projects/jibo-research-platform/overview/","https://hri2024.jibo.media.mit.edu",{"hero":694},"__ASTRO_IMAGE_@/assets/images/hardware/jibo-hero.jpg",[],"## Overview\n\nJibo emerged from MIT Media Lab's Personal Robots Group under the direction of Cynthia Breazeal, pioneering the concept of a social robot designed specifically for home environments. The robot's distinctive design features a stationary base with a three-axis motor system that enables fluid, lifelike movements including turns and expressive gestures. Its animated face displayed on an HD touchscreen creates engaging social interactions through dynamic expressions and eye contact tracking, while 360-degree sound localization allows natural voice interactions from anywhere in the room.\n\nThe platform has evolved into the Jibo Research Platform, providing researchers with a deployable infrastructure for social robotics experimentation and data collection. This research-oriented version extends the original hardware and software architecture with enhanced capabilities for academic study, including improved data security measures and developer tools. The system's Linux-based architecture and accompanying SDK enable researchers and developers to create custom applications and behaviors, facilitating diverse studies in human-robot interaction and social robotics applications.\n\n## Research Publications\n\n- (2024). **Jibo Community Social Robot Research Platform @Scale.** *HRI 2024 Workshop/Tutorial*, Boulder, Colorado. MIT Media Lab.\n\n- Breazeal, C. et al. **Jibo: The World's First Social Robot for the Home.** *MIT Media Lab Personal Robots Group*.","src/content/hardware/jibo.mdx",[699],"@/assets/images/hardware/jibo-hero.jpg","4f2cde20d7817cbd","quoriv1",{"id":701,"data":703,"body":763,"filePath":764,"assetImports":765,"digest":768,"deferredRender":19},{"id":704,"name":705,"description":706,"shortDescription":707,"category":663,"status":708,"specifications":709,"features":724,"topics":735,"links":740,"images":743,"contributors":746,"featured":26},"quori-v1","Quori v1","A modular, affordable socially interactive robot platform developed for enabling human-robot interaction research. Features an expressive projected face, gesturing arms with shoulder-like articulation, flexible spine, and omnidirectional mobility.","Modular social robot platform for HRI research with projected face and expressive gestures","deprecated",{"height":710,"weight":711,"battery":712,"sensors":713,"actuators":718,"computePlatform":723},"1.35 meters (resting position)","Approx. 45-50 kg","Onboard rechargeable battery system",[714,715,716,672,717],"Depth camera","RGB camera","Microphone array","Proximity sensors",[719,720,721,722],"2 DOF shoulder joints per arm","Omnidirectional base motors","Spine articulation motors","Turret rotation motor","Intel NUC with ROS integration",[725,726,727,728,729,730,731,732,733,734],"Rear-projected animated face for flexible expression design","Two gesturing arms with shoulder-like ball joint articulation (2 DOF each)","Bowing spine mechanism for body language expression","Omnidirectional mobile base (0.8 m/s linear, 180°/s rotational)","Modular panelized design with magnetic attachment system","ROS-based control interfaces at multiple abstraction levels","Browser-based content creation and animation tools","Built-in text-to-speech capabilities","ADA-compliant base dimensions","Low noise operation (quiet at 1 meter distance)",[249,322,736,737,738,739,386],"Non-verbal Communication","Projected Faces","Gesture and Expression","Research Platform",{"website":741,"documentation":742},"https://quori.org","https://quori-robot.github.io/quori_v1_documentation/",{"logo":744,"hero":745},"__ASTRO_IMAGE_@/assets/images/events/quori.png","__ASTRO_IMAGE_@/assets/images/hardware/quori.v1-hero.jpg",[747,750,753,755,757,759,761],{"type":748,"organizationId":180,"role":749,"primary":19},"organization","Lead Development",{"type":751,"personId":423,"role":752,"primary":26},"person","Principal Investigator",{"type":748,"organizationId":194,"role":754,"primary":26},"Co-Development Partner",{"type":751,"personId":375,"role":756,"primary":26},"Co-Principal Investigator",{"type":751,"personId":546,"role":758,"primary":26},"Design Lead",{"type":748,"organizationId":97,"role":760,"primary":26},"Platform Support",{"type":751,"personId":591,"role":762,"primary":26},"Software Architecture","## Overview\n\nQuori is an affordable, modular social robot platform developed through a National Science Foundation initiative to democratize human-robot interaction research. Created through extensive community consultation, the robot features a distinctive rear-projected animated face for flexible expression design, two articulated arms with shoulder-like movement, a bowing spine mechanism, and an omnidirectional mobile base capable of smooth navigation in both laboratory and real-world settings. The platform's modular architecture allows researchers to customize and extend capabilities while maintaining standardization across the ten units distributed to U.S. research institutions.\n\nThe robot bridges technical accessibility gaps by providing control interfaces at multiple levels - from low-level ROS commands for direct hardware control to browser-based tools for creating conversational content and animations without extensive programming expertise. This tiered approach, combined with open-source hardware documentation and swappable components, enables diverse research applications ranging from non-verbal communication studies to field deployments in public spaces, all while maintaining ADA compliance and quiet operation suitable for human interaction contexts.\n\n## Research Publications\n\n- Specian, A., Eckenstein, N., Mead, R., McDorman, B., Kim, S., Mataric, M., & Yim, M. (2018). **Preliminary system and hardware design for Quori, a low-cost, modular, socially interactive robot.** *2018 HRI Workshop Social Robots in the Wild*, 1-6.","src/content/hardware/quori.v1.mdx",[766,767],"@/assets/images/events/quori.png","@/assets/images/hardware/quori.v1-hero.jpg","32f35733f721a936","ommie",{"id":769,"data":771,"body":811,"filePath":812,"assetImports":813,"digest":815,"deferredRender":19},{"id":769,"name":772,"description":773,"shortDescription":774,"category":775,"status":664,"specifications":776,"features":789,"topics":796,"links":801,"images":804,"contributors":806,"featured":19},"Ommie","A novel socially assistive robot designed to support deep breathing practices for anxiety reduction. Ommie uses haptic interaction and non-verbal cues to guide users through calming breathing exercises.","Social robot for anxiety reduction through guided deep breathing","assistive",{"height":777,"weight":778,"battery":779,"sensors":780,"actuators":785,"computePlatform":788},"Approx. 0.3 meters (tabletop size)","Approx. 2-3 kg","Powered via wall adapter",[781,782,783,784],"Capacitive touch","IMU","Motor encoders","Optional camera",[786,787],"Dynamixel MX-64AT (breathing motion)","Dynamixel AX-12A (head nodding)","Raspberry Pi",[790,791,792,793,794,795],"Haptic breathing guidance through physical expansion/contraction","Non-verbal interaction through eye animations and audio chimes","Soft sweater covering for comfortable touch","Body (breathing) and head motions (nodding)","Capacitive touch inputs","Multiple breathing patterns supported (customizable)",[293,797,798,799,800],"Mental Health","Haptic Interaction","Deep Breathing","Anxiety Reduction",{"website":802,"documentation":803},"https://scazlab.yale.edu/ommie-robot","https://dl.acm.org/doi/10.1145/3706122",{"hero":805},"__ASTRO_IMAGE_@/assets/images/hardware/ommie-hero.png",[807,808,809],{"type":748,"organizationId":207,"role":749,"primary":19},{"type":751,"personId":315,"role":752,"primary":26},{"type":751,"personId":359,"role":810,"primary":26},"Research Lead","## Overview\n\nOmmie is a socially assistive robot developed at Yale University's Social Robotics Lab to help individuals manage anxiety through guided deep breathing exercises. The robot's core functionality centers on its unique haptic interaction design: it physically expands and contracts in a rhythmic breathing pattern while users place their hands on its soft, sweater-covered body. This mechanical breathing motion, combined with synchronized audio chimes and expressive eye animations, helps users naturally synchronize their own breathing to therapeutic deep breathing patterns that have been scientifically shown to calm the autonomic nervous system.\n\nInitial research with 43 participants at a university wellness center demonstrated Ommie's effectiveness, with users experiencing statistically significant reductions in anxiety state measures after interacting with the robot. Beyond the quantitative improvements, participants consistently rated the robot highly for its calming, approachable, and engaging qualities. Users particularly highlighted the focusing effect of the haptic interaction, reporting that the physical sensation helped them achieve a calmer state more quickly than traditional breathing exercises. Many participants also described experiencing a sense of companionship during the interaction, comparing it to the motivational benefits of group meditation. The robot has shown promise beyond adult anxiety management, with ongoing applications in pediatric healthcare settings where it has been successfully deployed to help children manage stress during oral allergy challenges. Recent work has also explored the application of machine learning algorithms to detect deep breathing phase for monitoring and adjusting deep breathing practices. Future work involves expanding use of the robot to new populations benefitting from deep breathing, as well as more longitudinal, in-the-wild experimentation.\n\n## Research Publications\n\n- **Beilenson, J., Sahin, R., Zhong, Y., Brewer, R., Tang, F., Nguyen, L., Juca, M., Kofinas, D., Nelson, J., Scassellati, B.** (2024). A Socially Assistive Robot to Help Children Cope with Medical Procedures: Exploring the Effect of the Robot's Embodiment. In Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI '24).\n\n- **Nelson, J., Sahin, R., & Scassellati, B.** (2023). Ommie: A Socially Assistive Robot for Deep Breathing. In Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction (HRI '23).","src/content/hardware/ommie.mdx",[814],"@/assets/images/hardware/ommie-hero.png","50933b4bc8942824","quoriv2",{"id":816,"data":818,"body":867,"filePath":868,"assetImports":869,"digest":871,"deferredRender":19},{"id":819,"name":820,"description":821,"shortDescription":822,"category":663,"status":664,"specifications":823,"features":835,"topics":847,"links":849,"images":850,"contributors":852,"featured":19},"quori-v2","Quori v2","An advanced modular socially interactive robot platform with enhanced sensing and interaction capabilities. Features a touchscreen display, dual speakers, 14-DOF articulation including 4-DOF arms, multiple IMUs and laser rangefinders, programmable light arrays, and 3-DOF holonomic mobility.","Advanced modular social robot with touchscreen, enhanced sensors, and 14-DOF articulation",{"height":710,"weight":711,"battery":712,"sensors":824,"actuators":828,"computePlatform":834},[825,716,826,827],"RGB+D camera","Laser rangefinders (x2)","IMUs (x4)",[829,830,831,832,833],"14 DOFs total","2-DOF head","1-DOF neck","4-DOF arms (x2)","3-DOF holonomic base","Onboard computer",[836,837,838,839,840,841,842,843,844,845,846,730,731],"Touchscreen display for interactive communication","Dual speakers for enhanced audio output","Two articulated arms with 4 DOF each for complex gestures","2-DOF head with 1-DOF neck for expressive movements","3-DOF holonomic base for smooth omnidirectional mobility","Dual laser rangefinders for precise navigation","Four IMUs for enhanced motion sensing and stability","Four programmable light arrays for visual feedback","Onboard storage bin for carrying items","Customizable badge for personalization","Enhanced modular design and functionality",[249,322,738,689,386,848],"Multi-modal Communication",{"website":741,"documentation":742},{"logo":744,"hero":851},"__ASTRO_IMAGE_@/assets/images/hardware/quori.v2-hero.png",[853,854,856,858,860,862,863,865,866],{"type":751,"personId":423,"role":752,"primary":26},{"type":748,"organizationId":64,"role":855,"primary":19},"Community Development",{"type":748,"organizationId":135,"role":857,"primary":26},"Design For Manufacturing",{"type":748,"organizationId":30,"role":859,"primary":26},"Industrial Design",{"type":751,"personId":263,"role":861,"primary":26},"Co-Investigator",{"type":751,"personId":469,"role":861,"primary":26},{"type":748,"organizationId":97,"role":864,"primary":26},"Software Development",{"type":751,"personId":591,"role":861,"primary":26},{"type":751,"personId":546,"role":861,"primary":26},"## Overview\n\nQuori v2.0 represents a significant advancement in socially interactive robot platforms, building upon the successful foundation established by the National Science Foundation initiative. This enhanced version features a comprehensive sensor suite including an RGB+D camera, dual laser rangefinders, and four IMUs for superior environmental perception and stability. The robot's interaction capabilities have been dramatically expanded with a touchscreen display for direct user engagement, dual speakers for immersive audio experiences, and four programmable light arrays that enable rich visual communication patterns.\n\nThe mechanical design has evolved to offer 14 degrees of freedom, including sophisticated 4-DOF arms (8 DOF total) for complex gestural expression, a 2-DOF head with 1-DOF neck for nuanced non-verbal communication, and a 3-DOF holonomic base for smooth omnidirectional navigation. Additional features like the onboard storage bin enable practical applications in service scenarios, while the customizable badge system allows for personalization in multi-robot deployments. This modular architecture maintains backward compatibility while offering researchers unprecedented flexibility in customizing and extending the platform's capabilities for diverse human-robot interaction studies.\n\n## Research Publications\n\n- Specian, A., Eckenstein, N., Mead, R., McDorman, B., Kim, S., Mataric, M., & Yim, M. (2018). **Preliminary system and hardware design for Quori, a low-cost, modular, socially interactive robot.** *2018 HRI Workshop Social Robots in the Wild*, 1-6.","src/content/hardware/quori.v2.mdx",[766,870],"@/assets/images/hardware/quori.v2-hero.png","148672749d0b7910","software",["Map",874,875,934,935],"arora",{"id":874,"data":876,"body":929,"filePath":930,"assetImports":931,"digest":933,"deferredRender":19},{"id":874,"name":877,"description":878,"shortDescription":879,"category":880,"status":664,"license":881,"language":882,"platform":886,"requirements":890,"features":899,"topics":908,"links":913,"images":917,"contributors":919,"featured":19,"publishDate":928},"Arora","A software-as-a-service platform for creating, deploying, and executing multimodal natural language-based applications for interactive personal robots and digital/virtual characters. Provides tools for voice scripting, animation, and custom functionality development.","SaaS platform for natural language robot applications","framework","MIT",[883,884,885],"Python","TypeScript","JavaScript",[887,888,889],"Linux","Docker","Cloud",{"runtime":891,"hardware":894,"dependencies":896},[892,893],"Node.js 18+","Python 3.8+",[895],"Compatible robot platform",[897,898],"ROS/ROS2 (optional)","WebRTC",[900,901,902,903,904,905,906,907],"Voice script creation for conversational HRI","Animation tools for expressive movements","JavaScript SDK for custom functionality","Real-time simulation and testing","Multi-robot fleet orchestration","Cloud-based deployment pipeline","Usage analytics and monitoring","Natural language interaction engine",[909,910,249,911,912],"Conversational AI","Multi-robot Coordination","Behavior Animation","SaaS Platforms",{"website":914,"github":915,"documentation":916},"https://semio.ai","https://github.com/semio-community/arora","https://arora.readthedocs.io",{"hero":918},"__ASTRO_IMAGE_@/assets/images/software/arora-hero.png",[920,922,924,926],{"type":748,"organizationId":97,"role":921,"primary":19},"Lead Developer",{"type":751,"personId":591,"role":923,"primary":26},"Product Visionary",{"type":751,"personId":286,"role":925,"primary":26},"Engineer",{"type":751,"personId":234,"role":927,"primary":26},"User Experience",["Date","2023-06-15T00:00:00.000Z"],"## Overview\n\nArora is Semio's comprehensive software-as-a-service platform designed to streamline the development and deployment of natural language-based applications for robots and virtual characters. Built on the principle that natural language should be the primary interface for human-robot interaction, Arora provides an integrated suite of tools that significantly reduces development time - aiming to cut content creation time by 60%, software integration by 75%, and product launch timelines by 90%.\n\nThe platform enables developers and researchers to rapidly prototype and deploy sophisticated robot behaviors through its three core modules: voice scripting for conversational interactions, animation tools for expressive movements, and a JavaScript SDK for custom functionality. This unified approach allows teams to create, test in simulation, deploy to hardware, and analyze usage data all within a single ecosystem, making it ideal for both research institutions conducting HRI studies and companies developing commercial robot applications.","src/content/software/arora.mdx",[932],"@/assets/images/software/arora-hero.png","85a7dc4747a11d61","vizij",{"id":934,"data":936,"body":987,"filePath":988,"assetImports":989,"digest":991,"deferredRender":19},{"id":934,"name":937,"description":938,"shortDescription":939,"category":880,"status":664,"license":881,"language":940,"platform":941,"requirements":945,"features":954,"topics":966,"links":971,"images":973,"contributors":975,"featured":19,"publishDate":986},"Vizij","An open-source ecosystem of tools that provide a pipeline for building, animating, sharing, and deploying rendered robot faces. Defines a standardized-yet-modular, user-informed rigging and controller system including eye gaze, visemes, and emotional expression capabilities.","Open-source ecosystem for rendered robot faces",[883,885,884],[887,942,943,944],"Windows","macOS","Web",{"runtime":946,"hardware":948,"dependencies":950},[893,947],"Node.js 16+",[949],"Display device for robot face rendering",[951,952,953],"WebGL","Three.js","Blender (optional)",[955,956,957,958,959,960,961,962,963,964,965],"Standardized rigging system for robot faces","Face-agnostic abstraction layers","Built-in support for eye gaze control","Viseme generation for lip synchronization","Emotional expression mapping","Multi-screen face rendering support","Import/export for GLTF/GLB formats","Face designer and rig designer interfaces","Animation recording and playback","Programmatic control APIs","Community-standard rig templates",[967,249,968,969,970],"Rendered Robot Faces","Emotion Expression","Gaze Control","Open Source",{"github":972},"https://github.com/vizij-ai",{"logo":974},"__ASTRO_IMAGE_@/assets/images/software/vizij-icon.png",[976,978,979,981,983,985],{"type":751,"personId":569,"role":977,"primary":19},"Co-Lead Developer",{"type":751,"personId":234,"role":977,"primary":26},{"type":751,"personId":286,"role":980,"primary":26},"Core Developer",{"type":751,"personId":591,"role":982,"primary":26},"Project Advisor",{"type":748,"organizationId":78,"role":984,"primary":26},"Development Partner",{"type":748,"organizationId":97,"role":984,"primary":26},["Date","2024-09-01T00:00:00.000Z"],"## Overview\n\nVizij is an open-source ecosystem designed to address the fragmentation in robot face development by providing a unified platform for designing, animating, and deploying expressive rendered robot faces. Unlike existing solutions that require researchers to create custom, non-standard tools for each project, Vizij offers a comprehensive pipeline that enables code sharing, collaboration between diverse stakeholders, and the development of reusable components that can advance the entire field of human-robot interaction.\n\nAt its core, Vizij implements a data-flow architecture with multiple abstraction layers, allowing different roles—from 3D designers to HRI researchers—to contribute their expertise without needing to understand the entire system. The platform provides standardized interfaces for three critical aspects of robot facial expression: emotion mapping, viseme generation for speech synchronization, and gaze control for joint attention and turn-taking behaviors.\n\n## Key Components\n\n### Face Properties & Rendering\nThe base layer handles the rendering of primitive shapes and elements that compose the robot face, supporting both single and multi-screen configurations. Faces can be imported from standard 3D formats (GLTF/GLB) created in tools like Blender, or composed using Vizij's built-in component system.\n\n### Rigging System\nVizij introduces a dual-layer rigging approach:\n- **Face-Specific Rigs**: Map high-level properties to low-level rendering values for individual face designs\n- **Abstract Rigs**: Provide face-agnostic control interfaces that work across different robot platforms\n\n### Standard Control Interfaces\nThe platform defines community standards for:\n- **Emotion Expression**: Supporting multiple emotion models including FACS, PAD, and layered approaches\n- **Visemes**: Enabling lip synchronization for educational and communication applications\n- **Gaze Systems**: Implementing joint attention, turn-taking, and intimacy regulation behaviors\n\n## Supported Roles\n\nVizij is designed to accommodate various stakeholder roles in the robot face development pipeline:\n\n- **Designers**: Create and modify face elements using existing 3D modeling tools\n- **Face Riggers**: Connect rig systems to specific face elements\n- **Abstraction Riggers**: Define face-agnostic control abstractions\n- **Animators**: Create behaviors and animations for face elements\n- **Interaction Designers**: Design the overall user experience and interaction flows\n- **Developers**: Programmatically control faces through well-defined APIs\n\n## Research Support\n\nDeveloped with support from NSF grants (IIS-2235042/2235043), Vizij enables replicable scientific studies by allowing researchers to share face assets, rigs, and animations. The standardized interfaces mean that code developed for one robot face can be easily adapted to work with others, significantly improving research reproducibility and accelerating development cycles.","src/content/software/vizij.mdx",[990],"@/assets/images/software/vizij-icon.png","741cbfc2f8406934",["Map",993,994],"peerbots-fellowship-1",{"id":993,"data":995,"body":1017,"filePath":1018,"assetImports":1019,"digest":1020,"deferredRender":19},{"id":996,"title":997,"description":998,"contributors":999,"type":1003,"topics":1004,"organizations":1008,"relatedSoftware":1012,"links":1013,"images":1015,"featured":26,"draft":26,"publishDate":1016},"peerbots-expressive-faces","Examining the Adoption of Expressive Faces for Humanoid Robots in Industry and Academia","A qualitative study examining the adoption of expressive faces, like the Peerbots face, for humanoid robots in industry and academia. Using speculative design methods, this research focuses on understanding the needs that decision-makers are taking into consideration about the facial expression of the robots they are designing.",[1000,1002],{"personId":396,"order":91,"corresponding":26,"equalContribution":26,"affiliationSnapshot":1001},"University of Colorado Boulder",{"personId":569,"order":42,"corresponding":26,"equalContribution":26,"affiliationSnapshot":81},"study",[249,1005,1006,407,406,1007],"Expressive Faces","Humanoid Robots","Robot Design",[1009,1011],{"organizationId":78,"role":1010},"funding",{"organizationId":97,"role":1010},[934],{"website":1014},"https://www.peerbots.org/research-fellowship",{"logo":85},["Date","2025-11-01T07:00:00.000Z"],"## Fellowship Overview\n\nDylan Thomas Doyle is the 2025 Peerbots Research Fellow. As a post-doctoral researcher at the University of Colorado Boulder, Dylan's work focuses on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity.\n\n## Research Focus\n\nThis fellowship research conducts a qualitative study examining the adoption of expressive faces for humanoid robots in industry and academia, with particular attention to systems like the Peerbots face. The study employs speculative design methods to understand the considerations and needs that decision-makers take into account when determining the facial expression capabilities of the robots they are designing.\n\n## About the Fellow\n\nDylan Thomas Doyle is an HRI researcher whose work centers on values-based design for robots and the impacts of socio-technical contexts on the perception of robot identity. Outside of research, Dylan serves as the Director of the AI for All Tomorrows media collective and podcast.\n\nDylan received his PhD from the University of Colorado Boulder, Masters of Divinity from Columbia University, and BA from Sarah Lawrence College. Prior to a career in technology research, Dylan served as a Unitarian Universalist minister and hospital chaplain.\n\n## About the Peerbots Research Fellowship\n\nThe Peerbots Research Fellowship is part of Peerbots' commitment to raising the floor of Human-Robot Interaction (HRI) research and getting more experts involved in HRI research. The program aims to support researchers centering people, and specifically experts, in how robots are researched, developed, and used. The fellowship funds an early career researcher or researcher in training to complete at least one study and submit a publication to a leading academic venue.","src/content/research/peerbots-fellowship-1.mdx",[94],"ae47c94cf0273796","events",["Map",1023,1024,1052,1053,1078,1079,1096,1097,1113,1114,1135,1136,1154,1155,1174,1175,1193,1194,1213,1214,1233,1234,1253,1254,1272,1273,1291,1292,1315,1316,1333,1334,1355,1356,1384,1385,1405,1406,1425,1426,1444,1445,1463,1464,1484,1485,1502,1503,1518,1519,1537,1538,1559,1560,1578,1579,1598,1599,1617,1618,1635,1636,1657,1658,1677,1678,1696,1697,1713,1714,1736,1737,1756,1757,1779,1780,1798,1799],"aaai-2023-fss",{"id":1023,"data":1025,"filePath":1048,"assetImports":1049,"digest":1051,"deferredRender":19},{"name":1026,"displayName":1027,"description":1028,"type":1029,"startDate":1030,"endDate":1031,"location":1032,"roles":1034,"images":1036,"links":1038,"featured":26,"topics":1040},"AAAI 2023 Fall Symposium Series","AAAI 2023 FSS","AAAI’s 2023 Fall Symposium Series met at the Westin Arlington Gateway on October 25–27 to host coordinated symposia on agent teaming, climate-smart AI, human-robot interaction, trustworthy human-centered AI, cognitive architectures with generative models, survival prediction, and unified robot application representations.","conference",["Date","2023-10-25T00:00:00.000Z"],["Date","2023-10-27T00:00:00.000Z"],{"city":1033,"country":177,"online":26},"Arlington, VA",[1035],"attendee",{"logo":1037},"__ASTRO_IMAGE_@/assets/images/events/aaai.png",{"website":1039},"https://aaai.org/conference/fall-symposia/aaai-2023-fall-symposium-series/",[1041,1042,1043,1044,1045,1046,1047],"agent teaming in mixed-motive situations","ai and climate sustainability","ai for human-robot interaction","assured and trustworthy human-centered ai","cognitive architectures and generative models","survival prediction algorithms","unifying representations for robot application development","src/content/events/aaai-2023-fss.mdx",[1050],"@/assets/images/events/aaai.png","3e48a0257f44c71b","aaai-2023-sss",{"id":1052,"data":1054,"filePath":1075,"assetImports":1076,"digest":1077,"deferredRender":19},{"name":1055,"displayName":1056,"description":1057,"type":1029,"startDate":1058,"endDate":1059,"location":1060,"roles":1062,"images":1065,"links":1066,"featured":26,"topics":1068},"AAAI 2023 Spring Symposium Series","AAAI 2023 SSS","The Association for the Advancement of Artificial Intelligence hosted the 2023 Spring Symposium Series at the Hyatt Regency San Francisco Airport on March 27–29, featuring focused gatherings on topics such as climate tipping-point discovery, AI trustworthiness assessment, and the fusion of machine learning with knowledge engineering.",["Date","2023-03-27T00:00:00.000Z"],["Date","2023-03-29T00:00:00.000Z"],{"city":1061,"country":177,"online":26},"Palo Alto, CA",[1063,1064,1035],"organizer","speaker",{"logo":1037},{"website":1067},"https://aaai.org/conference/spring-symposia/sss23",[1069,1070,1071,1072,1073,1074],"ai climate tipping-point discovery","ai trustworthiness assessment","machine learning and knowledge engineering","evaluation of generalist systems","temporal logics on finite traces","socially responsible ai","src/content/events/aaai-2023-sss.mdx",[1050],"876c74f53a041516","aaai-2024-fss",{"id":1078,"data":1080,"filePath":1093,"assetImports":1094,"digest":1095,"deferredRender":19},{"name":1081,"displayName":1082,"description":1083,"type":1029,"startDate":1084,"endDate":1085,"location":1086,"roles":1087,"images":1088,"links":1089,"featured":26,"topics":1091},"AAAI 2024 Fall Symposium Series","AAAI 2024 FSS","The Association for the Advancement of Artificial Intelligence hosted the 2024 Fall Symposium Series at the Westin Arlington Gateway in Arlington, Virginia from November 7–9, featuring coordinated symposia on emerging topics across artificial intelligence.",["Date","2024-11-07T00:00:00.000Z"],["Date","2024-11-09T00:00:00.000Z"],{"city":1033,"country":177,"online":26},[1035],{"logo":1037},{"website":1090},"https://aaai.org/conference/fall-symposia/fss24/",[1092],"artificial intelligence","src/content/events/aaai-2024-fss.mdx",[1050],"191599b246cb48ba","aaai-2025-fss",{"id":1096,"data":1098,"filePath":1110,"assetImports":1111,"digest":1112,"deferredRender":19},{"name":1099,"displayName":1100,"description":1101,"type":1029,"startDate":1102,"endDate":1103,"location":1104,"roles":1105,"images":1106,"links":1107,"featured":26,"topics":1109},"AAAI 2025 Fall Symposium Series","AAAI 2025 FSS","The Association for the Advancement of Artificial Intelligence hosts the 2025 Fall Symposium Series at the Westin Arlington Gateway in Arlington, Virginia on November 6–8, bringing together multiple focused symposia to explore emerging directions in artificial intelligence research.",["Date","2025-11-06T00:00:00.000Z"],["Date","2025-11-08T00:00:00.000Z"],{"city":1033,"country":177,"online":26},[1035],{"logo":1037},{"website":1108},"https://aaai.org/conference/fall-symposia/fss25/",[1092],"src/content/events/aaai-2025-fss.mdx",[1050],"e473b028eeb98135","advancing-hri-research-and-benchmarking-2023",{"id":1113,"data":1115,"filePath":1133,"digest":1134,"deferredRender":19},{"name":1116,"displayName":1117,"description":1118,"type":1119,"startDate":1120,"endDate":1121,"location":1122,"roles":1125,"links":1126,"featured":26,"topics":1128},"HRI 2023 Workshop on Advancing Human-Robot Interaction Research and Benchmarking Through Open-Source Ecosystems","Advancing HRI Research and Benchmarking Through Open-Source Ecosystems","This HRI 2023 workshop examined how open-source ecosystems, shared datasets, and governed communities can remove roadblocks to benchmarking human-robot interaction—especially at the intersection of HRI and robot manipulation.","workshop",["Date","2023-03-13T00:00:00.000Z"],["Date","2023-03-13T00:00:00.000Z"],{"city":1123,"country":1124,"online":26},"Stockholm","Sweden",[1035],{"website":1127},"https://www.robot-manipulation.org/hri-2023",[1129,1130,1131,1132],"open-source ecosystems","hri benchmarking","shared datasets","robot manipulation","src/content/events/advancing-hri-research-and-benchmarking-2023.mdx","639ac7fe0eab4d31","actuate-2025",{"id":1135,"data":1137,"filePath":1152,"digest":1153,"deferredRender":19},{"name":1138,"displayName":1138,"description":1139,"type":1029,"startDate":1140,"endDate":1141,"location":1142,"roles":1144,"links":1145,"featured":26,"topics":1147},"Actuate 2025","Actuate 2025 is a developer-focused conference dedicated to physical AI, showcasing the latest autonomous robotics breakthroughs and bringing together engineering leaders building the next generation of robots.",["Date","2025-09-23T00:00:00.000Z"],["Date","2025-09-24T00:00:00.000Z"],{"city":1143,"country":177,"online":26},"San Francisco, CA",[1035],{"website":1146},"https://actuate.foxglove.dev",[1148,1149,1150,1151],"physical ai","autonomous robotics","robotics engineering","developer conference","src/content/events/actuate-2025.mdx","f62cfa0c7ebdabe4","ces-2024",{"id":1154,"data":1156,"filePath":1172,"digest":1173,"deferredRender":19},{"name":1157,"displayName":1158,"description":1159,"type":1029,"startDate":1160,"endDate":1161,"location":1162,"roles":1164,"links":1165,"featured":26,"topics":1167},"Consumer Electronics Show","CES 2024","CES 2024, produced by the Consumer Technology Association, is the flagship tech trade show where innovators unveil breakthrough technologies, strike partnerships, and offer immersive demos across the entire tech landscape.",["Date","2024-01-09T00:00:00.000Z"],["Date","2024-01-12T00:00:00.000Z"],{"city":1163,"country":177,"online":26},"Las Vegas, NV",[1035],{"website":1166},"https://www.ces.tech",[1168,1169,1170,1171],"technology innovation","trade show","immersive demos","global partnerships","src/content/events/ces-2024.mdx","ceafc9e3e94ef6c3","chatbot-conference-2024",{"id":1174,"data":1176,"filePath":1191,"digest":1192,"deferredRender":19},{"name":1177,"displayName":1177,"description":1178,"type":1029,"startDate":1179,"endDate":1180,"location":1181,"roles":1182,"links":1183,"featured":26,"topics":1185},"Chatbot Conference 2024","Chatbot Conference 2024 returns to San Francisco for three days of live sessions and workshops on building state-of-the-art AI agents, large language model experiences, and conversational AI products.",["Date","2024-09-24T00:00:00.000Z"],["Date","2024-09-26T00:00:00.000Z"],{"city":1143,"country":177,"online":26},[1035],{"website":1184},"https://www.chatbotconference.com",[1186,1187,1188,1189,1190],"ai agents","large language models","conversational ux","ai voice","hands-on workshops","src/content/events/chatbot-conference-2024.mdx","6b2d89a0ee45b723","dialogue-with-robots-2023",{"id":1193,"data":1195,"filePath":1211,"digest":1212,"deferredRender":19},{"name":1196,"displayName":1196,"description":1197,"type":1119,"startDate":1198,"endDate":1199,"location":1200,"roles":1202,"links":1203,"featured":26,"topics":1205},"Dialogue with Robots","Dialogue with Robots brings together researchers in speech, NLP, robotics, and HRI to advance human-AI communication, continuing the momentum from NSF-funded workshops and a Dagstuhl Seminar on spoken interaction with robots.",["Date","2023-04-07T00:00:00.000Z"],["Date","2023-04-08T00:00:00.000Z"],{"city":1201,"country":177,"online":26},"Pittsburgh, PA",[1035],{"website":1204},"https://www.malihealikhani.com/dialogue-with-robots",[1206,1207,1208,1209,1210],"spoken dialog systems","multimodal interaction","situated language understanding","human-robot communication ethics","benchmarking tasks for dialogue","src/content/events/dialogue-with-robots-2023.mdx","7f0b12576ffc0055","emerging-test-methods-metrics-accessible-hri-2023",{"id":1213,"data":1215,"filePath":1231,"digest":1232,"deferredRender":19},{"name":1216,"displayName":1217,"description":1218,"type":1119,"startDate":1219,"endDate":1220,"location":1221,"roles":1222,"links":1223,"featured":26,"topics":1225},"HRI 2023 Workshop on Emerging Test Methods & Metrics for Accessible HRI","Emerging Test Methods & Metrics for Accessible HRI","This HRI 2023 workshop focused on accessibility barriers in human-centric robotics, exploring test methods, metrics, and standards efforts that broaden participation and improve comparability across HRI systems.",["Date","2023-03-13T00:00:00.000Z"],["Date","2023-03-13T00:00:00.000Z"],{"city":1123,"country":1124,"online":26},[1063,1035],{"website":1224},"https://hri-methods-metrics.github.io",[1226,1227,1228,1229,1230],"accessibility in hri","test methods and metrics","standards development","community knowledge sharing","human factors","src/content/events/emerging-test-methods-metrics-accessible-hri-2023.mdx","269dac41164f2f28","hri-2023",{"id":1233,"data":1235,"filePath":1251,"digest":1252,"deferredRender":19},{"name":1236,"displayName":1237,"description":1238,"type":1029,"startDate":1239,"endDate":1240,"location":1241,"roles":1242,"links":1243,"featured":26,"topics":1245},"18th IEEE/ACM International Conference on Human-Robot Interaction","HRI 2023","HRI 2023, themed “HRI for all,” convened the global human-robot interaction community in Stockholm and online to advance inclusive theories, methods, designs, and technologies through papers, demos, workshops, and student programs.",["Date","2023-03-13T00:00:00.000Z"],["Date","2023-03-16T00:00:00.000Z"],{"city":1123,"country":1124,"online":26},[1064,1035],{"website":1244},"https://humanrobotinteraction.org/2023",[1246,1247,1248,1249,1250],"inclusive hri","technical papers","demos and videos","workshops","student programs","src/content/events/hri-2023.mdx","02eada6e22714e47","hri-2024-pioneers",{"id":1253,"data":1255,"filePath":1270,"digest":1271,"deferredRender":19},{"name":1256,"displayName":1256,"description":1257,"type":1119,"startDate":1258,"endDate":1259,"location":1260,"roles":1261,"links":1263,"featured":26,"topics":1265},"HRI 2024 Pioneers","The 19th annual HRI Pioneers Workshop met on March 11, 2024 alongside HRI 2024, offering a hybrid forum where top student researchers share work, receive mentorship, and build collaborations across the human-robot interaction community.",["Date","2024-03-11T00:00:00.000Z"],["Date","2024-03-11T00:00:00.000Z"],{"city":176,"country":177,"online":26},[1262],"sponsor",{"website":1264},"https://hripioneers.org/archives/hri24",[1266,1267,1268,1269],"human-robot interaction","student research","mentorship","hybrid participation","src/content/events/hri-2024-pioneers.mdx","73cd4db55e51b3f5","hri-2024",{"id":1272,"data":1274,"filePath":1289,"digest":1290,"deferredRender":19},{"name":1275,"displayName":1276,"description":1277,"type":1029,"startDate":1278,"endDate":1279,"location":1280,"roles":1281,"links":1282,"featured":26,"topics":1284},"19th ACM/IEEE International Conference on Human-Robot Interaction","HRI 2024","HRI 2024 gathered the ACM/IEEE human-robot interaction community in Boulder, Colorado—with hybrid participation—for keynotes, technical papers, workshops, competitions, and design challenges highlighting the field’s latest theory, technology, and applications.",["Date","2024-03-11T00:00:00.000Z"],["Date","2024-03-15T00:00:00.000Z"],{"city":176,"country":177,"online":26},[1262,1035],{"website":1283},"https://humanrobotinteraction.org/2024",[1247,1285,1286,1287,1288],"workshops and tutorials","videos and demos","robot competition","student design challenge","src/content/events/hri-2024.mdx","06bbbbc1196ce34d","hri-2025-industry-day",{"id":1291,"data":1293,"filePath":1311,"assetImports":1312,"digest":1314,"deferredRender":19},{"name":1294,"displayName":1294,"description":1295,"type":1029,"startDate":1296,"endDate":1297,"location":1298,"roles":1301,"images":1302,"links":1304,"featured":26,"topics":1306},"HRI 2025 Industry Day","Industry Day at HRI 2025 is a half-day program of interactive panels, lightning talks, table discussions, and networking designed to spark collaborations among robotics companies, investors, and the HRI community.",["Date","2025-03-03T00:00:00.000Z"],["Date","2025-03-03T00:00:00.000Z"],{"city":1299,"country":1300,"online":26},"Melbourne","Australia",[1063,1064,1035],{"logo":1303},"__ASTRO_IMAGE_@/assets/images/events/hri.2025.png",{"website":1305},"https://humanrobotinteraction.org/2025/industry-day",[1307,1308,1309,1310],"industry partnerships","lightning talks","networking sessions","robotics companies","src/content/events/hri-2025-industry-day.mdx",[1313],"@/assets/images/events/hri.2025.png","a66482c819b8fdd6","hri-2025-pioneers",{"id":1315,"data":1317,"filePath":1330,"assetImports":1331,"digest":1332,"deferredRender":19},{"name":1318,"displayName":1318,"description":1319,"type":1119,"startDate":1320,"endDate":1321,"location":1322,"roles":1323,"images":1325,"links":1326,"featured":26,"topics":1328},"HRI 2025 Pioneers","The 20th HRI Pioneers Workshop meets on March 3, 2025 during HRI 2025 to empower emerging human-robot interaction scholars through cohort presentations, mentorship, and community building.",["Date","2025-03-03T00:00:00.000Z"],["Date","2025-03-03T00:00:00.000Z"],{"city":1299,"country":1300,"online":26},[1064,1324,1262],"panelist",{"logo":1303},{"website":1327},"https://hripioneers.org/archives/hri25",[1266,1267,1268,1329],"community building","src/content/events/hri-2025-pioneers.mdx",[1313],"0d7f5dcb3d681860","hri-2025-quori-community-feedback",{"id":1333,"data":1335,"filePath":1352,"assetImports":1353,"digest":1354,"deferredRender":19},{"name":1336,"displayName":1337,"description":1338,"type":1029,"startDate":1339,"endDate":1340,"location":1341,"roles":1342,"images":1344,"links":1345,"featured":26,"topics":1347},"HRI 2025 “Birds of a Feather” Session on Quori Community Feedback","HRI 2025 Quori Community Feedback","A Birds of a Feather gathering at HRI 2025 that invites the community to share feedback on the Quori social robot platform and help steer its community-driven hardware and software roadmap.",["Date","2025-03-05T00:00:00.000Z"],["Date","2025-03-05T00:00:00.000Z"],{"city":1299,"country":1300,"online":26},[1343,1063,1064,1035],"host",{"logo":1303},{"website":1346},"https://quori.org/community-input-meetings",[1348,1349,1350,1351],"community feedback","social robot platform","hardware and software input","human-robot interaction research","src/content/events/hri-2025-quori-community-feedback.mdx",[1313],"18c8fca2e432c5c9","hri-2025",{"id":1355,"data":1357,"body":1380,"filePath":1381,"assetImports":1382,"digest":1383,"deferredRender":19},{"name":1358,"displayName":1359,"description":1360,"type":1029,"startDate":1361,"endDate":1362,"location":1363,"roles":1365,"images":1367,"links":1368,"featured":26,"topics":1372},"HRI 2025: ACM/IEEE International Conference on Human-Robot Interaction","HRI 2025","The 20th Annual ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for presenting and discussing cutting-edge research in human-robot interaction. HRI 2025 brings together researchers, practitioners, and industry leaders to share the latest advances in HRI theory, methods, technologies, and applications.",["Date","2025-03-04T00:00:00.000Z"],["Date","2025-03-06T00:00:00.000Z"],{"venue":1364,"city":1299,"country":1300,"online":19},"Melbourne Convention and Exhibition Centre",[1063,1064,1366,1262,1035],"exhibitor",{"logo":1303},{"website":1369,"registration":1370,"program":1371},"https://humanrobotinteraction.org/2025/","https://humanrobotinteraction.org/2025/registration","https://humanrobotinteraction.org/2025/program",[1373,322,1374,1375,1376,1377,328,1378,599,1379],"Human-Robot Interaction Theory","Robot Design and Aesthetics","Ethics and Trust in HRI","Collaborative Robotics","Healthcare and Assistive Robotics","Field Studies and Applications","Robot Learning from Humans","## About HRI 2025\n\nThe ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for showcasing the very best interdisciplinary and multidisciplinary research in human-robot interaction. Researchers from diverse backgrounds including robotics, computer science, engineering, design, behavioral and social sciences come together to define and advance the state-of-the-art in HRI.\n\n## Conference Theme: \"Robots in the Wild\"\n\nHRI 2025's theme focuses on deploying robots in real-world, uncontrolled environments. As robots move from laboratories into homes, workplaces, and public spaces, understanding how they interact with diverse populations in complex, dynamic settings becomes crucial.","src/content/events/hri-2025.mdx",[1313],"d8e947b7974eefd7","hri-2026-pioneers",{"id":1384,"data":1386,"filePath":1401,"assetImports":1402,"digest":1404,"deferredRender":19},{"name":1387,"displayName":1387,"description":1388,"type":1119,"startDate":1389,"endDate":1390,"location":1391,"roles":1394,"images":1395,"links":1397,"featured":26,"topics":1399},"HRI 2026 Pioneers","The 21st annual Human-Robot Interaction Pioneers Workshop will take place on March 16, 2026 alongside the ACM/IEEE International Conference on Human-Robot Interaction. The program fosters creativity and collaboration on key HRI challenges by empowering student researchers to share their work with distinguished peers and senior scholars.",["Date","2026-03-16T00:00:00.000Z"],["Date","2026-03-16T00:00:00.000Z"],{"city":1392,"country":1393,"online":26},"Edinburgh, Scotland","UK",[1262],{"logo":1396},"__ASTRO_IMAGE_@/assets/images/events/hri.2026.png",{"website":1398},"https://hripioneers.org/archives/hri26",[1266,1267,1400],"collaboration","src/content/events/hri-2026-pioneers.mdx",[1403],"@/assets/images/events/hri.2026.png","ba6e5333eb477301","hri-2026",{"id":1405,"data":1407,"filePath":1422,"assetImports":1423,"digest":1424,"deferredRender":19},{"name":1408,"displayName":1409,"description":1410,"type":1029,"startDate":1411,"endDate":1412,"location":1413,"roles":1414,"images":1415,"links":1416,"featured":19,"topics":1418},"21st ACM/IEEE International Conference on Human-Robot Interaction","HRI 2026","The ACM/IEEE International Conference on Human-Robot Interaction (HRI) is the premier venue for innovations on human-robot interaction, bringing together researchers across robotics, human-computer interaction, human factors, artificial intelligence, engineering, and the social and behavioral sciences. The 21st edition emphasizes the theme “HRI Empowering Society,” focusing on ethically integrating robots into everyday processes and making robot technologies accessible to the public.",["Date","2026-03-16T00:00:00.000Z"],["Date","2026-03-19T00:00:00.000Z"],{"city":1392,"country":1393,"online":26},[1063,1366,1262,1035],{"logo":1396},{"website":1417},"https://humanrobotinteraction.org/2026",[1266,1419,1420,1421,1379],"societal impact","ethical integration","robot accessibility","src/content/events/hri-2026.mdx",[1403],"0a4d9ab18ecfcdff","hri-best-practices-2025",{"id":1425,"data":1427,"filePath":1442,"digest":1443,"deferredRender":19},{"name":1428,"displayName":1429,"description":1430,"type":1119,"startDate":1431,"endDate":1432,"location":1433,"roles":1436,"links":1437,"featured":26,"topics":1439},"RO-MAN 2025 Workshop on Best Practices for Enabling Reproducible & Replicable Studies in Human-Robot Interaction","HRI Best Practices","An IEEE RO-MAN 2025 workshop advancing community efforts to strengthen reproducibility, replicability, and stability within HRI research through formal reporting guidelines, recommended practices, and strategies for conducting high-value replications.",["Date","2025-08-25T00:00:00.000Z"],["Date","2025-08-25T00:00:00.000Z"],{"city":1434,"country":1435,"online":26},"Eindhoven","Netherlands",[1063,1035],{"website":1438},"https://sites.google.com/oregonstate.edu/ro-man2025workshopbestpractice",[1440,1441,1266],"reproducibility","replicability","src/content/events/hri-best-practices-2025.mdx","ee3a63e930995f4b","hri-in-academia-and-industry-aaai-2023",{"id":1444,"data":1446,"filePath":1461,"digest":1462,"deferredRender":19},{"name":1447,"displayName":1448,"description":1449,"type":1029,"startDate":1450,"endDate":1451,"location":1452,"roles":1453,"links":1454,"featured":26,"topics":1456},"AAAI 2023 Spring Symposium Series on HRI in Academia and Industry: Bridging the Gap","HRI in Academia and Industry (AAAI 2023)","This AAAI 2023 Spring Symposium gathered HRI researchers and practitioners from academia and industry to compare constraints, share priorities, and identify ways to collaborate on deploying human-robot interaction at scale.",["Date","2023-03-27T00:00:00.000Z"],["Date","2023-03-29T00:00:00.000Z"],{"city":1061,"country":177,"online":26},[1063,1064,1035],{"website":1455},"https://sites.google.com/view/aaai-hri-bridge",[1457,1458,1459,1460],"industry hri constraints","academic hri priorities","collaboration across ip boundaries","hri education and training","src/content/events/hri-in-academia-and-industry-aaai-2023.mdx","26860c6dd2c6d65f","hri-in-academia-and-industry-roman-2023",{"id":1463,"data":1465,"filePath":1482,"digest":1483,"deferredRender":19},{"name":1466,"displayName":1467,"description":1468,"type":1029,"startDate":1469,"endDate":1470,"location":1471,"roles":1474,"links":1475,"featured":26,"topics":1477},"RO-MAN 2023 Special Session on HRI in Academia and Industry: Bridging the Gap","HRI in Academia and Industry (RO-MAN 2023)","This RO-MAN 2023 special session explored how academic and industrial HRI teams can align goals, navigate constraints, and collaborate on real-world deployments of human-robot interaction.",["Date","2023-08-29T00:00:00.000Z"],["Date","2023-08-29T00:00:00.000Z"],{"city":1472,"country":1473,"online":26},"Busan","Korea",[1063,1035],{"website":1476},"https://ro-man2023.org/paperSubmission/callForSpecialSession",[1478,1479,1480,1481],"industry hri challenges","academic-industrial collaboration","deployment constraints","shared tools and publications","src/content/events/hri-in-academia-and-industry-roman-2023.mdx","ccecd2dc6772d2f9","humanoids-summit-2024",{"id":1484,"data":1486,"filePath":1500,"digest":1501,"deferredRender":19},{"name":1487,"displayName":1487,"description":1488,"type":1029,"startDate":1489,"endDate":1490,"location":1491,"roles":1493,"links":1494,"featured":26,"topics":1496},"Humanoids Summit 2024","Humanoids Summit 2024 filled the Computer History Museum in Silicon Valley with live humanoid demonstrations, keynote talks, panel discussions, and startup pitches featuring leaders from robotics, AI, and investment.",["Date","2024-12-11T00:00:00.000Z"],["Date","2024-12-12T00:00:00.000Z"],{"city":1492,"country":177,"online":26},"Silicon Valley, CA",[1324,1035],{"website":1495},"https://humanoidssummit.com/hs2024agenda",[1497,1498,1499],"humanoid robotics","industry leadership","live demonstrations","src/content/events/humanoids-summit-2024.mdx","c8a2eced2dd300ac","humanoids-summit-2025",{"id":1502,"data":1504,"filePath":1516,"digest":1517,"deferredRender":19},{"name":1505,"displayName":1505,"description":1506,"type":1029,"startDate":1507,"endDate":1508,"location":1509,"roles":1510,"links":1511,"featured":26,"topics":1513},"Humanoids Summit 2025","Humanoids Summit returns to Silicon Valley at the Computer History Museum with live humanoid demonstrations and sessions featuring global leaders advancing mass production, capital investment, and ecosystem growth for humanoid robotics.",["Date","2025-12-11T00:00:00.000Z"],["Date","2025-12-12T00:00:00.000Z"],{"city":1492,"country":177,"online":26},[1262,1035],{"website":1512},"https://humanoidssummit.com",[1497,1514,1515],"mass production","robotics investment","src/content/events/humanoids-summit-2025.mdx","2767b05cb3db7ccc","icra-2025",{"id":1518,"data":1520,"filePath":1535,"digest":1536,"deferredRender":19},{"name":1521,"displayName":1522,"description":1523,"type":1029,"startDate":1524,"endDate":1525,"location":1526,"roles":1528,"links":1529,"featured":26,"topics":1531},"2025 IEEE International Conference on Robotics and Automation","ICRA 2025","ICRA 2025, the flagship conference of the IEEE Robotics and Automation Society, gathers leading researchers and industry professionals in Atlanta to share ideas and advance robotics and automation for the benefit of humanity.",["Date","2025-05-19T00:00:00.000Z"],["Date","2025-05-23T00:00:00.000Z"],{"city":1527,"country":177,"online":26},"Atlanta, GA",[1064,1035],{"website":1530},"https://2025.ieee-icra.org",[1532,1533,1534],"robotics","automation","industry collaboration","src/content/events/icra-2025.mdx","69d03da8219f43ab","icsr-ai-2024",{"id":1537,"data":1539,"filePath":1557,"digest":1558,"deferredRender":19},{"name":1540,"displayName":1541,"description":1542,"type":1029,"startDate":1543,"endDate":1544,"location":1545,"roles":1548,"links":1549,"featured":26,"topics":1551},"16th International Conference on Social Robotics + Artificial Intelligence","ICSR+AI 2024","ICSR+AI 2024 meets in Odense under the theme “Empowering Humanity: The Role of Social and Collaborative Robotics in Shaping Our Future,” welcoming contributions across the technologies, studies, and artistic engagements that define social robotics and AI.",["Date","2024-10-23T00:00:00.000Z"],["Date","2024-10-26T00:00:00.000Z"],{"city":1546,"country":1547,"online":26},"Odense","Denmark",[1063,1035],{"website":1550},"https://icsr2024.dk",[1552,1266,1553,1554,1555,1556],"collaborative robots","affective and cognitive sciences","software frameworks for social robots","telepresence and remote interaction","real-world hri experiments","src/content/events/icsr-ai-2024.mdx","361158d41f822e64","icsr-ai-2025",{"id":1559,"data":1561,"filePath":1576,"digest":1577,"deferredRender":19},{"name":1562,"displayName":1563,"description":1564,"type":1029,"startDate":1565,"endDate":1566,"location":1567,"roles":1570,"links":1571,"featured":26,"topics":1573},"17th International Conference on Social Robotics + Artificial Intelligence","ICSR+AI 2025","ICSR+AI 2025 welcomes researchers to Naples from September 10–12 to explore advances in social robotics and AI under the theme “Emotivation at the Core: Empowering Social Robots to Inspire and Connect,” highlighting the interplay of emotion and motivation in building trust, empathy, and engagement.",["Date","2025-09-10T00:00:00.000Z"],["Date","2025-09-12T00:00:00.000Z"],{"city":1568,"country":1569,"online":26},"Naples","Italy",[1262],{"website":1572},"https://icsr2025.eu",[1574,1092,1575,1266],"social robotics","emotion and motivation","src/content/events/icsr-ai-2025.mdx","8bc59cc0dda3cec2","quori-community-workshop-2025",{"id":1578,"data":1580,"filePath":1595,"assetImports":1596,"digest":1597,"deferredRender":19},{"name":1581,"displayName":1581,"description":1582,"type":1119,"startDate":1583,"endDate":1584,"location":1585,"roles":1587,"images":1588,"links":1589,"featured":26,"topics":1590},"Quori Community Workshop","The Quori Community Workshop advances community-informed design of the Quori social robot platform through hands-on discussions of hardware, software, and behavior primitives that support human-robot interaction research.",["Date","2025-04-16T00:00:00.000Z"],["Date","2025-04-17T00:00:00.000Z"],{"city":1586,"country":177,"online":26},"Madison, WI",[1343,1063,1064,1262,1035],{"logo":744},{"website":1346},[1591,1592,1593,1351,1594],"community-informed design","social robot hardware","interaction software systems","ethical platform development","src/content/events/quori-community-workshop-2025.mdx",[766],"c52c5bb1fa554ff3","replication-studies-of-hri-research-2024",{"id":1598,"data":1600,"filePath":1615,"digest":1616,"deferredRender":19},{"name":1601,"displayName":1602,"description":1603,"type":1119,"startDate":1604,"endDate":1605,"location":1606,"roles":1608,"links":1609,"featured":26,"topics":1611},"RO-MAN 2024 Workshop on Towards Conducting Non-Trivial, Multi-Site Replication Studies of Human-Robot Interaction Research","Replication Studies of HRI Research","This RO-MAN 2024 workshop builds a community around replication science in HRI, outlining how to plan high-value, multi-site studies and culminating in a shared playbook for conducting meaningful replications.",["Date","2024-08-30T00:00:00.000Z"],["Date","2024-08-30T00:00:00.000Z"],{"city":1607,"country":177,"online":26},"Pasadena, CA",[1063,1035],{"website":1610},"https://sites.google.com/oregonstate.edu/roman24-hri-replication",[1612,1440,1613,1329,1614],"replication science","multi-site studies","hri standards","src/content/events/replication-studies-of-hri-research-2024.mdx","0cd7eb75c4b08c70","ro-man-2024",{"id":1617,"data":1619,"filePath":1633,"digest":1634,"deferredRender":19},{"name":1620,"displayName":1621,"description":1622,"type":1029,"startDate":1623,"endDate":1624,"location":1625,"roles":1626,"links":1627,"featured":26,"topics":1629},"33rd IEEE International Conference on Robot and Human Interactive Communication","RO-MAN 2024","RO-MAN 2024 in Pasadena is a leading forum for state-of-the-art advances in human-robot interaction, spanning theory, technology, empirical studies, and the social impact of collaborative and socially intelligent robots.",["Date","2024-08-26T00:00:00.000Z"],["Date","2024-08-30T00:00:00.000Z"],{"city":1607,"country":177,"online":26},[1063,1035],{"website":1628},"https://www.ro-man2024.org",[1266,1630,1631,1632],"robotics technology","cognitive and social sciences","ethics and policy","src/content/events/ro-man-2024.mdx","fa494f3c0022e618","robot-design-competition-2024",{"id":1635,"data":1637,"filePath":1655,"digest":1656,"deferredRender":19},{"name":1638,"displayName":1639,"description":1640,"type":1641,"startDate":1642,"endDate":1643,"location":1644,"roles":1645,"links":1648,"featured":26,"topics":1650},"ICSR+AI 2024 Robot Design Competition","Robot Design Competition 2024","ICSR 2024’s flagship Robot Design Competition invites innovative ideas, working prototypes, and impactful applications that show how social robots can enrich daily life across domains such as education, wellbeing, and tourism.","competition",["Date","2024-10-25T00:00:00.000Z"],["Date","2024-10-25T00:00:00.000Z"],{"city":1546,"country":1547,"online":26},[1646,1647,1035],"winner","competitor",{"website":1649},"https://icsr2024.dk/index.php/robot-design-competition",[1651,1652,1653,1654],"innovative robot design","social robotics applications","prototype development","competition showcase","src/content/events/robot-design-competition-2024.mdx","f57792ec10637f5c","ros4sr-2024",{"id":1657,"data":1659,"filePath":1675,"digest":1676,"deferredRender":19},{"name":1660,"displayName":1661,"description":1662,"type":1029,"startDate":1663,"endDate":1664,"location":1665,"roles":1666,"links":1667,"featured":26,"topics":1669},"ICSR+AI 2024 Special Session on ROS for Social Robots","ROS4SR","ROS4SR is a special session at ICSR 2024 that convenes researchers, developers, and practitioners to share ROS advances for social robots, co-located with ROSCon to encourage continuity between the communities.",["Date","2024-10-24T00:00:00.000Z"],["Date","2024-10-24T00:00:00.000Z"],{"city":1546,"country":1547,"online":26},[1063,1035],{"website":1668},"https://icsr2024.dk/index.php/ros4sr",[1670,1671,1672,1673,1674],"ros integration for social robots","ros tool development","multimodal interaction support","real-time social interaction","applications of ros in social robotics","src/content/events/ros4sr-2024.mdx","f9107802a040b480","sr-unboxed-2025",{"id":1677,"data":1679,"filePath":1694,"digest":1695,"deferredRender":19},{"name":1680,"displayName":1681,"description":1682,"type":1119,"startDate":1683,"endDate":1684,"location":1685,"roles":1686,"links":1687,"featured":26,"topics":1689},"ICSR+AI 2025 Workshop on Social Robots Unboxed: What Do We Want from Social Robot Products?","SR Unboxed 2025","This half-day workshop brings researchers, industry experts, and end-users together at the University of Naples Parthenope to examine how social robot products can better bridge technical capabilities and user expectations, debating applications, ethics, and human-centric roadmaps.",["Date","2025-09-10T00:00:00.000Z"],["Date","2025-09-10T00:00:00.000Z"],{"city":1568,"country":1569,"online":26},[1063,1064,1324],{"website":1688},"https://socialrobot-kros.github.io/srunboxed_icsr2025",[1690,1691,1692,1693],"social robots","user experience","ethics","human-centric design","src/content/events/sr-unboxed-2025.mdx","9b7339fc479cfa11","rss-2025",{"id":1696,"data":1698,"filePath":1711,"digest":1712,"deferredRender":19},{"name":1699,"displayName":1700,"description":1701,"type":1029,"startDate":1702,"endDate":1703,"location":1704,"roles":1705,"links":1706,"featured":26,"topics":1708},"21st Conference on Robotics: Science and Systems","RSS 2025","Robotics: Science and Systems 2025 returns to the University of Southern California in Los Angeles on June 21–25, bringing together the global robotics community for single-track keynotes, technical sessions, workshops, and tutorials.",["Date","2025-06-21T00:00:00.000Z"],["Date","2025-06-25T00:00:00.000Z"],{"city":204,"country":177,"online":26},[1366,1262,1035],{"website":1707},"https://roboticsconference.org/2025",[1532,1709,1249,1710],"technical sessions","tutorials","src/content/events/rss-2025.mdx","eda45fd77f536e8e","tahri-2024",{"id":1713,"data":1715,"filePath":1732,"assetImports":1733,"digest":1735,"deferredRender":19},{"name":1716,"displayName":1717,"description":1718,"type":1029,"startDate":1719,"endDate":1720,"location":1721,"roles":1722,"images":1723,"links":1725,"featured":26,"topics":1727},"2024 International Symposium on Technological Advances in Human-Robot Interaction","TAHRI 2024","The inaugural Technological Advances in Human-Robot Interaction symposium convenes in Boulder ahead of HRI 2024 to spotlight the hardware, software, sensors, and systems that enable human-robot interaction.",["Date","2024-03-09T00:00:00.000Z"],["Date","2024-03-10T00:00:00.000Z"],{"city":176,"country":177,"online":26},[1343,1063,1262,1035],{"logo":1724},"__ASTRO_IMAGE_@/assets/images/events/tahri.png",{"website":1726},"https://www.tahri.org",[1728,1729,1730,1731],"technological hri","robot hardware and software","sensors and perception","interaction frameworks","src/content/events/tahri-2024.mdx",[1734],"@/assets/images/events/tahri.png","8efb866da30f2e4f","unitree-g1-humanoid-training-2025",{"id":1736,"data":1738,"filePath":1754,"digest":1755,"deferredRender":19},{"name":1739,"displayName":1740,"description":1741,"type":1119,"startDate":1742,"endDate":1743,"location":1744,"roles":1747,"links":1748,"featured":26,"topics":1750},"Unitree G1 Humanoid Certificate Training","Unitree G1 Humanoid Training","Hands-on training that teaches participants to program the Unitree G1 Humanoid Robot in three days, combining simulation, real-robot exercises, and reinforcement learning workflows.",["Date","2025-11-06T00:00:00.000Z"],["Date","2025-11-07T00:00:00.000Z"],{"city":1745,"country":1746,"online":26},"Barcelona","Spain",[1035],{"website":1749},"https://www.theconstruct.ai/unitree-g1-humanoid-robotics-certificate-training-2025",[1751,1752,1753],"humanoid robots","reinforcement learning","robot programming","src/content/events/unitree-g1-humanoid-training-2025.mdx","f06993d24ddc3a0c","ur-rad-2023",{"id":1756,"data":1758,"filePath":1775,"assetImports":1776,"digest":1778,"deferredRender":19},{"name":1759,"displayName":1760,"description":1761,"type":1029,"startDate":1762,"endDate":1763,"location":1764,"roles":1765,"images":1766,"links":1768,"featured":26,"topics":1770},"AAAI 2023 Fall Symposium on Unifying Representations for Robot Application Development","UR-RAD 2023","UR-RAD 2023 convened at the Westin Arlington Gateway in Arlington, Virginia on October 25–27 to address how representations contextualize robot tasks, support reusable skills, and ensure verifiable behavior for robot application developers.",["Date","2023-10-25T00:00:00.000Z"],["Date","2023-10-27T00:00:00.000Z"],{"city":1033,"country":177,"online":26},[1063,1035],{"logo":1767},"__ASTRO_IMAGE_@/assets/images/events/ur-rad.2025.png",{"website":1769},"https://sites.google.com/view/aaai-ur-rad-23-symposium",[1771,1772,1773,1774],"robot representations","standardization of development","end-user programming interfaces","ai planning and formal methods","src/content/events/ur-rad-2023.mdx",[1777],"@/assets/images/events/ur-rad.2025.png","3e64c50fda7c7096","ur-rad-2025",{"id":1779,"data":1781,"filePath":1795,"assetImports":1796,"digest":1797,"deferredRender":19},{"name":1782,"displayName":1783,"description":1784,"type":1029,"startDate":1785,"endDate":1786,"location":1787,"roles":1788,"images":1789,"links":1790,"featured":19,"topics":1792},"AAAI 2025 Fall Symposium on Unifying Representations for Robot Application Development","UR-RAD 2025","UR-RAD 2025 convenes the AAAI community at the Westin Arlington Gateway in Arlington, Virginia on November 6–8, 2025 to advance unified representations for robot application development. The symposium emphasizes contextualizing robot tasks, supporting reusable skills, ensuring verifiable behavior, and connecting representation techniques across robotics and artificial intelligence.",["Date","2025-11-06T00:00:00.000Z"],["Date","2025-11-08T00:00:00.000Z"],{"city":1033,"country":177,"online":26},[1063,1262,1035],{"logo":1767},{"website":1791},"https://ur-rad.github.io/",[1771,1793,1794,1092],"skill reuse","behavior verification","src/content/events/ur-rad-2025.mdx",[1777],"827843b6f31f7a80","ur-rad-2024",{"id":1798,"data":1800,"filePath":1812,"assetImports":1813,"digest":1814,"deferredRender":19},{"name":1801,"displayName":1802,"description":1803,"type":1029,"startDate":1804,"endDate":1805,"location":1806,"roles":1807,"images":1808,"links":1809,"featured":26,"topics":1811},"AAAI 2024 Fall Symposium on Unifying Representations for Robot Application Development","UR-RAD 2024","UR-RAD 2024 convened at the Westin Arlington Gateway in Arlington, Virginia on November 7–9 to address how representations contextualize robot tasks, support reusable skills, and ensure verifiable behavior for robot application developers.",["Date","2024-11-07T00:00:00.000Z"],["Date","2024-11-09T00:00:00.000Z"],{"city":1033,"country":177,"online":26},[1063,1035],{"logo":1767},{"website":1810},"https://sites.google.com/view/aaai-ur-rad-24-symposium",[1771,1793,1794],"src/content/events/ur-rad-2024.mdx",[1777],"2b737cedb073656f"]