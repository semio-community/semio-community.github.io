[["Map",1,2,9,10,111,112,151,152,225,226],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.1","content-config-digest","483ae5495a417c99","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://semio-community.github.io/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":true,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"prefetch\":true,\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[\"webmention.io\"],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[[null,{\"rel\":[\"nofollow\",\"noreferrer\"],\"target\":\"_blank\"}],[null,{\"theme\":{\"light\":\"rose-pine-dawn\",\"dark\":\"rose-pine\"},\"transformers\":[{\"name\":\"@shikijs/transformers:notation-diff\"},{\"name\":\"@shikijs/transformers:meta-highlight\"}]}],null],\"remarkRehype\":{\"footnoteLabelProperties\":{\"className\":[\"\"]},\"footnoteBackContent\":\"⤴\"},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{\"WEBMENTION_API_KEY\":{\"context\":\"server\",\"access\":\"secret\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_URL\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_PINGBACK\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"}},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","hardware",["Map",11,12,74,75],"ommie",{"id":11,"data":13,"body":69,"filePath":70,"assetImports":71,"digest":73,"deferredRender":67},{"name":14,"description":15,"shortDescription":16,"category":17,"status":18,"specifications":19,"features":32,"applications":39,"researchAreas":44,"links":50,"images":53,"maintainers":55,"institutions":57,"tags":59,"featured":67,"publishDate":68},"Ommie","A novel socially assistive robot designed to support deep breathing practices for anxiety reduction. Ommie uses haptic interaction and non-verbal cues to guide users through calming breathing exercises.","Social robot for anxiety reduction through guided deep breathing","assistive","experimental",{"height":20,"weight":21,"battery":22,"sensors":23,"actuators":28,"computePlatform":31},"Approx. 0.3 meters (tabletop size)","Approx. 2-3 kg","Powered via wall adapter",[24,25,26,27],"Capacitive touch","IMU","Motor encoders","Optional camera",[29,30],"Dynamixel MX-64AT (breathing motion)","Dynamixel AX-12A (head nodding)","Raspberry Pi",[33,34,35,36,37,38],"Haptic breathing guidance through physical expansion/contraction","Non-verbal interaction through eye animations and audio chimes","Soft sweater covering for comfortable touch","Body (breathing) and head motions (nodding)","Capacitive touch inputs","Multiple breathing patterns supported (customizable)",[40,41,42,43],"Anxiety reduction","Deep breathing","Stress management","Mental health support",[45,46,47,48,49],"Socially Assistive Robotics","Human-Robot Interaction","Mental Health Technology","Haptic Interaction","Therapeutic Robotics",{"documentation":51,"website":52},"https://dl.acm.org/doi/10.1145/3706122","https://scazlab.yale.edu/ommie-robot",{"hero":54},"__ASTRO_IMAGE_@/assets/images/hardware/ommie-hero.png",[56],"Yale University Social Robotics Lab",[58],"Yale University",[60,61,62,63,64,65,66],"mental-health","therapeutic","haptic","social-robot","anxiety","breathing","wellness",true,["Date","2025-02-19T00:00:00.000Z"],"## Overview\n\nOmmie is a socially assistive robot developed at Yale University's Social Robotics Lab to help individuals manage anxiety through guided deep breathing exercises. The robot's core functionality centers on its unique haptic interaction design: it physically expands and contracts in a rhythmic breathing pattern while users place their hands on its soft, sweater-covered body. This mechanical breathing motion, combined with synchronized audio chimes and expressive eye animations, helps users naturally synchronize their own breathing to therapeutic deep breathing patterns that have been scientifically shown to calm the autonomic nervous system.\n\nInitial research with 43 participants at a university wellness center demonstrated Ommie's effectiveness, with users experiencing statistically significant reductions in anxiety state measures after interacting with the robot. Beyond the quantitative improvements, participants consistently rated the robot highly for its calming, approachable, and engaging qualities. Users particularly highlighted the focusing effect of the haptic interaction, reporting that the physical sensation helped them achieve a calmer state more quickly than traditional breathing exercises. Many participants also described experiencing a sense of companionship during the interaction, comparing it to the motivational benefits of group meditation. The robot has shown promise beyond adult anxiety management, with ongoing applications in pediatric healthcare settings where it has been successfully deployed to help children manage stress during oral allergy challenges. Recent work has also explored the application of machine learning algorithms to detect deep breathing phase for monitoring and adjusting deep breathing practices. Future work involves expanding use of the robot to new populations benefitting from deep breathing, as well as more longitudinal, in-the-wild experimentation.\n\n## Research Publications\n\n- Chun, A., Mamantov, E., Stahl, U., Scassellati, B., & Leeds, S. (2025). **Breathe Easy: Harnessing Robots for Stress Reduction During Pediatric Oral Challenges.** *Journal of Allergy and Clinical Immunology*, 155(2).\n\n- Matheus, K., Vázquez, M., & Scassellati, B. (2024). **Ommie: The Design and Development of a Social Robot for Anxiety Reduction.** *ACM Transactions on Human-Robot Interaction*.\n\n- Matheus, K., Mamantov, E., Vázquez, M., & Scassellati, B. (2023). **Deep Breathing Phase Classification with a Social Robot for Mental Health.** *International Conference on Multimodal Interaction (ICMI '23)*, Paris, France.\n\n- Matheus, K., Vázquez, M., & Scassellati, B. (2022). **A Social Robot for Anxiety Reduction via Deep Breathing.** *31st IEEE International Conference on Robot Human Interactive Communication (RO-MAN)*, Naples, Italy. **[Best Student Paper Award]**","src/content/hardware/ommie.mdx",[72],"@/assets/images/hardware/ommie-hero.png","5c6f410816ae50ff","quoriv1",{"id":74,"data":76,"body":106,"filePath":107,"assetImports":108,"digest":110,"deferredRender":67},{"name":77,"description":78,"shortDescription":79,"category":80,"status":81,"features":82,"applications":86,"researchAreas":89,"links":91,"images":94,"maintainers":96,"institutions":98,"tags":101,"featured":67,"publishDate":105},"Quori v1.0","A socially interactive robot platform designed for human-robot interaction research","Social robot platform for HRI research","social","available",[83,84,85],"Social interaction capabilities","Research-grade platform","Community supported",[87,88],"Human-robot interaction research","Social robotics studies",[46,90],"Social Robotics",{"documentation":92,"github":93},"https://docs.quori.org","https://github.com/semio-community/quori",{"hero":95},"__ASTRO_IMAGE_@/assets/images/hardware/quori.v1-hero.jpg",[97],"Semio Community",[99,100],"University of Pennsylvania","University of Southern California",[102,103,104],"social-robotics","hri","research",["Date","2024-01-15T00:00:00.000Z"],"## Overview\n\nQuori is a socially interactive robot designed for human-robot interaction research. It provides researchers with a capable and standardized platform for conducting HRI studies.\n\n## Features\n\n- Expressive upper body for social interaction\n- Mobile base for navigation\n- Comprehensive sensor suite\n- Open-source software stack\n- Community support and documentation\n\n## Getting Started\n\nFor documentation and resources, visit the [Quori documentation site](https://docs.quori.org).","src/content/hardware/quori.v1.mdx",[109],"@/assets/images/hardware/quori.v1-hero.jpg","03a98c8ba69b8aa5","software",["Map",113,114],"arora",{"id":113,"data":115,"body":148,"filePath":149,"digest":150,"deferredRender":67},{"name":116,"description":117,"shortDescription":118,"category":119,"status":120,"license":121,"language":122,"platform":125,"features":128,"useCases":132,"links":136,"maintainers":140,"institutions":141,"tags":142,"featured":67,"lastUpdate":146,"publishDate":147},"Arora","A robotics orchestration platform for managing multi-robot systems","Orchestration platform for multi-robot systems","framework","stable","MIT",[123,124],"Python","TypeScript",[126,127],"Linux","Docker",[129,130,131],"Multi-robot fleet management","Behavior orchestration","Data collection pipeline",[133,134,135],"Multi-robot research studies","Fleet coordination","Data collection automation",{"documentation":137,"github":138,"website":139},"https://arora.readthedocs.io","https://github.com/semio-community/arora","https://arora.semio.ai",[97],[97],[143,144,145],"orchestration","fleet-management","multi-robot",["Date","2024-11-20T00:00:00.000Z"],["Date","2023-06-15T00:00:00.000Z"],"## Overview\n\nArora is a robotics orchestration platform designed to simplify the management of multi-robot systems in research environments.\n\n## Features\n\n- Multi-robot fleet management\n- Behavior orchestration engine\n- Real-time monitoring\n- Data collection automation\n\n## Getting Started\n\nDocumentation and installation instructions are available at [https://arora.readthedocs.io](https://arora.readthedocs.io).","src/content/software/arora.mdx","1de3d410b87da673","events",["Map",153,154],"hri-2025",{"id":153,"data":155,"body":222,"filePath":223,"digest":224,"deferredRender":67},{"name":156,"description":157,"type":158,"format":159,"startDate":160,"endDate":161,"registrationDeadline":162,"location":163,"organizers":170,"speakers":180,"tracks":193,"topics":200,"links":210,"pricing":214,"capacity":219,"featured":67,"tags":220},"HRI 2025: ACM/IEEE International Conference on Human-Robot Interaction","The 20th Annual ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for presenting and discussing cutting-edge research in human-robot interaction. HRI 2025 brings together researchers, practitioners, and industry leaders to share the latest advances in HRI theory, methods, technologies, and applications.","conference","hybrid",["Date","2025-03-04T00:00:00.000Z"],["Date","2025-03-06T00:00:00.000Z"],["Date","2025-02-15T00:00:00.000Z"],{"venue":164,"city":165,"country":166,"online":67,"coordinates":167},"Melbourne Convention and Exhibition Centre","Melbourne","Australia",{"lat":168,"lng":169},-37.8258,144.9559,[171,175,178],{"name":172,"role":173,"affiliation":174},"ACM SIGCHI","Co-organizer","Association for Computing Machinery",{"name":176,"role":173,"affiliation":177},"IEEE RAS","IEEE Robotics and Automation Society",{"name":97,"role":179,"affiliation":97},"Community Partner",[181,185,190],{"name":182,"title":183,"affiliation":121,"topic":184},"Dr. Cynthia Breazeal","Professor and Dean for Digital Learning","Social Robots and Human Flourishing",{"name":186,"title":187,"affiliation":188,"topic":189},"Dr. Hiroshi Ishiguro","Professor","Osaka University","Android Science and Human-Robot Symbiosis",{"name":191,"title":187,"affiliation":121,"topic":192},"Dr. Julie Shah","Human-Robot Collaboration in Complex Environments",[194,195,196,197,198,199],"Technical Sessions","Late Breaking Reports","Demonstrations","Student Design Competition","Workshops and Tutorials","Video Presentations",[201,90,202,203,204,205,206,207,208,209],"Human-Robot Interaction Theory","Robot Design and Aesthetics","Ethics and Trust in HRI","Collaborative Robotics","Healthcare and Assistive Robotics","Educational Robotics","Field Studies and Applications","Multi-modal Interaction","Robot Learning from Humans",{"website":211,"registration":212,"program":213},"https://humanrobotinteraction.org/2025/","https://humanrobotinteraction.org/2025/registration","https://humanrobotinteraction.org/2025/program",{"student":215,"academic":216,"industry":217,"virtual":218},350,650,850,150,800,[103,158,102,221,104],"human-robot-interaction","## About HRI 2025\n\nThe ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for showcasing the very best interdisciplinary and multidisciplinary research in human-robot interaction. Researchers from diverse backgrounds including robotics, computer science, engineering, design, behavioral and social sciences come together to define and advance the state-of-the-art in HRI.\n\n## Conference Theme: \"Robots in the Wild\"\n\nHRI 2025's theme focuses on deploying robots in real-world, uncontrolled environments. As robots move from laboratories into homes, workplaces, and public spaces, understanding how they interact with diverse populations in complex, dynamic settings becomes crucial.\n\n## Key Dates\n\n- **Paper Submission Deadline**: October 1, 2024\n- **Notification of Acceptance**: December 15, 2024\n- **Camera-Ready Deadline**: January 15, 2025\n- **Early Registration Deadline**: February 15, 2025\n- **Conference Dates**: March 4-6, 2025\n\n## Program Highlights\n\n### Technical Sessions\n- 80+ full papers presenting cutting-edge research\n- 150+ late-breaking reports on emerging work\n- Interactive poster sessions\n\n### Special Programs\n- **Student Design Competition**: Teams compete to create innovative HRI solutions\n- **Robot Demonstrations**: Live demos of the latest robotic systems\n- **Industry Showcase**: Leading companies present commercial HRI applications\n- **Video Session**: Creative presentations of HRI research and applications\n\n### Workshops & Tutorials\n- \"LLMs for Human-Robot Interaction\"\n- \"Ethics and Responsible Innovation in HRI\"\n- \"Participatory Design Methods for Social Robots\"\n- \"Measuring Trust in Human-Robot Teams\"\n- \"Cross-Cultural Perspectives in HRI\"\n\n## Keynote Speakers\n\n### Dr. Cynthia Breazeal\n**\"Social Robots and Human Flourishing\"**\nExploring how social robots can support human wellbeing, learning, and social connection across the lifespan.\n\n### Dr. Hiroshi Ishiguro\n**\"Android Science and Human-Robot Symbiosis\"**\nExamining the future of human-like robots and their role in understanding human nature and society.\n\n### Dr. Julie Shah\n**\"Human-Robot Collaboration in Complex Environments\"**\nAddressing challenges in developing robots that can effectively partner with humans in dynamic, high-stakes settings.\n\n## Semio Community Involvement\n\nSemio Community is proud to be a Community Partner for HRI 2025. We're organizing several activities:\n\n### Hardware Showcase\nDemonstration of community-driven robotics platforms:\n- Quori social robot demonstrations\n- Hands-on sessions with Ommie platform\n- BeholderBot perception system demos\n\n### Community Workshop\n\"Building Reproducible HRI Research with Open Hardware and Software\"\n- Best practices for reproducible research\n- Introduction to Semio hardware platforms\n- ROS 2 HRI Toolkit tutorial\n- Community collaboration opportunities\n\n### Meetup\nJoin the Semio Community meetup on March 5th, 6:00 PM for:\n- Networking with community members\n- Updates on new initiatives\n- Collaboration opportunities\n- Light refreshments\n\n## Venue Information\n\n### Melbourne Convention and Exhibition Centre\n- World-class facilities in the heart of Melbourne\n- Easy access to public transportation\n- Walking distance to restaurants and hotels\n- Full accessibility for all attendees\n\n### Virtual Participation\n- Live streaming of keynotes and selected sessions\n- Virtual poster sessions with interactive Q&A\n- Access to recorded content for 6 months post-conference\n- Virtual networking opportunities\n\n## Registration\n\n### In-Person Registration Includes:\n- All conference sessions and keynotes\n- Conference proceedings\n- Welcome reception\n- Coffee breaks and lunch\n- Conference dinner (March 5th)\n- Conference bag and materials\n\n### Virtual Registration Includes:\n- Live stream access to main sessions\n- Interactive virtual poster sessions\n- Access to conference proceedings\n- 6-month access to recorded content\n- Virtual networking platform access\n\n## Travel and Accommodation\n\n### Recommended Hotels\n- **Crown Metropol Melbourne** (5-min walk)\n- **Novotel Melbourne South Wharf** (3-min walk)\n- **Holiday Inn Express Melbourne** (10-min walk)\n\nSpecial conference rates available through the registration page.\n\n### Travel Grants\nLimited travel grants available for:\n- Student presenters\n- Researchers from developing countries\n- Early career researchers\n\nApplication deadline: January 15, 2025\n\n## COVID-19 Safety\n\nHRI 2025 follows local health guidelines and implements safety measures including:\n- Optional mask-wearing\n- Hand sanitizing stations\n- Hybrid attendance options\n- Flexible cancellation policy\n\n## Contact\n\nFor general inquiries: info@hri2025.org\nFor Semio Community activities: events@community.semio.ai\n\n## Sponsors\n\n### Platinum Sponsors\n- Major robotics companies and research institutions\n\n### Gold Sponsors\n- Technology companies and foundations\n\n### Community Partners\n- **Semio Community**: Supporting reproducible HRI research\n- Other HRI-focused organizations\n\nJoin us in Melbourne for this landmark conference celebrating 20 years of HRI research and looking toward the future of human-robot interaction!","src/content/events/hri-2025.mdx","e7505cc72f81cfdb","partners",["Map",227,228],"george-mason-university",{"id":227,"data":229,"body":246,"filePath":247,"digest":248,"deferredRender":67},{"name":230,"description":231,"type":232,"category":104,"website":233,"collaboration":234,"location":242,"featured":67,"order":245},"George Mason University","George Mason University is a leading research institution partnering with Semio Community on robotics and human-robot interaction initiatives.","academic","https://www.gmu.edu",{"areas":235,"projects":239,"startDate":241,"active":67},[236,237,238],"Human-Robot Interaction Research","Hardware Development","Educational Programs",[240],"MuSoHu (GMU Helmet)",["Date","2023-01-01T00:00:00.000Z"],{"city":243,"country":244},"Fairfax","United States",1,"## Partnership Overview\n\nGeorge Mason University is an academic partner of the Semio Community, contributing to the development of the MuSoHu (Multi-modal Social Human) helmet platform and advancing research in human-robot interaction.\n\n## Key Contributions\n\n### MuSoHu (GMU Helmet)\n\nGMU is the lead institution developing the MuSoHu helmet system, an innovative wearable device for HRI research.\n\n## Contact\n\nFor partnership inquiries, please contact us through the main Semio Community channels.","src/content/partners/george-mason-university.mdx","299abbb0834b429b"]