[["Map",1,2,9,10,258,259,327,328,402,403],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.1","content-config-digest","8135562df6edd95a","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://semio-community.github.io/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":true,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"prefetch\":true,\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[\"webmention.io\"],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[[null,{\"rel\":[\"nofollow\",\"noreferrer\"],\"target\":\"_blank\"}],[null,{\"theme\":{\"light\":\"rose-pine-dawn\",\"dark\":\"rose-pine\"},\"transformers\":[{\"name\":\"@shikijs/transformers:notation-diff\"},{\"name\":\"@shikijs/transformers:meta-highlight\"}]}],null],\"remarkRehype\":{\"footnoteLabelProperties\":{\"className\":[\"\"]},\"footnoteBackContent\":\"â¤´\"},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{\"WEBMENTION_API_KEY\":{\"context\":\"server\",\"access\":\"secret\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_URL\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"},\"WEBMENTION_PINGBACK\":{\"context\":\"client\",\"access\":\"public\",\"optional\":true,\"type\":\"string\"}},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","hardware",["Map",11,12,79,80,140,141,210,211],"jibo",{"id":11,"data":13,"body":73,"filePath":74,"assetImports":75,"digest":77,"deferredRender":78},{"name":14,"description":15,"shortDescription":16,"category":17,"status":18,"specifications":19,"features":30,"applications":40,"researchAreas":47,"links":54,"images":57,"maintainers":59,"institutions":61,"tags":63,"featured":71,"publishDate":72},"Jibo","A social robot platform originally developed as the world's first commercial social robot for the home. Features an expressive animated face on a touchscreen display, three-axis motor system for fluid movement, and interactive capabilities for family engagement.","Social robot with animated touchscreen face and expressive movement","social","experimental",{"height":20,"weight":21,"battery":22,"sensors":23,"actuators":27,"computePlatform":29},"11 inches (28 cm)","6 pounds (2.7 kg)","AC power adapter",[24,25,26],"2 color stereo cameras","360-degree microphone array","Touch sensors",[28],"3 full-revolute axes motor system","Embedded Linux system",[31,32,33,34,35,36,37,38,39],"HD LCD touchscreen display for animated face","Three-axis motor system enabling fluid turns and movements","360-degree sound localization for voice interaction","Face recognition and tracking capabilities","Full-body touch sensing","WiFi and Bluetooth connectivity","Premium stereo speakers","Full spectrum ambient LED lighting","Jibo Alive SDK for developer applications",[41,42,43,44,45,46],"Family companion robot","Social robotics research","Human-robot interaction studies","Interactive storytelling","Video calling and telepresence","Home assistance",[48,49,50,51,52,53],"Social Robotics","Human-Computer Interaction","Artificial Intelligence","Behavioral Science","Affective Computing","Machine Learning",{"documentation":55,"website":56},"https://hri2024.jibo.media.mit.edu","https://www.media.mit.edu/projects/jibo-research-platform/overview/",{"hero":58},"__ASTRO_IMAGE_@/assets/images/hardware/jibo-hero.jpg",[60],"MIT Media Lab Personal Robots Group",[62],"Massachusetts Institute of Technology",[64,65,66,67,68,69,70],"social-robotics","hri","family-robot","companion","mit","touchscreen","expressive",false,["Date","2024-03-01T00:00:00.000Z"],"## Overview\n\nJibo emerged from MIT Media Lab's Personal Robots Group under the direction of Cynthia Breazeal, pioneering the concept of a social robot designed specifically for home environments. The robot's distinctive design features a stationary base with a three-axis motor system that enables fluid, lifelike movements including turns and expressive gestures. Its animated face displayed on an HD touchscreen creates engaging social interactions through dynamic expressions and eye contact tracking, while 360-degree sound localization allows natural voice interactions from anywhere in the room.\n\nThe platform has evolved into the Jibo Research Platform, providing researchers with a deployable infrastructure for social robotics experimentation and data collection. This research-oriented version extends the original hardware and software architecture with enhanced capabilities for academic study, including improved data security measures and developer tools. The system's Linux-based architecture and accompanying SDK enable researchers and developers to create custom applications and behaviors, facilitating diverse studies in human-robot interaction and social robotics applications.\n\n## Research Publications\n\n- (2024). **Jibo Community Social Robot Research Platform @Scale.** *HRI 2024 Workshop/Tutorial*, Boulder, Colorado. MIT Media Lab.\n\n- Breazeal, C. et al. **Jibo: The World's First Social Robot for the Home.** *MIT Media Lab Personal Robots Group*.","src/content/hardware/jibo.mdx",[76],"@/assets/images/hardware/jibo-hero.jpg","b9b688e0cf0c5038",true,"ommie",{"id":79,"data":81,"body":135,"filePath":136,"assetImports":137,"digest":139,"deferredRender":78},{"name":82,"description":83,"shortDescription":84,"category":85,"status":18,"specifications":86,"features":99,"applications":106,"researchAreas":111,"links":117,"images":120,"maintainers":122,"institutions":124,"tags":126,"featured":78,"publishDate":134},"Ommie","A novel socially assistive robot designed to support deep breathing practices for anxiety reduction. Ommie uses haptic interaction and non-verbal cues to guide users through calming breathing exercises.","Social robot for anxiety reduction through guided deep breathing","assistive",{"height":87,"weight":88,"battery":89,"sensors":90,"actuators":95,"computePlatform":98},"Approx. 0.3 meters (tabletop size)","Approx. 2-3 kg","Powered via wall adapter",[91,92,93,94],"Capacitive touch","IMU","Motor encoders","Optional camera",[96,97],"Dynamixel MX-64AT (breathing motion)","Dynamixel AX-12A (head nodding)","Raspberry Pi",[100,101,102,103,104,105],"Haptic breathing guidance through physical expansion/contraction","Non-verbal interaction through eye animations and audio chimes","Soft sweater covering for comfortable touch","Body (breathing) and head motions (nodding)","Capacitive touch inputs","Multiple breathing patterns supported (customizable)",[107,108,109,110],"Anxiety reduction","Deep breathing","Stress management","Mental health support",[112,113,114,115,116],"Socially Assistive Robotics","Human-Robot Interaction","Mental Health Technology","Haptic Interaction","Therapeutic Robotics",{"documentation":118,"website":119},"https://dl.acm.org/doi/10.1145/3706122","https://scazlab.yale.edu/ommie-robot",{"hero":121},"__ASTRO_IMAGE_@/assets/images/hardware/ommie-hero.png",[123],"Yale University Social Robotics Lab",[125],"Yale University",[127,128,129,130,131,132,133],"mental-health","therapeutic","haptic","social-robot","anxiety","breathing","wellness",["Date","2025-02-19T00:00:00.000Z"],"## Overview\n\nOmmie is a socially assistive robot developed at Yale University's Social Robotics Lab to help individuals manage anxiety through guided deep breathing exercises. The robot's core functionality centers on its unique haptic interaction design: it physically expands and contracts in a rhythmic breathing pattern while users place their hands on its soft, sweater-covered body. This mechanical breathing motion, combined with synchronized audio chimes and expressive eye animations, helps users naturally synchronize their own breathing to therapeutic deep breathing patterns that have been scientifically shown to calm the autonomic nervous system.\n\nInitial research with 43 participants at a university wellness center demonstrated Ommie's effectiveness, with users experiencing statistically significant reductions in anxiety state measures after interacting with the robot. Beyond the quantitative improvements, participants consistently rated the robot highly for its calming, approachable, and engaging qualities. Users particularly highlighted the focusing effect of the haptic interaction, reporting that the physical sensation helped them achieve a calmer state more quickly than traditional breathing exercises. Many participants also described experiencing a sense of companionship during the interaction, comparing it to the motivational benefits of group meditation. The robot has shown promise beyond adult anxiety management, with ongoing applications in pediatric healthcare settings where it has been successfully deployed to help children manage stress during oral allergy challenges. Recent work has also explored the application of machine learning algorithms to detect deep breathing phase for monitoring and adjusting deep breathing practices. Future work involves expanding use of the robot to new populations benefitting from deep breathing, as well as more longitudinal, in-the-wild experimentation.\n\n## Research Publications\n\n- Chun, A., Mamantov, E., Stahl, U., Scassellati, B., & Leeds, S. (2025). **Breathe Easy: Harnessing Robots for Stress Reduction During Pediatric Oral Challenges.** *Journal of Allergy and Clinical Immunology*, 155(2).\n\n- Matheus, K., VÃ¡zquez, M., & Scassellati, B. (2024). **Ommie: The Design and Development of a Social Robot for Anxiety Reduction.** *ACM Transactions on Human-Robot Interaction*.\n\n- Matheus, K., Mamantov, E., VÃ¡zquez, M., & Scassellati, B. (2023). **Deep Breathing Phase Classification with a Social Robot for Mental Health.** *International Conference on Multimodal Interaction (ICMI '23)*, Paris, France.\n\n- Matheus, K., VÃ¡zquez, M., & Scassellati, B. (2022). **A Social Robot for Anxiety Reduction via Deep Breathing.** *31st IEEE International Conference on Robot Human Interactive Communication (RO-MAN)*, Naples, Italy. **[Best Student Paper Award]**","src/content/hardware/ommie.mdx",[138],"@/assets/images/hardware/ommie-hero.png","5c6f410816ae50ff","quoriv1",{"id":140,"data":142,"body":205,"filePath":206,"assetImports":207,"digest":209,"deferredRender":78},{"name":143,"description":144,"shortDescription":145,"category":17,"status":146,"specifications":147,"features":162,"applications":173,"researchAreas":179,"links":184,"images":187,"maintainers":189,"institutions":193,"tags":196,"featured":78,"publishDate":204},"Quori v1","A modular, affordable socially interactive robot platform developed for enabling human-robot interaction research. Features an expressive projected face, gesturing arms with shoulder-like articulation, flexible spine, and omnidirectional mobility.","Modular social robot platform for HRI research with projected face and expressive gestures","available",{"height":148,"weight":149,"battery":150,"sensors":151,"actuators":156,"computePlatform":161},"1.35 meters (resting position)","Approx. 45-50 kg","Onboard rechargeable battery system",[152,153,154,26,155],"Depth camera","RGB camera","Microphone array","Proximity sensors",[157,158,159,160],"2 DOF shoulder joints per arm","Omnidirectional base motors","Spine articulation motors","Turret rotation motor","Intel NUC with ROS integration",[163,164,165,166,167,168,169,170,171,172],"Rear-projected animated face for flexible expression design","Two gesturing arms with shoulder-like ball joint articulation (2 DOF each)","Bowing spine mechanism for body language expression","Omnidirectional mobile base (0.8 m/s linear, 180Â°/s rotational)","Modular panelized design with magnetic attachment system","ROS-based control interfaces at multiple abstraction levels","Browser-based content creation and animation tools","Built-in text-to-speech capabilities","ADA-compliant base dimensions","Low noise operation (quiet at 1 meter distance)",[43,42,174,175,176,177,178],"Non-contact interaction experiments","In-lab behavioral studies","Field deployment research","Multi-modal communication research","Gesture and expression studies",[113,48,180,50,181,182,183],"Non-verbal Communication","Behavioral Computing","Interactive Systems","Embodied AI",{"documentation":185,"website":186},"https://quori-robot.github.io/quori_v1_documentation/","https://quori.org",{"hero":188},"__ASTRO_IMAGE_@/assets/images/hardware/quori.v1-hero.jpg",[190,191,192],"Semio","UPenn Modlab","USC Interaction Lab",[194,195,190],"University of Pennsylvania","University of Southern California",[64,65,197,198,199,200,201,202,203],"research-platform","modular","open-hardware","ros","projected-face","gesture","mobile",["Date","2024-01-15T00:00:00.000Z"],"## Overview\n\nQuori is an affordable, modular social robot platform developed through a National Science Foundation initiative to democratize human-robot interaction research. Created through extensive community consultation, the robot features a distinctive rear-projected animated face for flexible expression design, two articulated arms with shoulder-like movement, a bowing spine mechanism, and an omnidirectional mobile base capable of smooth navigation in both laboratory and real-world settings. The platform's modular architecture allows researchers to customize and extend capabilities while maintaining standardization across the ten units distributed to U.S. research institutions.\n\nThe robot bridges technical accessibility gaps by providing control interfaces at multiple levels - from low-level ROS commands for direct hardware control to browser-based tools for creating conversational content and animations without extensive programming expertise. This tiered approach, combined with open-source hardware documentation and swappable components, enables diverse research applications ranging from non-verbal communication studies to field deployments in public spaces, all while maintaining ADA compliance and quiet operation suitable for human interaction contexts.\n\n## Research Publications\n\n- Specian, A., Eckenstein, N., Mead, R., McDorman, B., Kim, S., Mataric, M., & Yim, M. (2018). **Preliminary system and hardware design for Quori, a low-cost, modular, socially interactive robot.** *2018 HRI Workshop Social Robots in the Wild*, 1-6.","src/content/hardware/quori.v1.mdx",[208],"@/assets/images/hardware/quori.v1-hero.jpg","3906cb990e39ebf4","quoriv2",{"id":210,"data":212,"body":253,"filePath":254,"assetImports":255,"digest":257,"deferredRender":78},{"name":213,"description":214,"shortDescription":215,"category":17,"status":216,"specifications":217,"features":229,"applications":241,"researchAreas":242,"links":243,"images":244,"maintainers":246,"institutions":248,"tags":249,"featured":78,"publishDate":252},"Quori v2","An advanced modular socially interactive robot platform with enhanced sensing and interaction capabilities. Features a touchscreen display, dual speakers, 14-DOF articulation including 4-DOF arms, multiple IMUs and laser rangefinders, programmable light arrays, and 3-DOF holonomic mobility.","Advanced modular social robot with touchscreen, enhanced sensors, and 14-DOF articulation","coming-soon",{"height":148,"weight":149,"battery":150,"sensors":218,"actuators":222,"computePlatform":228},[219,154,220,221],"RGB+D camera","Laser rangefinders (x2)","IMUs (x4)",[223,224,225,226,227],"14 DOFs total","2-DOF head","1-DOF neck","4-DOF arms (x2)","3-DOF holonomic base","Onboard computer",[230,231,232,233,234,235,236,237,238,239,240,168,169],"Touchscreen display for interactive communication","Dual speakers for enhanced audio output","Two articulated arms with 4 DOF each for complex gestures","2-DOF head with 1-DOF neck for expressive movements","3-DOF holonomic base for smooth omnidirectional mobility","Dual laser rangefinders for precise navigation","Four IMUs for enhanced motion sensing and stability","Four programmable light arrays for visual feedback","Onboard storage bin for carrying items","Customizable badge for personalization","Enhanced modular design and functionality",[43,42,174,175,176,177,178],[113,48,180,50,181,182,183],{"documentation":185,"website":186},{"hero":245},"__ASTRO_IMAGE_@/assets/images/hardware/quori.v2-hero.png",[190,247],"Oregon State University",[247,190],[64,65,197,198,199,200,69,203,250,251],"multi-sensor","interactive-display",["Date","2024-01-15T00:00:00.000Z"],"## Overview\n\nQuori v2.0 represents a significant advancement in socially interactive robot platforms, building upon the successful foundation established by the National Science Foundation initiative. This enhanced version features a comprehensive sensor suite including an RGB+D camera, dual laser rangefinders, and four IMUs for superior environmental perception and stability. The robot's interaction capabilities have been dramatically expanded with a touchscreen display for direct user engagement, dual speakers for immersive audio experiences, and four programmable light arrays that enable rich visual communication patterns.\n\nThe mechanical design has evolved to offer 14 degrees of freedom, including sophisticated 4-DOF arms (8 DOF total) for complex gestural expression, a 2-DOF head with 1-DOF neck for nuanced non-verbal communication, and a 3-DOF holonomic base for smooth omnidirectional navigation. Additional features like the onboard storage bin enable practical applications in service scenarios, while the customizable badge system allows for personalization in multi-robot deployments. This modular architecture maintains backward compatibility while offering researchers unprecedented flexibility in customizing and extending the platform's capabilities for diverse human-robot interaction studies.\n\n## Research Publications\n\n- Specian, A., Eckenstein, N., Mead, R., McDorman, B., Kim, S., Mataric, M., & Yim, M. (2018). **Preliminary system and hardware design for Quori, a low-cost, modular, socially interactive robot.** *2018 HRI Workshop Social Robots in the Wild*, 1-6.","src/content/hardware/quori.v2.mdx",[256],"@/assets/images/hardware/quori.v2-hero.png","83607f51457bf384","software",["Map",260,261],"arora",{"id":260,"data":262,"body":322,"filePath":323,"assetImports":324,"digest":326,"deferredRender":78},{"name":263,"description":264,"shortDescription":265,"category":266,"status":18,"license":267,"language":268,"platform":272,"requirements":276,"features":285,"useCases":294,"links":301,"images":305,"maintainers":307,"institutions":310,"tags":311,"featured":78,"lastUpdate":320,"publishDate":321},"Arora","A software-as-a-service platform for creating, deploying, and executing multimodal natural language-based applications for interactive personal robots and digital/virtual characters. Provides tools for voice scripting, animation, and custom functionality development.","SaaS platform for natural language robot applications","framework","MIT",[269,270,271],"Python","TypeScript","JavaScript",[273,274,275],"Linux","Docker","Cloud",{"runtime":277,"hardware":280,"dependencies":282},[278,279],"Node.js 18+","Python 3.8+",[281],"Compatible robot platform",[283,284],"ROS/ROS2 (optional)","WebRTC",[286,287,288,289,290,291,292,293],"Voice script creation for conversational HRI","Animation tools for expressive movements","JavaScript SDK for custom functionality","Real-time simulation and testing","Multi-robot fleet orchestration","Cloud-based deployment pipeline","Usage analytics and monitoring","Natural language interaction engine",[295,296,297,298,299,300],"Conversational robot applications","Multi-robot research studies","Interactive character development","Human-robot interaction experiments","Fleet coordination and management","Behavior animation and scripting",{"documentation":302,"github":303,"website":304},"https://arora.readthedocs.io","https://github.com/semio-community/arora","https://semio.ai",{"hero":306},"__ASTRO_IMAGE_@/assets/images/software/arora-hero.png",[308,309],"Semio Community","Semio AI Inc.",[190],[312,313,314,315,316,317,318,319],"orchestration","fleet-management","multi-robot","natural-language","saas","animation","voice","conversational-ai",["Date","2024-11-20T00:00:00.000Z"],["Date","2023-06-15T00:00:00.000Z"],"## Overview\n\nArora is Semio's comprehensive software-as-a-service platform designed to streamline the development and deployment of natural language-based applications for robots and virtual characters. Built on the principle that natural language should be the primary interface for human-robot interaction, Arora provides an integrated suite of tools that significantly reduces development time - aiming to cut content creation time by 60%, software integration by 75%, and product launch timelines by 90%.\n\nThe platform will developers and researchers to rapidly prototype and deploy sophisticated robot behaviors through its three core modules: voice scripting for conversational interactions, animation tools for expressive movements, and a JavaScript SDK for custom functionality. This unified approach allows teams to create, test in simulation, deploy to hardware, and analyze usage data all within a single ecosystem, making it ideal for both research institutions conducting HRI studies and companies developing commercial robot applications.","src/content/software/arora.mdx",[325],"@/assets/images/software/arora-hero.png","eb195a47ae8347f5","events",["Map",329,330],"hri-2025",{"id":329,"data":331,"body":399,"filePath":400,"digest":401,"deferredRender":78},{"name":332,"description":333,"type":334,"format":335,"startDate":336,"endDate":337,"registrationDeadline":338,"location":339,"organizers":346,"speakers":356,"tracks":369,"topics":376,"links":386,"pricing":390,"capacity":395,"featured":78,"tags":396},"HRI 2025: ACM/IEEE International Conference on Human-Robot Interaction","The 20th Annual ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for presenting and discussing cutting-edge research in human-robot interaction. HRI 2025 brings together researchers, practitioners, and industry leaders to share the latest advances in HRI theory, methods, technologies, and applications.","conference","hybrid",["Date","2025-03-04T00:00:00.000Z"],["Date","2025-03-06T00:00:00.000Z"],["Date","2025-02-15T00:00:00.000Z"],{"venue":340,"city":341,"country":342,"online":78,"coordinates":343},"Melbourne Convention and Exhibition Centre","Melbourne","Australia",{"lat":344,"lng":345},-37.8258,144.9559,[347,351,354],{"name":348,"role":349,"affiliation":350},"ACM SIGCHI","Co-organizer","Association for Computing Machinery",{"name":352,"role":349,"affiliation":353},"IEEE RAS","IEEE Robotics and Automation Society",{"name":308,"role":355,"affiliation":308},"Community Partner",[357,361,366],{"name":358,"title":359,"affiliation":267,"topic":360},"Dr. Cynthia Breazeal","Professor and Dean for Digital Learning","Social Robots and Human Flourishing",{"name":362,"title":363,"affiliation":364,"topic":365},"Dr. Hiroshi Ishiguro","Professor","Osaka University","Android Science and Human-Robot Symbiosis",{"name":367,"title":363,"affiliation":267,"topic":368},"Dr. Julie Shah","Human-Robot Collaboration in Complex Environments",[370,371,372,373,374,375],"Technical Sessions","Late Breaking Reports","Demonstrations","Student Design Competition","Workshops and Tutorials","Video Presentations",[377,48,378,379,380,381,382,383,384,385],"Human-Robot Interaction Theory","Robot Design and Aesthetics","Ethics and Trust in HRI","Collaborative Robotics","Healthcare and Assistive Robotics","Educational Robotics","Field Studies and Applications","Multi-modal Interaction","Robot Learning from Humans",{"website":387,"registration":388,"program":389},"https://humanrobotinteraction.org/2025/","https://humanrobotinteraction.org/2025/registration","https://humanrobotinteraction.org/2025/program",{"student":391,"academic":392,"industry":393,"virtual":394},350,650,850,150,800,[65,334,64,397,398],"human-robot-interaction","research","## About HRI 2025\n\nThe ACM/IEEE International Conference on Human-Robot Interaction is the premier venue for showcasing the very best interdisciplinary and multidisciplinary research in human-robot interaction. Researchers from diverse backgrounds including robotics, computer science, engineering, design, behavioral and social sciences come together to define and advance the state-of-the-art in HRI.\n\n## Conference Theme: \"Robots in the Wild\"\n\nHRI 2025's theme focuses on deploying robots in real-world, uncontrolled environments. As robots move from laboratories into homes, workplaces, and public spaces, understanding how they interact with diverse populations in complex, dynamic settings becomes crucial.\n\n## Key Dates\n\n- **Paper Submission Deadline**: October 1, 2024\n- **Notification of Acceptance**: December 15, 2024\n- **Camera-Ready Deadline**: January 15, 2025\n- **Early Registration Deadline**: February 15, 2025\n- **Conference Dates**: March 4-6, 2025\n\n## Program Highlights\n\n### Technical Sessions\n- 80+ full papers presenting cutting-edge research\n- 150+ late-breaking reports on emerging work\n- Interactive poster sessions\n\n### Special Programs\n- **Student Design Competition**: Teams compete to create innovative HRI solutions\n- **Robot Demonstrations**: Live demos of the latest robotic systems\n- **Industry Showcase**: Leading companies present commercial HRI applications\n- **Video Session**: Creative presentations of HRI research and applications\n\n### Workshops & Tutorials\n- \"LLMs for Human-Robot Interaction\"\n- \"Ethics and Responsible Innovation in HRI\"\n- \"Participatory Design Methods for Social Robots\"\n- \"Measuring Trust in Human-Robot Teams\"\n- \"Cross-Cultural Perspectives in HRI\"\n\n## Keynote Speakers\n\n### Dr. Cynthia Breazeal\n**\"Social Robots and Human Flourishing\"**\nExploring how social robots can support human wellbeing, learning, and social connection across the lifespan.\n\n### Dr. Hiroshi Ishiguro\n**\"Android Science and Human-Robot Symbiosis\"**\nExamining the future of human-like robots and their role in understanding human nature and society.\n\n### Dr. Julie Shah\n**\"Human-Robot Collaboration in Complex Environments\"**\nAddressing challenges in developing robots that can effectively partner with humans in dynamic, high-stakes settings.\n\n## Semio Community Involvement\n\nSemio Community is proud to be a Community Partner for HRI 2025. We're organizing several activities:\n\n### Hardware Showcase\nDemonstration of community-driven robotics platforms:\n- Quori social robot demonstrations\n- Hands-on sessions with Ommie platform\n- BeholderBot perception system demos\n\n### Community Workshop\n\"Building Reproducible HRI Research with Open Hardware and Software\"\n- Best practices for reproducible research\n- Introduction to Semio hardware platforms\n- ROS 2 HRI Toolkit tutorial\n- Community collaboration opportunities\n\n### Meetup\nJoin the Semio Community meetup on March 5th, 6:00 PM for:\n- Networking with community members\n- Updates on new initiatives\n- Collaboration opportunities\n- Light refreshments\n\n## Venue Information\n\n### Melbourne Convention and Exhibition Centre\n- World-class facilities in the heart of Melbourne\n- Easy access to public transportation\n- Walking distance to restaurants and hotels\n- Full accessibility for all attendees\n\n### Virtual Participation\n- Live streaming of keynotes and selected sessions\n- Virtual poster sessions with interactive Q&A\n- Access to recorded content for 6 months post-conference\n- Virtual networking opportunities\n\n## Registration\n\n### In-Person Registration Includes:\n- All conference sessions and keynotes\n- Conference proceedings\n- Welcome reception\n- Coffee breaks and lunch\n- Conference dinner (March 5th)\n- Conference bag and materials\n\n### Virtual Registration Includes:\n- Live stream access to main sessions\n- Interactive virtual poster sessions\n- Access to conference proceedings\n- 6-month access to recorded content\n- Virtual networking platform access\n\n## Travel and Accommodation\n\n### Recommended Hotels\n- **Crown Metropol Melbourne** (5-min walk)\n- **Novotel Melbourne South Wharf** (3-min walk)\n- **Holiday Inn Express Melbourne** (10-min walk)\n\nSpecial conference rates available through the registration page.\n\n### Travel Grants\nLimited travel grants available for:\n- Student presenters\n- Researchers from developing countries\n- Early career researchers\n\nApplication deadline: January 15, 2025\n\n## COVID-19 Safety\n\nHRI 2025 follows local health guidelines and implements safety measures including:\n- Optional mask-wearing\n- Hand sanitizing stations\n- Hybrid attendance options\n- Flexible cancellation policy\n\n## Contact\n\nFor general inquiries: info@hri2025.org\nFor Semio Community activities: events@community.semio.ai\n\n## Sponsors\n\n### Platinum Sponsors\n- Major robotics companies and research institutions\n\n### Gold Sponsors\n- Technology companies and foundations\n\n### Community Partners\n- **Semio Community**: Supporting reproducible HRI research\n- Other HRI-focused organizations\n\nJoin us in Melbourne for this landmark conference celebrating 20 years of HRI research and looking toward the future of human-robot interaction!","src/content/events/hri-2025.mdx","e7505cc72f81cfdb","partners",["Map",404,405],"george-mason-university",{"id":404,"data":406,"body":423,"filePath":424,"digest":425,"deferredRender":78},{"name":407,"description":408,"type":409,"category":398,"website":410,"collaboration":411,"location":419,"featured":78,"order":422},"George Mason University","George Mason University is a leading research institution partnering with Semio Community on robotics and human-robot interaction initiatives.","academic","https://www.gmu.edu",{"areas":412,"projects":416,"startDate":418,"active":78},[413,414,415],"Human-Robot Interaction Research","Hardware Development","Educational Programs",[417],"MuSoHu (GMU Helmet)",["Date","2023-01-01T00:00:00.000Z"],{"city":420,"country":421},"Fairfax","United States",1,"## Partnership Overview\n\nGeorge Mason University is an academic partner of the Semio Community, contributing to the development of the MuSoHu (Multi-modal Social Human) helmet platform and advancing research in human-robot interaction.\n\n## Key Contributions\n\n### MuSoHu (GMU Helmet)\n\nGMU is the lead institution developing the MuSoHu helmet system, an innovative wearable device for HRI research.\n\n## Contact\n\nFor partnership inquiries, please contact us through the main Semio Community channels.","src/content/partners/george-mason-university.mdx","299abbb0834b429b"]